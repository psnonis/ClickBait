{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -9 java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pprint\n",
    "from pyspark import SparkContext, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.parquet.normed.filled.masked-60000.encode.picked-987.packed\n",
      "valid.parquet.normed.filled.masked-60000.encode.picked-987.packed\n",
      "tests.parquet.normed.filled.masked-60000.encode.picked-987.packed\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('data'): \n",
    "    if 'packed' in file:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Logistic Regression with Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sqlContext.read.parquet('data/train.parquet.normed.filled.masked-60000.encode.picked-987.packed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, features=SparseVector(1000, {0: 0.0529, 1: -0.0128, 2: 0.0151, 5: 0.0444, 8: -0.1634, 9: 0.1, 10: -0.1923, 14: 1.0, 137: 1.0, 192: 1.0, 202: 1.0, 218: 1.0, 264: 1.0, 267: 1.0, 322: 1.0, 428: 1.0, 444: 1.0, 532: 1.0, 545: 1.0, 695: 1.0, 710: 1.0, 822: 1.0, 827: 1.0, 847: 1.0, 893: 1.0, 902: 1.0, 906: 1.0, 969: 1.0, 974: 1.0, 992: 1.0}), weight=0.2561962930260516)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation     import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(featuresCol='features', labelCol='label', \n",
    "                                   maxIter = 10, family = 'binomial',\n",
    "                              weightCol='weight')\n",
    "    \n",
    "start_train = time.time()\n",
    "model = estimator.fit(train)\n",
    "end_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = sqlContext.read.parquet('data/valid.parquet.normed.filled.masked-60000.encode.picked-987.packed')\n",
    "\n",
    "# Make Evaluations\n",
    "start_prediction = time.time()\n",
    "transformed_train = model.transform(train)\n",
    "transformed_dev = model.transform(dev)\n",
    "end_prediction = time.time()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "auc_train = evaluator.evaluate(transformed_train)\n",
    "auc_dev = evaluator.evaluate(transformed_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests - AUC on train is: 73.30\n",
      "Random Forests - AUC on dev is: 73.29\n",
      "Training completed in 0.93 minutes\n",
      "Prediction Completed in 0.00 minutes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Random Forests - AUC on train is: {auc_train * 100:.2f}')\n",
    "print(f'Random Forests - AUC on dev is: {auc_dev * 100:.2f}')\n",
    "print(f'Training completed in {(end_train - start_train)/60:.2f} minutes')\n",
    "print(f'Prediction Completed in {(end_prediction - start_prediction)/60:.2f} minutes')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Random Forests before resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(featuresCol='features', labelCol='label', \n",
    "                                   numTrees = 10)\n",
    "    \n",
    "start_train = time.time()\n",
    "model = estimator.fit(train)\n",
    "end_train = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample the Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Masked 100 Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sqlContext.read.parquet('data/train.parquet.normed.filled.masked-60000.encode.picked-987.packed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27274974"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_count = train.filter(train['label']==0).count()\n",
    "neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9394612"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count = train.filter(train['label']==1).count()\n",
    "pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of train examples is 36669586\n",
      "Percentage of negative examples is 74.38037069739484\n",
      "Percentage of postive examples is 25.619629302605162\n"
     ]
    }
   ],
   "source": [
    "total_count = pos_count + neg_count\n",
    "print(f'Total number of train examples is {total_count}')\n",
    "print (f'Percentage of negative examples is {neg_count*100/total_count}')\n",
    "print (f'Percentage of postive examples is {pos_count*100/total_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data points = 17880362\n",
      "The resample rate is 1.9032571009851178\n"
     ]
    }
   ],
   "source": [
    "# Number of extra data points\n",
    "extra = neg_count - pos_count\n",
    "print(f'Extra data points = {extra}')\n",
    "\n",
    "# Resample rate\n",
    "resample_rate = extra/pos_count\n",
    "resample_rate\n",
    "print(f'The resample rate is {resample_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_positives = train.filter(train['label']==1).sample(withReplacement=True, fraction=1.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|label|            features|            weight|\n",
      "+-----+--------------------+------------------+\n",
      "|    1|(1000,[0,1,2,4,5,...|0.7438037069739484|\n",
      "|    1|(1000,[0,1,2,4,5,...|0.7438037069739484|\n",
      "|    1|(1000,[0,1,2,4,5,...|0.7438037069739484|\n",
      "|    1|(1000,[0,1,2,4,5,...|0.7438037069739484|\n",
      "|    1|(1000,[0,1,2,4,5,...|0.7438037069739484|\n",
      "+-----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extra_positives.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_train = train.union(extra_positives)\n",
    "oversampled_train = oversampled_train.sample(withReplacement = False, fraction = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of train examples is 54508313\n",
      "Percentage of negative examples is 50.03819142228819\n",
      "Percentage of postive examples is 49.96180857771181\n"
     ]
    }
   ],
   "source": [
    "sampled_neg_count = oversampled_train.filter(oversampled_train['label']==0).count()\n",
    "sampled_pos_count = oversampled_train.filter(oversampled_train['label']==1).count()\n",
    "\n",
    "sampled_total_count = sampled_pos_count + sampled_neg_count\n",
    "print(f'Total number of train examples is {sampled_total_count}')\n",
    "print (f'Percentage of negative examples is {sampled_neg_count*100/sampled_total_count}')\n",
    "print (f'Percentage of postive examples is {sampled_pos_count*100/sampled_total_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_train.select('features').first().features.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del extra_positives, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the Oversampled df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_train.write.parquet('data/resampled_data/train.parquet.normed.filled.masked-60000.encode.picked-987.packed.oversampled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model without weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation     import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(featuresCol='features', labelCol='label', maxIter = 10, regParam = 0.0, family = 'binomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = time.time()\n",
    "model = estimator.fit(oversampled_train)\n",
    "end_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train is 1.3215594331423441 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f'Time to train is {(end_train-start_train)/60} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the model on Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = sqlContext.read.parquet('data/valid.parquet.normed.filled.masked-60000.encode.picked-987.packed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Evaluations\n",
    "start_prediction = time.time()\n",
    "transformed_train = model.transform(oversampled_train)\n",
    "transformed_dev = model.transform(dev)\n",
    "end_prediction = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to predict is 0.0013621369997660318\n"
     ]
    }
   ],
   "source": [
    "print(f'Time to predict is {(end_prediction - start_prediction)/60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = evaluator.evaluate(transformed_train)\n",
    "auc_dev = evaluator.evaluate(transformed_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - AUC on train is: 73.31\n",
      "Logistic Regression - AUC on dev is: 73.29\n"
     ]
    }
   ],
   "source": [
    "print(f'Logistic Regression - AUC on train is: {auc_train * 100:.2f}')\n",
    "print(f'Logistic Regression - AUC on dev is: {auc_dev * 100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Regularization and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_train = sqlContext.read.parquet('data/oversampled_data/train.parquet.normed.filled.masked-60000.encode.picked-987.packed.oversampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = sqlContext.read.parquet('data/criteo.parquet.df.dev.normed.filled.masked-100.encode.packed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation     import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(featuresCol='features', labelCol='label', maxIter = 10, regParam = 0.01, family = 'binomial')\n",
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = time.time()\n",
    "model = estimator.fit(oversampled_train)\n",
    "end_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Regulaization: 0.01 - AUC on train is: 73.28\n",
      "Logistic Regression - Regularization: 0.01 - AUC on dev is: 73.26\n",
      "Logistic Regression - Regulaization: 0.1 - AUC on train is: 72.84\n",
      "Logistic Regression - Regularization: 0.1 - AUC on dev is: 72.83\n",
      "Logistic Regression - Regulaization: 0.5 - AUC on train is: 71.62\n",
      "Logistic Regression - Regularization: 0.5 - AUC on dev is: 71.62\n",
      "Logistic Regression - Regulaization: 1.0 - AUC on train is: 70.95\n",
      "Logistic Regression - Regularization: 1.0 - AUC on dev is: 70.96\n",
      "Completed Regulaization parameter search in 7.8684061646461485 minutes\n"
     ]
    }
   ],
   "source": [
    "regularization_values = [0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "start_reg_search = time.time()\n",
    "\n",
    "for reg_value in regularization_values:\n",
    "    \n",
    "    estimator = LogisticRegression(featuresCol='features', labelCol='label', \n",
    "                                   maxIter = 10, regParam = reg_value, family = 'binomial')\n",
    "    \n",
    "    model = estimator.fit(oversampled_train)\n",
    "    \n",
    "    # Make Evaluations\n",
    "\n",
    "    transformed_train = model.transform(oversampled_train)\n",
    "    transformed_dev = model.transform(dev)\n",
    "    end_prediction = time.time()\n",
    "\n",
    "    # Make Evaluations\n",
    "    transformed_train = model.transform(oversampled_train)\n",
    "    transformed_dev = model.transform(dev)\n",
    "\n",
    "\n",
    "    # Get the AUC\n",
    "    auc_train = evaluator.evaluate(transformed_train)\n",
    "    auc_dev = evaluator.evaluate(transformed_dev)\n",
    "\n",
    "    # Print the AUC\n",
    "    print(f'Logistic Regression - Regulaization: {reg_value} - AUC on train is: {auc_train * 100:.2f}')\n",
    "    print(f'Logistic Regression - Regularization: {reg_value} - AUC on dev is: {auc_dev * 100:.2f}')\n",
    "    \n",
    "end_reg_search = time.time()\n",
    "\n",
    "print(f'Completed Regulaization parameter search in {(end_reg_search - start_reg_search)/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(estimator.regParam, [0.01, 0.1, 1.0])\n",
    "             #.addGrid(estimator.maxIter, [5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=estimator, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# train using the crossvalidator\n",
    "start_cv_train = time.time()\n",
    "cvModel = cv.fit(oversampled_train)\n",
    "end_cv_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training with the cross validator took {(end_cv_train - start_cv_train)/60):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions on train and dev\n",
    "train_predictions = cvModel.transform(oversampled_train)\n",
    "test_predictions = cvModel.transform(dev)\n",
    "\n",
    "# Calculate the AUC for train and dev\n",
    "auc_train = evaluator.evaluate(train_predictions)\n",
    "auc_dev = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f'Logistic Regression - AUC on train is: {auc_train * 100:.2f}')\n",
    "print(f'Logistic Regression - AUC on dev is: {auc_dev * 100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "start_train = time.time()\n",
    "model = rf.fit(oversampled_train)\n",
    "end_train = time.time()\n",
    "\n",
    "#evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests - AUC on train is: 70.48\n",
      "Random Forests - AUC on dev is: 70.47\n",
      "Time to train the Random Forests model is 5.293456252415975 minutes\n",
      "Time to run predictions is 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "start_predictions = time.time()\n",
    "train_predictions = model.transform(oversampled_train)\n",
    "test_predictions = model.transform(dev)\n",
    "end_predictions = time.time()\n",
    "\n",
    "#Calculate the AUC for train and dev\n",
    "auc_train = evaluator.evaluate(train_predictions)\n",
    "auc_dev = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f'Random Forests - AUC on train is: {auc_train * 100:.2f}')\n",
    "print(f'Random Forests - AUC on dev is: {auc_dev * 100:.2f}')\n",
    "print(f'Time to train the Random Forests model is {(end_train-start_train)/60} minutes')\n",
    "print(f'Time to run predictions is {(end_predictions-start_predictions)/60:.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sqlContext.read.parquet('data/train.parquet.normed.filled.masked-60000.encode.picked-987.packed')\n",
    "\n",
    "neg_count = train.filter(train['label']==0).count()\n",
    "\n",
    "pos_count = train.filter(train['label']==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The resample rate is 0.34444073163919425\n"
     ]
    }
   ],
   "source": [
    "# Resample rate\n",
    "resample_rate = pos_count/neg_count\n",
    "resample_rate\n",
    "print(f'The resample rate is {resample_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_negatives = train.filter(train['label']==0).sample(withReplacement=False, fraction =0.344)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_train = train.filter(train['label']==1).union(new_negatives)\n",
    "undersampled_train = undersampled_train.sample(withReplacement = False, fraction = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of train examples is 18781446\n",
      "Percentage of negative examples is 49.979293394129506\n",
      "Percentage of postive examples is 50.020706605870494\n"
     ]
    }
   ],
   "source": [
    "sampled_neg_count = undersampled_train.filter(train['label']==0).count()\n",
    "sampled_pos_count = undersampled_train.filter(train['label']==1).count()\n",
    "\n",
    "sampled_total_count = sampled_pos_count + sampled_neg_count\n",
    "print(f'Total number of train examples is {sampled_total_count}')\n",
    "print (f'Percentage of negative examples is {sampled_neg_count*100/sampled_total_count}')\n",
    "print (f'Percentage of postive examples is {sampled_pos_count*100/sampled_total_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_train.write.parquet('data/resampled_data/train.parquet.normed.filled.masked-60000.encode.picked-987.packed.undersampled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_train.cache()\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev = sqlContext.read.parquet('data/criteo.parquet.df.dev.normed.filled.masked-100.encode.packed')\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(featuresCol='features', labelCol='label', maxIter = 10, regParam = 0.0, family = 'binomial')\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "\n",
    "start_train = time.time()\n",
    "model = estimator.fit(undersampled_train)\n",
    "end_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - AUC on train is: 73.32\n",
      "Logistic Regression - AUC on dev is: 73.30\n",
      "Training completed in 0.65 minutes\n",
      "Prediction Completed in 0.02 minutes\n"
     ]
    }
   ],
   "source": [
    "# Make Evaluations\n",
    "start_prediction = time.time()\n",
    "transformed_train = model.transform(undersampled_train)\n",
    "transformed_dev = model.transform(dev)\n",
    "end_prediction = time.time()\n",
    "\n",
    "auc_train = evaluator.evaluate(transformed_train)\n",
    "auc_dev = evaluator.evaluate(transformed_dev)\n",
    "\n",
    "\n",
    "print(f'Logistic Regression - AUC on train is: {auc_train * 100:.2f}')\n",
    "print(f'Logistic Regression - AUC on dev is: {auc_dev * 100:.2f}')\n",
    "print(f'Training completed in {(end_train - start_train)/60:.2f} minutes')\n",
    "print(f'Prediction Completed in {(end_prediction - start_prediction)/60:.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests on the undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "start_train = time.time()\n",
    "model = rf.fit(undersampled_train)\n",
    "end_train = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - AUC on train is: 70.40\n",
      "Logistic Regression - AUC on dev is: 70.39\n",
      "Training completed in 0.93 minutes\n",
      "Prediction Completed in 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "# Make Evaluations\n",
    "start_prediction = time.time()\n",
    "transformed_train = model.transform(undersampled_train)\n",
    "transformed_dev = model.transform(dev)\n",
    "end_prediction = time.time()\n",
    "\n",
    "auc_train = evaluator.evaluate(transformed_train)\n",
    "auc_dev = evaluator.evaluate(transformed_dev)\n",
    "\n",
    "\n",
    "print(f'Logistic Regression - AUC on train is: {auc_train * 100:.2f}')\n",
    "print(f'Logistic Regression - AUC on dev is: {auc_dev * 100:.2f}')\n",
    "print(f'Training completed in {(end_train - start_train)/60:.2f} minutes')\n",
    "print(f'Prediction Completed in {(end_prediction - start_prediction)/60:.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests find ideal number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_train = sqlContext.read.parquet('data/resampled_data/train.parquet.normed.filled.masked-60000.encode.picked-987.packed.oversampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "for num_trees in num_trees_list:\n",
    "    \n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=num_trees)\n",
    "    \n",
    "    start_train = time.time()\n",
    "    model = rf.fit(oversampled_train)\n",
    "    end_train = time.time()\n",
    "    \n",
    "    # Make predictions on Traina and Dev\n",
    "    start_predictions = time.time()\n",
    "    train_predictions = model.transform(oversampled_data)\n",
    "    test_predictions = model.transform(dev)\n",
    "    end_predictions = time.time()\n",
    "\n",
    "    # Calculate the AUC for train and dev\n",
    "    auc_train = evaluator.evaluate(train_predictions)\n",
    "    auc_dev = evaluator.evaluate(test_predictions)\n",
    "\n",
    "    print(f'Random Forests - AUC on train is: {auc_train * 100:.2f}')\n",
    "    print(f'Random Forests - AUC on dev is: {auc_dev * 100:.2f}')\n",
    "    print(f'Training completed in {(end_train - start_train)/60:.2f} minutes')\n",
    "    print(f'Prediction Completed in {(end_prediction - start_prediction)/60:.2f} minutes')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
