{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "In this question, we consider an application of SVM as a classifier. The SVM classifier main idea is to separate the two classess (We will transform the data from $y_i \\in \\{0, 1\\}$ to $y_i \\in \\{-1, 1\\}$\n",
    "\n",
    "## Let's start with $\\ell_2$ SVM\n",
    "\n",
    "The ridged-SVM classification problem can be formulated as the following optimization problem:\n",
    "\n",
    "$$\\underset{w, b}{\\text{min }} \\frac{\\lambda}{2}\\left\\|w\\right\\|_2^2 + \\frac{1}{n}\\sum_{i=1}^{N}{\\left(1 - y_i\\left(w^\\top x_i +b\\right)\\right)_+}$$\n",
    "\n",
    "where $y_i$ denotes the $i^{th}$ label, $x_i$ denotes the $i^{th}$ vector of features in the dataset, $w$ is the weights or vector of coefficients, $b$ is the bias term, and $\\lambda$ is a model parameter is inversely related to the ridge regularization of the weights vector $w$. This is a quadratic optimization problem.\n",
    "\n",
    "For this SVM, we are using a Linear Kernel, that explain the hyperplane that we are using on the Loss Function. Later we will play with this idea and introduce more interesting kernels, and introduce some non-linearities.\n",
    "\n",
    "<!-- Using `cvxpy`, implement this SVM (estimate the $w$ and $b$ parameters) on the training set and tune the parameter $C$ from $0$ to $100$ by checking classification accuracy on the validation set. Plot the training accuracy versus $C$ curve and validation accuracy versus $C$ curve. Briefly comment on the results. -->\n",
    "\n",
    "### Gradient Descent - Reprise\n",
    "\n",
    "Let's compute the derivatives of the Loss function so we can use Gradient Descent as our method of solving for the weights of SVM. We have two terms on our Loss function, the regularized part and the sum of the errors with the hyperplane\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\omega} \\frac{\\lambda}{2}\\left\\|w\\right\\|_2^2 = \\lambda \\omega\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\omega} {\\left(1 - y_i\\left(w^\\top x_i+b\\right)\\right)_+}  = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            0 & \\quad \\text{if} \\quad y_i\\left(w^\\top x_i+b\\right) \\geq 1 \\\\\n",
    "            -y_ix_i , & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "To understand the process of the gradient, it's divided into 2 parts: the Regularizer and the Hyperplane. When a sample $x_i$ it's correctly classified, we update the vector only by the regularizer, if the sample $x_i$ it's incorrectly misclassified, we update the weights with both the regularizer and the gradient of the plane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.ml.linalg      import Vectors\n",
    "from pyspark.mllib.linalg   import SparseVector, DenseVector\n",
    "\n",
    "from sklearn                import  metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "app_name = \"svm_toy\"\n",
    "master = \"local[*]\"\n",
    "ss = SparkSession.builder\\\n",
    "     .config('spark.executor.memory',       '4G')\\\n",
    "     .config('spark.driver.memory',        '40G')\\\n",
    "     .config('spark.driver.maxResultSize', '10G')\\\n",
    "     .getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "sq = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 s, sys: 50.1 ms, total: 1.22 s\n",
      "Wall time: 9.87 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>[0.052855321300768685, -0.0051331139029484624,...</td>\n",
       "      <td>0.256196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.030798683417690775, 0.0025158333659636...</td>\n",
       "      <td>0.743804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>[0.0, 0.7674005284907951, 0.010063333463854473...</td>\n",
       "      <td>0.256196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>[0.0, -0.0051331139029484624, 0.00503166673192...</td>\n",
       "      <td>0.256196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>[0.052855321300768685, -0.010266227805896925, ...</td>\n",
       "      <td>0.256196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           features   weights\n",
       "0     -1  [0.052855321300768685, -0.0051331139029484624,...  0.256196\n",
       "1      1  [0.0, 0.030798683417690775, 0.0025158333659636...  0.743804\n",
       "2     -1  [0.0, 0.7674005284907951, 0.010063333463854473...  0.256196\n",
       "3     -1  [0.0, -0.0051331139029484624, 0.00503166673192...  0.256196\n",
       "4     -1  [0.052855321300768685, -0.010266227805896925, ...  0.256196"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Let's read the toy dataset. \n",
    "toy_data = ss.read.parquet('../data/train.parquet.toy')\n",
    "\n",
    "# Transform Sparse Vectors to Dense Vectors and transform to Pandas\n",
    "toy_data = toy_data.rdd.map(\n",
    "    lambda x: (x[0], DenseVector(x[1].toArray()), x[2])).toDF(\n",
    "        [\"label\", \"features\", \"weights\"]).toPandas()\n",
    "\n",
    "# Let's replace 0 for -1 in the Label data, so we can use the perceptron\n",
    "toy_data['label'] = toy_data['label'].replace(0,-1)\n",
    "toy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our toy data set into a RDD, with the corresponding form (y, features_array)\n",
    "# We map to a list to be able to use regular RDD commands. We use a helper function to parse the Dataframe\n",
    "# I won't be using the weights factor yet\n",
    "def parse(line):\n",
    "    \"\"\"\n",
    "    Map records from Row --> (tuple,of,fields)\n",
    "    \"\"\"\n",
    "    fields = np.array(line) #Will be added later , dtype = 'float')\n",
    "    features,y = fields[1], fields[0]\n",
    "    return(features, y)\n",
    "\n",
    "toy_dataRDD = sq.createDataFrame(toy_data).rdd.map(parse).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... held out 1649 records for evaluation and assigned 6351 for training.\n"
     ]
    }
   ],
   "source": [
    "# Split 80/20 the data (pseudo)random train/test.\n",
    "trainRDD, heldOutRDD = toy_dataRDD.randomSplit([0.8,0.2], seed = 1)\n",
    "print(f\"\"\"... held out {heldOutRDD.count()} records for evaluation and assigned {trainRDD.count()} for training.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of label\n",
    "meanLabel = trainRDD.map(lambda x: x[1]).mean() \n",
    "\n",
    "# Baseline Model\n",
    "BASELINE = np.append([meanLabel], np.zeros(len(trainRDD.take(1)[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the hinge loss\n",
    "def HingeLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    # We will map the loss calculation to each observation, we calculate difference between the real value of the \n",
    "    # observation and the estimated value given by the weights\n",
    "    loss = augmentedData.map(lambda x: (np.maximum(1-x[1]*(np.matmul(x[0],W)),0))).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Update with Regularization\n",
    "def SVM_GDupdate(dataRDD, W, lr = 0.1, regPar = 0.1, reg = 'l2', kernel = 'linear'):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent update, you can decide kernel or Type of regularization #Work in Progress\n",
    "    Args:\n",
    "        dataRDD  - tuple of (features_array, y)\n",
    "        W        - (array) model coefficients with bias at index 0\n",
    "        lr       - (float) defaults to 0.1\n",
    "        regPar   - (float) defaults to 0.1\n",
    "        reg      - (str) Type of regularization used - defaults to L2, can go to L1\n",
    "        kernel   - (str) type of kernel used, defaults to Linear\n",
    "    Returns:\n",
    "        model   - (array) updated coefficients, bias still at index 0\n",
    "    \"\"\"\n",
    "    # First step, we broadcast the initial weights\n",
    "    w = sc.broadcast(W)\n",
    "    \n",
    "    # Second, let's augment our data\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    \n",
    "    # Helper functions\n",
    "    def l2_grad(line):\n",
    "        \"\"\"\n",
    "        Helper function with the L2 gradient\n",
    "        Args:\n",
    "            w     - Array of old weights to be updated\n",
    "            line  - Observation point tuple (feature_array, Y)\n",
    "        Output:\n",
    "            w_new - New weights\n",
    "        \"\"\"\n",
    "        # From the tuple of observations, get the y and X\n",
    "        y, X = line[1], line[0]\n",
    "        \n",
    "        # The gradient will depend on the misclassification of any given point\n",
    "        if (y*np.dot(X,w.value)) < 1:\n",
    "            grad =  -1*y*X\n",
    "        else:\n",
    "            grad = 0\n",
    "            \n",
    "        # Finally we yield the new total gradient\n",
    "        yield (grad)\n",
    "                \n",
    "    # Let's add to each gradient its penalization, depending of the type of regression       \n",
    "    if reg == 'l2':\n",
    "        # Calculate the batch gradient\n",
    "        grad = augmentedData.flatMap(l2_grad).mean()\n",
    "        # We only regularized features weights\n",
    "        w_nobias = np.append([0.0], W[1:])\n",
    "        # We update the gradient including regularization\n",
    "        grad += regPar*w_nobias\n",
    "       \n",
    "    # Update the Weights\n",
    "    new_model = W-lr*grad\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Algorithm\n",
    "def SVM_GD(trainRDD, testRDD, wInit, nSteps = 20, LR = 0.1, regPar = 0.1, \n",
    "           regType = \"l2\", kernel = \"linear\", verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of regularized gradient descent and \n",
    "    track loss on a train/test set. Return lists of\n",
    "    train/test loss and the models themselves.\n",
    "    \"\"\" \n",
    "    # Initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # Perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    m = sc.broadcast(model)\n",
    "    \n",
    "    for idx in range(nSteps):  \n",
    "        # update the model \n",
    "        model = SVM_GDupdate(trainRDD, m.value, LR, regPar, regType, kernel)\n",
    "        m = sc.broadcast(model)\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(HingeLoss(trainRDD, model))\n",
    "        test_history.append(HingeLoss(testRDD, model))\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {train_history}\")\n",
    "            print(f\"test loss: {test_history}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svml2__results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fe001f278898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m svml2_results = SVM_GD(heldOutRDD, heldOutRDD, wInit, nSteps = 1000, LR = 0.1, \n\u001b[1;32m      5\u001b[0m                                      regType='l2', regPar = 0.1, kernel = \"linear\", verbose=False)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n... trained {len(svml2__results[2])} iterations in {time.time() - start} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrainLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvml2_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPWD\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/svml2_models.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svml2__results' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "wInit = BASELINE\n",
    "start = time.time()\n",
    "svml2_results = SVM_GD(heldOutRDD, heldOutRDD, wInit, nSteps = 1000, LR = 0.1, \n",
    "                                     regType='l2', regPar = 0.1, kernel = \"linear\", verbose=False)\n",
    "print(f\"\\n... trained {len(svml2__results[2])} iterations in {time.time() - start} seconds\")\n",
    "trainLoss, testLoss, models = svml2_results\n",
    "np.savetxt(PWD + '/data/svml2_models.csv', np.array(models), delimiter=',')\n",
    "np.savetxt(PWD + '/data/svml2_loss.csv', np.array([trainLoss, testLoss]), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "svml2_models = np.loadtxt(PWD + '/data/svml2_models.csv', dtype=float, delimiter=',')\n",
    "best_svml2 = svml2_models[-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the toy test dataset. \n",
    "toy_data_test = ss.read.parquet('../data/tests.parquet.toy')\n",
    "\n",
    "# Transform Sparse Vectors to Dense Vectors and transform to Pandas\n",
    "toy_data_test = toy_data_test.rdd.map(\n",
    "    lambda x: (x[0], DenseVector(x[1].toArray()), x[2])).toDF(\n",
    "        [\"label\", \"features\", \"weights\"]).toPandas()\n",
    "\n",
    "# Let's replace 0 for -1 in the Label data, so we can use the perceptron\n",
    "toy_data_test['label'] = toy_data_test['label'].replace(0,-1)\n",
    "\n",
    "# Transform into RDD\n",
    "toy_data_testRDD = sq.createDataFrame(toy_data_test).rdd.map(parse).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification comes from the sign of the the classifier\n",
    "def predict_svm(testRDD, W):\n",
    "    \"\"\"\n",
    "    Perform prediction on test set\n",
    "    Arg:\n",
    "        testRDD:      RDD of (vector_features, y)\n",
    "        W:            array of the best model after GD\n",
    "    Output:\n",
    "        y_predict:    array of predictions\n",
    "    \"\"\"  \n",
    "    # First step, we broadcast the initial weights\n",
    "    w = sc.broadcast(W)\n",
    "    \n",
    "    # Second, let's augment our data\n",
    "    augmentedData = testRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    \n",
    "    # Helper functions\n",
    "    def pred_line(line):\n",
    "        \"\"\"\n",
    "        Helper function for producing the prediction\n",
    "            line  - Observation point tuple (feature_array, Y)\n",
    "        \"\"\"\n",
    "        # From the tuple of observations, get the y and X\n",
    "        y, X = line[1], line[0]\n",
    "        \n",
    "        # Predict the class\n",
    "        y_predict = np.sign(np.dot(X, w.value))\n",
    "            \n",
    "        # Finally we yield the prediction\n",
    "        yield (y_predict)\n",
    "        \n",
    "    # Predict\n",
    "    y_predict = augmentedData.flatMap(pred_line).cache()\n",
    "    \n",
    "    return y_predict\n",
    "\n",
    "# Save the prediction and the real values\n",
    "y_predict = np.array(predict_svm(toy_data_testRDD, best_svml2).collect())\n",
    "y = np.array(toy_data_testRDD.map(lambda x: x[1]).collect())\n",
    "\n",
    "# Calculate AUC\n",
    "metrics.roc_auc_score(y, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
