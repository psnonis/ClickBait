{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "import time    as ti\n",
    "\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets        as widgets\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg         import Vectors\n",
    "from pyspark.ml.feature        import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml                import Pipeline\n",
    "\n",
    "from pyspark.sql               import SparkSession, SQLContext\n",
    "from pyspark.sql.types         import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions     import countDistinct, col, desc\n",
    "\n",
    "from os.path                   import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sparse in module pyspark.ml.linalg:\n",
      "\n",
      "sparse(size, *args)\n",
      "    Create a sparse vector, using either a dictionary, a list of\n",
      "    (index, value) pairs, or two separate arrays of indices and\n",
      "    values (sorted by index).\n",
      "    \n",
      "    :param size: Size of the vector.\n",
      "    :param args: Non-zero entries, as a dictionary, list of tuples,\n",
      "                 or two sorted lists containing indices and values.\n",
      "    \n",
      "    >>> Vectors.sparse(4, {1: 1.0, 3: 5.5})\n",
      "    SparseVector(4, {1: 1.0, 3: 5.5})\n",
      "    >>> Vectors.sparse(4, [(1, 1.0), (3, 5.5)])\n",
      "    SparseVector(4, {1: 1.0, 3: 5.5})\n",
      "    >>> Vectors.sparse(4, [1, 3], [1.0, 5.5])\n",
      "    SparseVector(4, {1: 1.0, 3: 5.5})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Vectors.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---------------------------------+\n",
      "|s01|s02|s03|s04|features                         |\n",
      "+---+---+---+---+---------------------------------+\n",
      "|a1 |b1 |c1 |d4 |(16,[5,6,9,13],[1.0,1.0,1.0,1.0])|\n",
      "|a1 |b2 |c2 |d2 |(16,[3,8,9,10],[1.0,1.0,1.0,1.0])|\n",
      "|a3 |b2 |c3 |d3 |(16,[1,3,4,13],[1.0,1.0,1.0,1.0])|\n",
      "|a4 |b4 |c3 |d4 |(16,[0,1,5,15],[1.0,1.0,1.0,1.0])|\n",
      "+---+---+---+---+---------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s01</th>\n",
       "      <th>s02</th>\n",
       "      <th>s03</th>\n",
       "      <th>s04</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>b1</td>\n",
       "      <td>c1</td>\n",
       "      <td>d4</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a1</td>\n",
       "      <td>b2</td>\n",
       "      <td>c2</td>\n",
       "      <td>d2</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>b2</td>\n",
       "      <td>c3</td>\n",
       "      <td>d3</td>\n",
       "      <td>(0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4</td>\n",
       "      <td>b4</td>\n",
       "      <td>c3</td>\n",
       "      <td>d4</td>\n",
       "      <td>(1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  s01 s02 s03 s04                                           features\n",
       "0  a1  b1  c1  d4  (0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...\n",
       "1  a1  b2  c2  d2  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...\n",
       "2  a3  b2  c3  d3  (0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "3  a4  b4  c3  d4  (1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = spark.createDataFrame([\n",
    "    (\"a1\", \"b1\", \"c1\", \"d4\"),\n",
    "    (\"a1\", \"b2\", \"c2\", \"d2\"),\n",
    "    (\"a3\", \"b2\", \"c3\", \"d3\"),\n",
    "    (\"a4\", \"b4\", \"c3\", \"d4\")\n",
    "], [\"s01\", \"s02\", \"s03\", \"s04\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"s01\", \"s02\", \"s03\", \"s04\"],\n",
    "                       outputCol=\"features\", numFeatures = 16)\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.show(truncate = False)\n",
    "featurized.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-------------------+\n",
      "|s01|s02|s03|s04|features           |\n",
      "+---+---+---+---+-------------------+\n",
      "|a1 |a1 |c1 |d4 |(2,[0,1],[2.0,2.0])|\n",
      "|a1 |a2 |c2 |d2 |(2,[0,1],[2.0,2.0])|\n",
      "|a3 |a2 |c3 |d3 |(2,[0,1],[1.0,3.0])|\n",
      "|a4 |a4 |c3 |d4 |(2,[1],[4.0])      |\n",
      "+---+---+---+---+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s01</th>\n",
       "      <th>s02</th>\n",
       "      <th>s03</th>\n",
       "      <th>s04</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>a1</td>\n",
       "      <td>c1</td>\n",
       "      <td>d4</td>\n",
       "      <td>(2.0, 2.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a1</td>\n",
       "      <td>a2</td>\n",
       "      <td>c2</td>\n",
       "      <td>d2</td>\n",
       "      <td>(2.0, 2.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>a2</td>\n",
       "      <td>c3</td>\n",
       "      <td>d3</td>\n",
       "      <td>(1.0, 3.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4</td>\n",
       "      <td>a4</td>\n",
       "      <td>c3</td>\n",
       "      <td>d4</td>\n",
       "      <td>(0.0, 4.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  s01 s02 s03 s04    features\n",
       "0  a1  a1  c1  d4  (2.0, 2.0)\n",
       "1  a1  a2  c2  d2  (2.0, 2.0)\n",
       "2  a3  a2  c3  d3  (1.0, 3.0)\n",
       "3  a4  a4  c3  d4  (0.0, 4.0)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = spark.createDataFrame([\n",
    "    (\"a1\", \"a1\", \"c1\", \"d4\"),\n",
    "    (\"a1\", \"a2\", \"c2\", \"d2\"),\n",
    "    (\"a3\", \"a2\", \"c3\", \"d3\"),\n",
    "    (\"a4\", \"a4\", \"c3\", \"d4\")\n",
    "], [\"s01\", \"s02\", \"s03\", \"s04\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"s01\", \"s02\", \"s03\", \"s04\"],\n",
    "                       outputCol=\"features\", numFeatures = 2)\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.show(truncate = False)\n",
    "featurized.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initSpark(workingSet):\n",
    "    \n",
    "    workingSet['ss'] = SparkSession.builder \\\n",
    "                                   .config('spark.driver.memory', '240G') \\\n",
    "                                   .getOrCreate()\n",
    "    workingSet['sc'] = workingSet['ss'].sparkContext\n",
    "    workingSet['sq'] = SQLContext(workingSet['sc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(workingSet):\n",
    "\n",
    "    start = ti.time()\n",
    "    \n",
    "    if  not exists('../data/criteo.parquet.full'):\n",
    "\n",
    "        ds = StructType([StructField(f'ctr'    ,  FloatType(), True)                      ] + \\\n",
    "                        [StructField(f'i{f:02}',  FloatType(), True) for f in range(1, 14)] + \\\n",
    "                        [StructField(f's{f:02}', StringType(), True) for f in range(1, 27)])\n",
    "\n",
    "        df = workingSet['sq'].read.format('csv') \\\n",
    "                             .options(header = 'true', delimiter = '\\t') \\\n",
    "                             .schema(ds) \\\n",
    "                             .load('../data/train.txt')\n",
    "\n",
    "        df.write.parquet('../data/criteo.parquet.full')\n",
    "\n",
    "    df = workingSet['ss'].read.parquet('../data/criteo.parquet.full')\n",
    "\n",
    "    workingSet['df_full'    ] = df\n",
    "    workingSet['df_toy'     ] = df.sample(fraction = 0.01, seed = 2019)\n",
    "\n",
    "    workingSet['num_columns'] = [c for c in df.columns if 'i'       in c]\n",
    "    workingSet['cat_columns'] = [c for c in df.columns if 's'       in c]\n",
    "    workingSet['all_columns'] = [c for c in df.columns if 'ctr' not in c]\n",
    "    \n",
    "    print(f'\\nFinished DataFrame Loading in {ti.time()-start:.3f} Seconds\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(workingSet):\n",
    "\n",
    "    start = ti.time()\n",
    "    \n",
    "    if  not exists('../data/criteo.parquet.train') or \\\n",
    "        not exists('../data/criteo.parquet.test' ) or \\\n",
    "        not exists('../data/criteo.parquet.dev'  )    :\n",
    "\n",
    "        train, test, dev = workingSet['df_full'].randomSplit([0.8, 0.1, 0.1], seed = 2019)\n",
    "        \n",
    "        train.write.parquet('../data/criteo.parquet.train')\n",
    "        test.write.parquet('../data/criteo.parquet.test')\n",
    "        dev.write.parquet('../data/criteo.parquet.dev')\n",
    "        \n",
    "    workingSet['df_train'] = workingSet['ss'].read.parquet('../data/criteo.parquet.train')\n",
    "    workingSet['df_test '] = workingSet['ss'].read.parquet('../data/criteo.parquet.test')\n",
    "    workingSet['df_dev'  ] = workingSet['ss'].read.parquet('../data/criteo.parquet.dev')\n",
    "    \n",
    "    print(f'\\nFinished DataFrame Splitting in {ti.time()-start:.3f} Seconds\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df[df.ctr == 1].count() / rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.agg(*[(count(c)/train_total).alias(c) for c in df.columns]).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df[[c for c in df.columns if 'I' in c or 'CTR' in c]].describe().toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(*(countDistinct(col(c)).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rc_1 = df[df.ctr == 1].count()\n",
    "rc_0 = df[df.ctr == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[df.ctr == 1][cat_features]\n",
    "df_0 = df[df.ctr == 0][cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf_1 = df_1.groupBy(['s17']).count().cache()\n",
    "xf_0 = df_0.groupBy(['s17']).count().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xf_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xf_0.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r_1, r_0 in zip(xf_1.collect(),xf_0.collect()):\n",
    "    print(r_1['count']/r_0['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = df.select(cat_features).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = ss.read.parquet('../data/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rc_1 = df_columnar[df_columnar.ctr == 1].count()\n",
    "rc_0 = df_columnar[df_columnar.ctr == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xf_1 = df_1.groupBy(['s17']).count()\n",
    "xf_0 = df_0.groupBy(['s17']).count()\n",
    "xf_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_distinct = {}\n",
    "\n",
    "for f in cat_features:\n",
    "    s  = ti.time()\n",
    "    cat_distinct[f] = pf.agg(countDistinct(f)).collect()[0][0]\n",
    "    print( f'{f} : {cat_distinct[f]:>8} : {ti.time() - s:.3f}' )\n",
    "\n",
    "print( f'sum : {sum(cat_distinct.values()):>8}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_summary_1 = pf[pf.ctr==1].describe( num_features ).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf[pf.ctr==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_summary_0 = pf[pf.ctr==0].describe( num_features ).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_summary.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_summary_1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_summary_0.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in num_features:\n",
    "    pf.groupBy(f).count().select('count').toPandas().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler( withMean = True, withStd = True).fit( features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, \"/usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: x.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = df.fillna({c : '00000000' for c in cat_features })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol = ['i01'], outputCol = ['n01'], withStd = True, withMean = True)\n",
    "model  = scaler.fit(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = ss.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../data;tar xzf criteo.kaggle2014.svm.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "/dev/xvda2       99G   72G   23G  77% /home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "!df -h ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 39G\n",
      "-rw-r--r-- 1 jovyan users 9.2K Nov 20 00:14 ToyData.pkl\n",
      "drwxr-xr-x 2 jovyan users  20K Nov 20 02:30 train_w261.parquet\n",
      "drwxr-xr-x 2 jovyan users  20K Nov 20 02:32 test_w261.parquet\n",
      "drwxr-xr-x 2 jovyan users  20K Nov 20 02:31 dev_w261.parquet\n",
      "drwxr-xr-x 2 jovyan users  20K Nov 25 23:57 criteo.parquet.train\n",
      "drwxr-xr-x 2 jovyan users  20K Nov 25 23:57 criteo.parquet.test\n",
      "drwxr-xr-x 2 jovyan users  20K Nov 25 23:37 criteo.parquet.full\n",
      "drwxr-xr-x 2 jovyan users  24K Nov 25 23:58 criteo.parquet.dev\n",
      "-r--r--r-- 1 jovyan users 3.3G Sep 26  2017 criteo.kaggle2014.test.svm\n",
      "-rwxrwxrwx 1 root   root   11G May 12  2014 train.txt\n",
      "-r--r--r-- 1 jovyan users  25G Sep 26  2017 criteo.kaggle2014.train.svm\n"
     ]
    }
   ],
   "source": [
    "!ls -lhSr ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the columns\n",
    "encoder = OneHotEncoderEstimator(inputCols= cat_features , outputCols=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder.fit(df_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages   = [StringIndexer(inputCol = f, outputCol= f'{f}_index') for f in cat_features]\n",
    "pipeline = Pipeline(stages = stages)\n",
    "model    = pipeline.fit(df_rare)\n",
    "df_indexed       = model.transform(df_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed = df_indexed.drop(*[col for col in df_indexed.columns if 'i' not in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features = [f'{feature}_encoded' for feature in cat_features ]\n",
    "\n",
    "# Encode the columns\n",
    "encoder = OneHotEncoderEstimator(inputCols= cat_features , outputCols=encoded_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rare = df_filled.replace(rb_values['s03'], 'rarebeef', 's03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_rare.take(10), columns = df_rare.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rare.groupBy('s01').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_rare = df_filled.replace(rb_values['s03'], 'rarebeef', 's03')\n",
    "df_rare.groupBy('s03').count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if  not exists('../data/train.parquet.indexed'):\n",
    "\n",
    "    stages   = [StringIndexer(inputCol = f, outputCol= f'{f}_index').setHandleInvalid('keep') for f in cat_columns]\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    model    = pipeline.fit(df)\n",
    "    df       = model.transform(df)\n",
    "\n",
    "    \"\"\"\n",
    "    for c in cat_indexes:\n",
    "        df = df.withColumn(c, col(c).cast('float'))\n",
    "    \n",
    "    df = df.select(['ctr'] + num_columns + cat_indexes)\n",
    "    \"\"\"\n",
    "    df.write.parquet('../data/train.parquet.indexed')\n",
    "    \n",
    "df = ss.read.parquet('../data/train.parquet.indexed')\n",
    "tf = df.sample(fraction = 0.01, seed = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [f'{f}_index' for f in cat_features]:\n",
    "    df = df.withColumn(c, col(c).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('../data/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.read.parquet('../data/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_distinct = {}\n",
    "\n",
    "for f in cat_features:\n",
    "    s  = ti.time()\n",
    "    cat_distinct[f] = df.agg(countDistinct(f)).collect()[0][0]\n",
    "    print( f'{f} : {cat_distinct[f]:>8} : {ti.time() - s:.3f}' )\n",
    "\n",
    "print( f'sum : {sum(cat_distinct.values()):>8}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_distinct = {}\n",
    "\n",
    "for f in cat_features:\n",
    "    s  = ti.time()\n",
    "    cat_distinct[f] = df.agg(countDistinct(f)).collect()[0][0]\n",
    "    print( f'{f} : {cat_distinct[f]:>8} : {ti.time() - s:.3f}' )\n",
    "\n",
    "print( f'sum : {sum(cat_distinct.values()):>8}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "imputer = Imputer(inputCols = num_features, outputCols = num_features)\n",
    "model   = imputer.fit(df)\n",
    "xf      = model.transform(df)\n",
    "xf.describe(num_features).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_assembler = VectorAssembler(inputCols = num_features, outputCol = 'num_features')\n",
    "cat_assembler = VectorAssembler(inputCols = cat_features, outputCol = 'cat_features')\n",
    "xf            = num_assembler.transform(xf)\n",
    "#xf            = cat_assembler.transform(xf)\n",
    "\n",
    "xf.describe(num_features).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
