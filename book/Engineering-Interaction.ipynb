{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Common\n",
       "defined object Engineering\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "class Common {\n",
    "    \n",
    "    import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "\n",
    "    import reflect.io._, Path._\n",
    "\n",
    "    import scala.collection.mutable.HashMap\n",
    "\n",
    "    val frames = new HashMap[String,DataFrame]()\n",
    "\n",
    "    var prefix  : String       = \"data\"\n",
    "    var spark   : SparkSession = null\n",
    "    \n",
    "    val num_features: Array[String] = (1 to 13).map(f => f\"n$f%02d\"         ).toArray\n",
    "    val std_features: Array[String] = (1 to 13).map(f => f\"n$f%02d_standard\").toArray\n",
    "    val cat_features: Array[String] = (1 to 26).map(f => f\"c$f%02d\"         ).toArray\n",
    "    \n",
    "    val cat_uncommon = new HashMap[String,Array[Any]]()\n",
    "    val cat_frequent = new HashMap[String,Array[Any]]()\n",
    "    val cat_distinct = new HashMap[String,Array[Any]]()\n",
    "    \n",
    "    def timePrint(message: String) {\n",
    "        \n",
    "     // import java.time.format.DateTimeFormatter\n",
    "        var timeStamp = java.time.format.DateTimeFormatter.ofPattern(\"HH:mm:ss\").format(java.time.LocalDateTime.now)\n",
    "        println(f\"${timeStamp} : ${message}\")\n",
    "    }\n",
    "   \n",
    "    def setupSpark(application: String = \"w261\", master: String = \"local[*]\", memory: String = \"220G\") {\n",
    "\n",
    "        import org.apache.spark.{SparkContext,SparkConf}\n",
    "        \n",
    "        timePrint(\"Starting Spark Initialization\")\n",
    "\n",
    "        SparkSession.builder.getOrCreate().stop()\n",
    "\n",
    "        spark = SparkSession\n",
    "            .builder()\n",
    "            .master(master)\n",
    "            .appName(application)\n",
    "            .config(\"spark.driver.memory\", memory)\n",
    "            .getOrCreate()\n",
    "\n",
    "        timePrint(\"Stopping Spark Initialization\\n\")\n",
    "    }\n",
    "    \n",
    "    def importData(location: String = \"data\", clean: Boolean = false)\n",
    "    {\n",
    "        import org.apache.spark.sql.types.{StructType,StructField,IntegerType,FloatType,StringType}\n",
    "\n",
    "        timePrint(\"Starting Data Import\")\n",
    "\n",
    "        prefix = f\"$location\"\n",
    "\n",
    "        if (clean) {\n",
    "        }\n",
    "\n",
    "        var frame = f\"whole\"\n",
    "        var train = f\"${location}/${frame}.zip\"\n",
    "        var whole = f\"${location}/${frame}.parquet\"\n",
    "\n",
    "        if (!File(whole).exists) {\n",
    "\n",
    "            var schema = StructType(Seq(StructField(\"label\", IntegerType, true)))\n",
    "            \n",
    "            num_features.foreach(column => {\n",
    "\n",
    "                schema = schema.add(StructField(column,  FloatType, true))\n",
    "            })\n",
    "\n",
    "            cat_features.foreach(column => {\n",
    "\n",
    "                schema = schema.add(StructField(column, StringType, true))\n",
    "            })\n",
    "            \n",
    "            val criteo = spark.read.format(\"csv\")\n",
    "                        .option(\"header\", \"false\")\n",
    "                        .option(\"delimiter\", \"\\t\")\n",
    "                        .schema(schema)\n",
    "                        .load(train)\n",
    "\n",
    "            criteo.write.parquet(whole)\n",
    "        }\n",
    "        \n",
    "        frames(frame) = spark.read.parquet(whole)\n",
    "        \n",
    "        /*\n",
    "        location.toDirectory.dirs.map(_.path)\n",
    "            .filter( name => name matches f\"\"\"$location/(whole|train|valid|tests).parquet.*\"\"\")\n",
    "            .toList.sorted.reverse.foreach ((path) => {\n",
    "\n",
    "            timePrint(f\"Loading -> $path\")\n",
    "\n",
    "            var sub       = path.split(f\"$location/\").last.split(\".parquet\").head\n",
    "            var frame     = path.split(f\"$location/$sub.parquet\").last.stripSuffix(\".\")\n",
    "\n",
    "            frames(f\"$sub.$frame\") = spark.read.parquet(path)\n",
    "\n",
    "        })\n",
    "        */\n",
    "        \n",
    "        timePrint(\"Stopping Data Import\\n\")\n",
    "    }\n",
    "    \n",
    "    def splitsData(ratios: Array[Double] = Array(0.8, 0.1, 0.1)) {\n",
    "\n",
    "        timePrint(\"Starting Data Splits\")\n",
    "        \n",
    "        val splits  = frames(\"whole\").randomSplit(ratios, seed = 2019)\n",
    "        val indexes = Array(\"train\", \"tests\", \"valid\")\n",
    "\n",
    "        for ((split, subset) <- splits zip indexes) {\n",
    "            var path = f\"$prefix/$subset.parquet\"\n",
    "\n",
    "            if  (!path.toDirectory.exists) {\n",
    "                timePrint(f\"Saving -> $path\")\n",
    "                split.write.parquet(path)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        timePrint(\"Stopping Data Splits\\n\")\n",
    "    }\n",
    "}\n",
    "\n",
    "object Engineering extends Common {\n",
    "  \n",
    "    import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "    import org.apache.spark.ml.feature.{ChiSqSelector, Interaction, VectorAssembler, Imputer, StandardScaler}\n",
    "    import org.apache.spark.sql.{DataFrame}\n",
    "    import org.apache.spark.sql.functions.{col}\n",
    "\n",
    "    import reflect.io._, Path._\n",
    "\n",
    "    var iFrame: String = null\n",
    "    var oStage: String = null\n",
    "    var oFrame: String = null\n",
    "    var oModel: String = null\n",
    "    \n",
    "    var num_measures: DataFrame = null\n",
    "    var cat_measures: DataFrame = null\n",
    "\n",
    "    def taskStarting(stage: String, message: String, subset: String, iStage: String): DataFrame = {\n",
    "    \n",
    "        iFrame = f\"$prefix/$subset.parquet.$iStage\"\n",
    "        oStage = f\"$iStage.$stage\"\n",
    "        oFrame = f\"$prefix/$subset.parquet.$oStage\"\n",
    "        oModel = f\"$prefix/model.pickled.$oStage\"\n",
    "\n",
    "        timePrint(f\"Starting $message : iFrame = $iFrame\")\n",
    "        \n",
    "        if (iFrame.toDirectory.exists && !oFrame.toDirectory.exists) {\n",
    "\n",
    "            return spark.read.parquet(iFrame)\n",
    "        }\n",
    "        else {\n",
    "            \n",
    "            return null\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def taskStopping(stage: String, message: String, subset: String, oData: DataFrame) {\n",
    "\n",
    "        if (oData != null) {\n",
    "\n",
    "            oData.write.parquet(oFrame)\n",
    "        }\n",
    "\n",
    "        timePrint(f\"Stopping $message : oFrame = $oFrame\\n\")\n",
    "    }\n",
    "    \n",
    "    def numStandardize (subset: String, iStage: String, fit: Boolean = false) {\n",
    "        \n",
    "        var iData: DataFrame = taskStarting(\"scaled\", \"Numerical Data Standardization\", subset, iStage)\n",
    "        var oData: DataFrame = null\n",
    "        var xPipe: PipelineModel = null\n",
    "\n",
    "        if (iData != null) {\n",
    "\n",
    "            if (fit == true)\n",
    "            {\n",
    "                val imputer   = new Imputer().setInputCols(num_features).setOutputCols(std_features).setStrategy(\"median\")\n",
    "                val assembler = new VectorAssembler().setInputCols(std_features).setOutputCol(\"imp_features\")\n",
    "                var scaler    = new StandardScaler().setInputCol(\"imp_features\").setOutputCol(\"std_features\")\n",
    "                var pipeline  = new Pipeline().setStages(Array(imputer,assembler,scaler))\n",
    "\n",
    "                xPipe = pipeline.fit(iData)\n",
    "                xPipe.write.overwrite().save(oModel)\n",
    "            }\n",
    "            else\n",
    "            {\n",
    "                xPipe = PipelineModel.load(oModel)\n",
    "            }\n",
    "\n",
    "            oData = xPipe.transform(iData)\n",
    "        }\n",
    "\n",
    "        taskStopping(\"scaled\", \"Numerical Data Standardization\", subset, oData)\n",
    "    }\n",
    "\n",
    "    def catMaskUncommon (subset: String, iStage: String, fit: Boolean = false, threshold: Int = 1000) {\n",
    "        \n",
    "        var iData: DataFrame = taskStarting(f\"masked-$threshold\", \"Mask Uncommon Categories\", subset, iStage)\n",
    "        var oData: DataFrame = null\n",
    "\n",
    "        if (iData != null) {\n",
    "\n",
    "            iData  = iData.na.fill(\"deadbeef\", cat_features).cache()\n",
    "            \n",
    "            if (fit == true)\n",
    "            {\n",
    "                val frequent = f\"count >= $threshold\"\n",
    "                val uncommon = f\"count <  $threshold\"\n",
    "                \n",
    "                var frequent_total = 0\n",
    "                var uncommon_total = 0\n",
    "\n",
    "                cat_features.foreach(feature => {\n",
    "\n",
    "                    var count: DataFrame  = iData.select( feature).groupBy(feature).count()\n",
    "                    cat_frequent(feature) = count.filter(frequent).sort(col(\"count\").desc).select(feature).collect.map(row => row(0).asInstanceOf[String])\n",
    "                    cat_uncommon(feature) = count.filter(uncommon).sort(col(\"count\").desc).select(feature).collect.map(row => row(0).asInstanceOf[String])\n",
    "                //  cat_distinct(feature) = cat_uncommon(feature) + cat_frequent(feature)\n",
    "                    \n",
    "                    frequent_total += cat_frequent(feature).size\n",
    "                    uncommon_total += cat_uncommon(feature).size\n",
    "                    \n",
    "                    timePrint(\"$feature > found ${cat_frequent(feature).size}%7d frequent and ${cat_uncommon(feature).size}%7d uncommon > ${frequent_total}%5d total frequent\")\n",
    "                })\n",
    "            }\n",
    "            else\n",
    "            {\n",
    "                \n",
    "            }\n",
    "        }\n",
    "\n",
    "        taskStopping(f\"masked-$threshold\", \"Mask Uncommon Categories\", subset, oData)\n",
    "    }    \n",
    "    \n",
    "    def allJoinInteract (subset: String, iStage: String) {\n",
    "\n",
    "        var iData: DataFrame = taskStarting(\"action\", \"Numerical vs Categorical Interactions\", subset, iStage)\n",
    "        var oData: DataFrame = null\n",
    "        \n",
    "        if (iData != null) {\n",
    "            \n",
    "            \n",
    "            val interaction = new Interaction()\n",
    "                .setInputCols(Array(\"std_features\", \"cat_features\"))\n",
    "                .setOutputCol(\"cxn_features\")\n",
    "\n",
    "            oData = interaction.transform(iData)\n",
    "        }\n",
    "\n",
    "        taskStopping(\"action\", \"Numerical vs Categorical Interactions\", subset, oData)\n",
    "    }\n",
    "\n",
    "    def allPackFeatures (subset: String, iStage: String) {\n",
    "\n",
    "        var iData: DataFrame = taskStarting(\"packed\", \"Final Feature Pack\", subset, iStage)\n",
    "        var oData: DataFrame = null\n",
    "        \n",
    "        if (iData != null) {\n",
    "            val assembler = new VectorAssembler()\n",
    "                .setInputCols(Array(\"std_features\", \"top_features\", \"cxn_features\"))\n",
    "                .setOutputCol(\"features\")\n",
    "            \n",
    "            oData = assembler.transform(iData)\n",
    "        }\n",
    "\n",
    "        taskStopping(\"packed\", \"Final Feature Pack\", subset, oData)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:52:27 : Starting Spark Initialization\n",
      "15:52:29 : Stopping Spark Initialization\n",
      "\n",
      "15:52:29 : Starting Data Import\n",
      "15:52:32 : Stopping Data Import\n",
      "\n",
      "15:52:32 : Starting Data Splits\n",
      "15:52:32 : Stopping Data Splits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Engineering.setupSpark(application = \"prep\")\n",
    "Engineering.importData(location = \"data\")\n",
    "Engineering.splitsData(ratios = Array(0.8, 0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:21:26 : Starting Numerical vs Categorical Interactions : iFrame = data/train.parquet.normed.filled.masked-060000.encode.picked-000987\n",
      "00:21:26 : Stopping Numerical vs Categorical Interactions : oFrame = data/train.parquet.normed.filled.masked-060000.encode.picked-000987.action\n",
      "\n",
      "00:21:26 : Starting Numerical vs Categorical Interactions : iFrame = data/tests.parquet.normed.filled.masked-060000.encode.picked-000987\n",
      "00:21:26 : Stopping Numerical vs Categorical Interactions : oFrame = data/tests.parquet.normed.filled.masked-060000.encode.picked-000987.action\n",
      "\n",
      "00:21:26 : Starting Numerical vs Categorical Interactions : iFrame = data/valid.parquet.normed.filled.masked-060000.encode.picked-000987\n",
      "00:21:26 : Stopping Numerical vs Categorical Interactions : oFrame = data/valid.parquet.normed.filled.masked-060000.encode.picked-000987.action\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Engineering.allJoinInteract(subset = \"train\", iStage = \"normed.masked-060000.encode.picked-000987\")\n",
    "Engineering.allJoinInteract(subset = \"tests\", iStage = \"normed.masked-060000.encode.picked-000987\")\n",
    "Engineering.allJoinInteract(subset = \"valid\", iStage = \"normed.masked-060000.encode.picked-000987\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
