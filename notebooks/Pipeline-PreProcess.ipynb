{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from code.common import workingSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Starting Spark Initializing\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Finished Spark Initializing in 3.009 Seconds\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from code.common import initSpark\n",
    "\n",
    "initSpark(workingSet, application = 'prep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Starting Data Loading\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Finished Data Loading in 5.439 Seconds\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from code.common import loadData\n",
    "\n",
    "loadData(workingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training, Test, and Dev Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Starting Data Splitting\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Finished Data Splitting in 3.513 Seconds\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from code.common import splitData\n",
    "\n",
    "splitData(workingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering : Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, col, when, isnan, count, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df           = workingSet['df_train'    ]\n",
    "cat_features = workingSet['cat_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df.agg(*(countDistinct(col(c)).alias(c) for c in df.columns)).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_filled = df.fillna('deadbeef', cat_features).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rb_thresh = 360000\n",
    "rb_values = {}\n",
    "\n",
    "for feature in cat_features :\n",
    "    df_counts          = df_filled.groupBy(feature).count()\n",
    "    rb_values[feature] = df_counts.filter(df_counts['count'] < rb_thresh).select(feature).rdd.flatMap(list).collect()\n",
    "\n",
    "    print(feature, f'found {len(rb_values[feature]):>8} rare categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rare = df_filled\n",
    "\n",
    "for feature, rare_categories in rb_values.items():\n",
    "    df_rare = df_rare.replace(rare_categories, 'rarebeef', feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rare = df_filled.replace(rb_values['s03'], 'rarebeef', 's03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_rare.take(10), columns = df_rare.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rare.groupBy('s01').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_rare = df_filled.replace(rb_values['s03'], 'rarebeef', 's03')\n",
    "df_rare.groupBy('s03').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if  not exists('../data/train.parquet.indexed'):\n",
    "\n",
    "    stages   = [StringIndexer(inputCol = f, outputCol= f'{f}_index').setHandleInvalid('keep') for f in cat_columns]\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    model    = pipeline.fit(df)\n",
    "    df       = model.transform(df)\n",
    "\n",
    "    \"\"\"\n",
    "    for c in cat_indexes:\n",
    "        df = df.withColumn(c, col(c).cast('float'))\n",
    "    \n",
    "    df = df.select(['ctr'] + num_columns + cat_indexes)\n",
    "    \"\"\"\n",
    "    df.write.parquet('../data/train.parquet.indexed')\n",
    "    \n",
    "df = ss.read.parquet('../data/train.parquet.indexed')\n",
    "tf = df.sample(fraction = 0.01, seed = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [f'{f}_index' for f in cat_features]:\n",
    "    df = df.withColumn(c, col(c).cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('../data/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.read.parquet('../data/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_distinct = {}\n",
    "\n",
    "for f in cat_features:\n",
    "    s  = ti.time()\n",
    "    cat_distinct[f] = df.agg(countDistinct(f)).collect()[0][0]\n",
    "    print( f'{f} : {cat_distinct[f]:>8} : {ti.time() - s:.3f}' )\n",
    "\n",
    "print( f'sum : {sum(cat_distinct.values()):>8}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_distinct = {}\n",
    "\n",
    "for f in cat_features:\n",
    "    s  = ti.time()\n",
    "    cat_distinct[f] = df.agg(countDistinct(f)).collect()[0][0]\n",
    "    print( f'{f} : {cat_distinct[f]:>8} : {ti.time() - s:.3f}' )\n",
    "\n",
    "print( f'sum : {sum(cat_distinct.values()):>8}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "imputer = Imputer(inputCols = num_features, outputCols = num_features)\n",
    "model   = imputer.fit(df)\n",
    "xf      = model.transform(df)\n",
    "xf.describe(num_features).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_assembler = VectorAssembler(inputCols = num_features, outputCol = 'num_features')\n",
    "cat_assembler = VectorAssembler(inputCols = cat_features, outputCol = 'cat_features')\n",
    "xf            = num_assembler.transform(xf)\n",
    "#xf            = cat_assembler.transform(xf)\n",
    "\n",
    "xf.describe(num_features).toPandas().T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
