{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team 14   \n",
    "Brian Musisi, Pri Nonis, Vinicio Del Sola, Laura Chutny   \n",
    "Fall 2019, Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advertisers on websites make money when people click on an ad, visit the advertiser's site and then purchase something. This means that understanding the rate (or probability) at which people click on an ad is important - higher 'click-through' rates have the potential for more revenue. This study will not address the next step, which is how an advertiser converts a person who has 'clicked-through' to their site into a paying customer. Instead, our question is how to predict the click through rate for a given (unseen) ad based on the training data supplied to the model. In other words, for a given ad, what is the probability that a person will click on the ad. Ads cost money, so advertisers need to know which ads will generate more clicks and thus which ads are more valuable to the advertiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem - a 'positive' result (1) if the ad is clicked on and a 'negative' result (0) if the ad is not clicked on. There is a very large imbalance between classes - far more impressions (views of the ad) with no click (0) than impressions which result in a click (1). In this instance we need to decide between false positives (type 1 error - where we predict a click that did not actually happen) and false negatives (type 2 errors - where we do not predict a click when there actually was one). Because advertisers pay more for ads that are clicked, we want to be conservative in our predictions, and avoid false positives.\n",
    "\n",
    "Note that Click Through Rate is defined as the number of ads that are clicked on as a fraction of the total impressions of that ad that are viewed. In this case, each example in the dataset is an impression of the ad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training Algorithm Choices:  \n",
    "- Logistic Regression\n",
    "- Decision Tree / Forest\n",
    "- Factorization Machine\n",
    "\n",
    "2. Loss function:\n",
    "- Log Loss (Cross Entropy)\n",
    "- Exponential Loss\n",
    "- Hinge Loss (as a proxy for 0/1 loss) - can't be used with general gradient descent as it is not differentiable for all x, but can be used with subgradients which are locally differentiable\n",
    "\n",
    "3. Hyperparameter tuning\n",
    "\n",
    "4. Evaluation Metric\n",
    "- Accuracy is not a good metric - we could have excellent accuracy by correctly predicting 100% of the test examples as 0, while the true number might be 96% - so we would have a great accuracy of 96, but in actual fact, we would have missed out predicting the actual positive values (1).\n",
    "- With a goal of limiting the false positives, precision (TP/(TP + FP)) will work well. We would trade this off with minimizing the number of false negatives (sensitivity (recall): TP/(TP+FN)). \n",
    "- Or we could combine them to optimize the most precision with the best sensitivity by using the F-score: 2\\*((P\\*S)/(P+S))\n",
    "- An average precision - the area under the precision recall curve (AUC) helps give better inference than just a single F score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "import time    as ti\n",
    "\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets        as widgets\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg         import Vectors\n",
    "from pyspark.ml.feature        import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml                import Pipeline\n",
    "\n",
    "from pyspark.sql               import SparkSession, SQLContext\n",
    "from pyspark.sql.types         import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions     import countDistinct, col, desc\n",
    "\n",
    "from os.path                   import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initSpark(workingSet):\n",
    "    \n",
    "    workingSet['ss'] = SparkSession.builder \\\n",
    "                                   .config('spark.driver.memory', '240G') \\\n",
    "                                   .getOrCreate()\n",
    "    workingSet['sc'] = workingSet['ss'].sparkContext\n",
    "    workingSet['sq'] = SQLContext(workingSet['sc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(workingSet):\n",
    "\n",
    "    start = ti.time()\n",
    "    \n",
    "    if  not exists('../data/criteo.parquet.full'):\n",
    "\n",
    "        ds = StructType([StructField(f'ctr'    ,  FloatType(), True)                      ] + \\\n",
    "                        [StructField(f'i{f:02}',  FloatType(), True) for f in range(1, 14)] + \\\n",
    "                        [StructField(f's{f:02}', StringType(), True) for f in range(1, 27)])\n",
    "\n",
    "        df = workingSet['sq'].read.format('csv') \\\n",
    "                             .options(header = 'true', delimiter = '\\t') \\\n",
    "                             .schema(ds) \\\n",
    "                             .load('../data/train.txt')\n",
    "\n",
    "        df.write.parquet('../data/criteo.parquet.full')\n",
    "\n",
    "    df = workingSet['ss'].read.parquet('../data/criteo.parquet.full')\n",
    "\n",
    "    workingSet['df_full'    ] = df\n",
    "    workingSet['df_toy'     ] = df.sample(fraction = 0.01, seed = 2019)\n",
    "\n",
    "    workingSet['num_columns'] = [c for c in df.columns if 'i'       in c]\n",
    "    workingSet['cat_columns'] = [c for c in df.columns if 's'       in c]\n",
    "    workingSet['all_columns'] = [c for c in df.columns if 'ctr' not in c]\n",
    "    \n",
    "    print(f'\\nFinished DataFrame Loading in {ti.time()-start:.3f} Seconds\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished DataFrame Loading in 0.126 Seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "workingSet = {}\n",
    "\n",
    "initSpark(workingSet)\n",
    "loadData(workingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 80/10/10 split Training/Dev/Test\n",
    "- Discuss Schema - numerical variables (and normalization); Character variables and (indexing? or whatever else we do with them)\n",
    "- Distribution of values - mean, median, skewness\n",
    "- number of NaNs and what our approach is\n",
    "- Feature Engineering - how to increase/reduce features and implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Split\n",
    "Data was split before processing and stored using parquet files. An 80/10/10 train/dev/test split was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(workingSet):\n",
    "\n",
    "    start = ti.time()\n",
    "    \n",
    "    if  not exists('../data/criteo.parquet.train') or \\\n",
    "        not exists('../data/criteo.parquet.test' ) or \\\n",
    "        not exists('../data/criteo.parquet.dev'  )    :\n",
    "\n",
    "        train, test, dev = workingSet['df_full'].randomSplit([0.8, 0.1, 0.1], seed = 2019)\n",
    "        \n",
    "        train.write.parquet('../data/criteo.parquet.train')\n",
    "        test.write.parquet('../data/criteo.parquet.test')\n",
    "        dev.write.parquet('../data/criteo.parquet.dev')\n",
    "        \n",
    "    workingSet['df_train'] = workingSet['ss'].read.parquet('../data/criteo.parquet.train')\n",
    "    workingSet['df_test '] = workingSet['ss'].read.parquet('../data/criteo.parquet.test')\n",
    "    workingSet['df_dev'  ] = workingSet['ss'].read.parquet('../data/criteo.parquet.dev')\n",
    "    \n",
    "    print(f'\\nFinished DataFrame Splitting in {ti.time()-start:.3f} Seconds\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished DataFrame Splitting in 0.305 Seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitData(workingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Numerical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic statistics for all the numerical variables was first run and reported, as shown in the attached table. Median and skewness were added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "      <th>Pearson2Skew</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i01</th>\n",
       "      <td>20035247</td>\n",
       "      <td>3.500060</td>\n",
       "      <td>9.427662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5775.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.795550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i02</th>\n",
       "      <td>36673203</td>\n",
       "      <td>105.881091</td>\n",
       "      <td>391.940275</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>257675.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i03</th>\n",
       "      <td>28802877</td>\n",
       "      <td>26.910415</td>\n",
       "      <td>396.797641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65535.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.150533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i04</th>\n",
       "      <td>28723574</td>\n",
       "      <td>7.322778</td>\n",
       "      <td>8.799146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>969.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.132875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i05</th>\n",
       "      <td>35725965</td>\n",
       "      <td>18545.307976</td>\n",
       "      <td>69435.515294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23159456.0</td>\n",
       "      <td>2868.0</td>\n",
       "      <td>0.677347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i06</th>\n",
       "      <td>28469401</td>\n",
       "      <td>116.147388</td>\n",
       "      <td>391.380525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>431037.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.629674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i07</th>\n",
       "      <td>35085776</td>\n",
       "      <td>16.322690</td>\n",
       "      <td>65.524064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34536.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.564191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i08</th>\n",
       "      <td>36654939</td>\n",
       "      <td>12.517513</td>\n",
       "      <td>16.816877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6047.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.805889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i09</th>\n",
       "      <td>35085776</td>\n",
       "      <td>106.106119</td>\n",
       "      <td>220.289905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29019.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.900261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i10</th>\n",
       "      <td>20035247</td>\n",
       "      <td>0.617435</td>\n",
       "      <td>0.683969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.677994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i11</th>\n",
       "      <td>35085776</td>\n",
       "      <td>2.732185</td>\n",
       "      <td>5.196922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i12</th>\n",
       "      <td>8615961</td>\n",
       "      <td>0.991808</td>\n",
       "      <td>5.672792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i13</th>\n",
       "      <td>28723574</td>\n",
       "      <td>8.219770</td>\n",
       "      <td>16.413245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7393.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.588507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctr</th>\n",
       "      <td>36673203</td>\n",
       "      <td>0.256198</td>\n",
       "      <td>0.436533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count          mean        stddev  min         max  median  \\\n",
       "variable                                                                  \n",
       "i01       20035247      3.500060      9.427662  0.0      5775.0     1.0   \n",
       "i02       36673203    105.881091    391.940275 -3.0    257675.0     3.0   \n",
       "i03       28802877     26.910415    396.797641  0.0     65535.0     7.0   \n",
       "i04       28723574      7.322778      8.799146  0.0       969.0     4.0   \n",
       "i05       35725965  18545.307976  69435.515294  0.0  23159456.0  2868.0   \n",
       "i06       28469401    116.147388    391.380525  0.0    431037.0    34.0   \n",
       "i07       35085776     16.322690     65.524064  0.0     34536.0     4.0   \n",
       "i08       36654939     12.517513     16.816877  0.0      6047.0     8.0   \n",
       "i09       35085776    106.106119    220.289905  0.0     29019.0    40.0   \n",
       "i10       20035247      0.617435      0.683969  0.0        10.0     1.0   \n",
       "i11       35085776      2.732185      5.196922  0.0       231.0     1.0   \n",
       "i12        8615961      0.991808      5.672792  0.0      4008.0     0.0   \n",
       "i13       28723574      8.219770     16.413245  0.0      7393.0     5.0   \n",
       "ctr       36673203      0.256198      0.436533  0.0         1.0     NaN   \n",
       "\n",
       "          Pearson2Skew  \n",
       "variable                \n",
       "i01           0.795550  \n",
       "i02           0.787475  \n",
       "i03           0.150533  \n",
       "i04           1.132875  \n",
       "i05           0.677347  \n",
       "i06           0.629674  \n",
       "i07           0.564191  \n",
       "i08           0.805889  \n",
       "i09           0.900261  \n",
       "i10          -1.677994  \n",
       "i11           0.999930  \n",
       "i12           0.524508  \n",
       "i13           0.588507  \n",
       "ctr                NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "num_stats=pd.read_csv('../notebooks/num_stats.csv', index_col='variable')\n",
    "num_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/statsmodels/nonparametric/kernels.py:128: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (1. / np.sqrt(2 * np.pi)) * np.exp(-(Xi - x)**2 / (h**2 * 2.))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the basic statistics up - the following steps to analysing the numerical variables were taken:  \n",
    "1) Plot histograms and scatter plots of each variable, along with a box plot and violin plot of a 10% sample of each variable.  \n",
    "2) Determine the distribution of each variable  \n",
    "3) Apply a standardization for each variable.  \n",
    " \n",
    "Variables are standardized not to create 'normal' variables, but to bring the values into a region of approximately (-1,3) so that machine learning techniques could be applied. The following table shows the variable, the distribution as observed, and the standardization applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Numerical Variable | Distribution Type          | Standardization          |\n",
    "|--------------------|----------------------------|--------------------------|\n",
    "| i01                | Exponentially Decreasing   | i01' = i01/(2*SD)        |\n",
    "| i02                | Truncated Skewed Normal    | i02' = (i02 - median)/SD |\n",
    "| i03                | Exponentially Decreasing   | i03' = i03/SD            |\n",
    "| i04                | Truncated Skewed Normal    | i04' = (i04-median)/SD   |\n",
    "| i05                | Truncated Skewed Normal    | i05' = (i05-median)/SD   |\n",
    "| i06                | Exponentially Decreasing   | i06' = i06/2*SD          |\n",
    "| i07                | Exponentially Decreasing   | i07' = i07/2*SD          |\n",
    "| i08                | Exponentially Decreasing   | i08' = i08/2*SD          |\n",
    "| i09                | Truncated Skewed Normal    | i09' = (i09-median)/SD   |\n",
    "| i10                | Sigmoid                    | i10' = i10/Max(i10)      |\n",
    "| i11                | Truncated Skewed Normal    | i11' = (i11-median)/SD   |\n",
    "| i12                | Exponentially Decreasing   | i12' = i12/2*SD          |\n",
    "| i13                | Truncated Skewed Normal    | i13' = (i13-median)/SD   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f3c903778949e38521625308717553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs  = []\n",
    "\n",
    "def featureAnalysisNumerical(df, feature):\n",
    "\n",
    "    output = widgets.Output()\n",
    "    data   = df[~np.isnan(df[feature])]\n",
    "    y      = data['ctr']\n",
    "    x      = data[feature]\n",
    "    \n",
    "    xmax   = max(x)\n",
    "    \n",
    "    with output:\n",
    "        fig = plt.figure(figsize = (28, 7))\n",
    "\n",
    "        ax1 = fig.add_subplot(1, 4, 1)\n",
    "        ax1 = sns.boxplot(x)\n",
    "      # ax1.set(xlim=(-1,40))\n",
    "        \n",
    "        ax2 = fig.add_subplot(1, 4, 2)\n",
    "        ax2 = sns.violinplot(x)\n",
    "      # ax2.set(xlim=(-1,40))\n",
    "\n",
    "        ax3 = fig.add_subplot(1, 4, 3)\n",
    "        ax3 = sns.distplot(x, hist = True, color = 'red')\n",
    "      # ax3.set(xlim=(-1,40))\n",
    "\n",
    "        ax4 = fig.add_subplot(1, 4, 4)\n",
    "        ax4 = sns.scatterplot(x, y, data = df, hue = 'ctr')\n",
    "\n",
    "        plt.show(fig)\n",
    "        \n",
    "    return [output]\n",
    "\n",
    "pf  = workingSet['df_toy'].sample(fraction = 0.001, seed = 2019).cache().toPandas()\n",
    "tab = widgets.Tab()\n",
    "for n, feature in enumerate(workingSet['num_columns']):\n",
    "    outputs += featureAnalysisNumerical(pf, feature)\n",
    "    tab.set_title(n, feature)\n",
    "\n",
    "tab.children = outputs\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Toy example with hand calculation/ simple code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. parallel implementation using MLLib (or similar)\n",
    "- challenges\n",
    "- validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
