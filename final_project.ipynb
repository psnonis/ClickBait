{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Click-through Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team 14   \n",
    "Brian Musisi, Pri Nonis, Vinicio Del Sola, Laura Chutny   \n",
    "Fall 2019, Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Question\n",
    "\n",
    "Advertisers on websites make money when people click on an ad, visit the advertiser's site and then purchase something. This means that understanding the rate (or probability) at which people click on an ad is important - higher 'click-through' rates have the potential for more revenue. This study will not address the next step, which is how an advertiser converts a person who has 'clicked-through' to their site into a paying customer. Instead, our question is how to predict the click through rate for a given (unseen) ad based on the training data supplied to the model. In other words, for a given ad, what is the probability that a person will click on the ad? Ads cost money, so advertisers need to know which ads will generate more clicks and thus which ads are more valuable to the advertiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data\n",
    "\n",
    "In 2014, Criteo introduced a Click Through Rate challenge on Kaggle [**2**], using a set of ads served by Criteo[**1**]. Each line is the serving of one ad and the training set comes from a period of 7 days. They are labeled as 1 if the ad was clicked on and 0 if the ad was not. The data was subsampled before being placed into the competition, so the dataset is smaller, and the percentage of '1's are higher than in the original data. This is the dataset we will use for the analysis in this report. Note that we will not submit to the Kaggle competition and we are not using their testing data, only the training set. Click Through Rate is defined as the number of ads that are clicked on as a fraction of the total impressions of ads that are viewed. In this case, each example in the dataset is an impression of the ad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem - a 'positive' result (1) if the ad is clicked on and a 'negative' result (0) if the ad is not clicked on. There is a large imbalance between classes - far more impressions (views of the ad) with no click (0) than impressions which result in a click (1). In this instance we need to decide between false positives (type 1 error - where we predict a click that did not actually happen) and false negatives (type 2 errors - where we do not predict a click when there actually was one). Because advertisers pay more for ads that are clicked, we want to be conservative in our predictions, and avoid false positives.\n",
    "\n",
    "The data itself is a combination of numerical and categorical features. Each row contains the label ($y$) which is either 1, or 0. Next are 13 numerical variables, i01-i13, which contain positive, negative and zero valued integers. Finally, there are 26 columns of categorical features - c1-c26. The values of these features have been hashed onto 32 bits for anonymization purposes. We have no descriptors of the data columns or features, and no indication if it was originally ranked data or not, and thus we have chosen to approach it as if unranked. All decisions in feature engineering are made purely from the frequency and relative importance of the feature values, as there is no subject matter expertise to inform feature selection. There are 45,842,555 total examples in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Evaluation Metric   \n",
    "\n",
    "Finally, we must consider how we will evaluate the performance of our model(s). For evaulation of binary classification, especially with the (potentially) highly skewed values in a click-through rate problem, accuracy is not a good measure. For example, we could have excellent accuracy by predicting 100% of the test examples as 0, while the true number might be 96% - so we would have a great accuracy of 96%, but in actual fact, we would have missed out predicting the actual positive values in the remaining 4%. \n",
    "\n",
    "Note that below, TP = True Positive, FP = False Positive, TN = True Negative and FN = False Negative.  \n",
    "\n",
    "Instead, if we set a goal of limiting the false positive predictions, precision, defined as $P = \\dfrac{TP}{(TP + FP)}$ will work well. We would trade gains in precision with minimizing the number of false negatives, otherwise known as sensitivity or recall: $R = \\dfrac{TP}{(TP + FN)}$. Luckily for us, there is a nice metric that is defined as the 'F-score' which combines the optimized precision and sensitivity, and we get: $F = 2\\cdot \\dfrac{P\\cdot R}{P+R}$. F-score would be a perfectly acceptable metric to use, except it is a single number.\n",
    "\n",
    "When we are optimizing over hyperparameters, we would like to understand a little better what the optimum is for different ratios of true and false positives. If we plot the way the true and false positive rates change for a given model, we get what is known as a 'receiver operating characteristic curve' (or ROC curve) as shown in Figure 1. The ROC plots the true-positive rate against the false-positive rate. As the ROC becomes more concave, the achievable true positive rate for a given false positive rate increases, as does the area under the curve (AUC). In the following diagram, curve 'A' has a larger AUC and as can be seen, a higher true positive rate for a given false positive rate.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"assets/ROC.png\" width=\"350\">\n",
    "    <figcaption> <b>Figure 1. Receiver Operating Characteristic Curve [3]</b> </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Another way to say this, as summarized on Wikipedia [__4__], is that the AUC is \"equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one\".  The 2nd place winner of the Criteo Kaggle competition had an AUC of 0.8097 [__5__], also another attempt at prediction on the Criteo dataset reported an AUC of 0.79 [__6__].  Given this data, we believe that obtaining an AUC of 0.75 or better would be a really useful result for advertisers.\n",
    "See [**7**] for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__References - Section 1__  \n",
    "\n",
    "[1] “Kaggle Display Advertising Challenge Dataset,” Criteo Labs, 10-Feb-2014. [Online]. Available: https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/. [Accessed: 11-Dec-2019].  \n",
    "[2] “Display Advertising Challenge.” [Online]. Available: https://kaggle.com/c/criteo-display-ad-challenge. [Accessed: 11-Dec-2019].  \n",
    "[3] M. E. Cross and E. V. E. Plunkett, “Receiver operating characteristic curve,” in Physics, Pharmacology and Physiology for Anaesthetists: Key Concepts for the FRCA, 2nd ed., Cambridge: Cambridge University Press, 2014, pp. 369–369.  \n",
    "[4] “Receiver Operating Characteristic,” wikipedia.org, Nov-2019. [Online]. Available: https://en.wikipedia.org/wiki/Receiver_operating_characteristic.<br/>\n",
    "[5] M. Jahrer, “Can anyone tell me the AUC of benchmark model ?,” kaggle.com, 2014. [Online]. Available: https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/9821.<br/>\n",
    "[6] wormhole developers, “Binary Classification on the Criteo CTR Dataset,” wormhole.readthedocs.io, 2015. [Online]. Available: https://wormhole.readthedocs.io/en/latest/tutorial/criteo_kaggle.html.<br/>\n",
    "[7] S. Narkhede, “Understanding AUC - ROC Curve,” Medium, 26-May-2019. [Online]. Available: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5. [Accessed: 11-Dec-2019].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Algorithm - Support Vector Machine  \n",
    "There are many possible choices of algorithm for binary classification problems. Logistic Regression, Decision Tree / Decision Tree Forest, Support Vector Machine (SVM), and Factorization Machine have each proven to be worthy choices. In this project, for our toy model and main algorithm, we decided to focus on the SVM, as this algorithm had not been focused on in the original Kaggle Criteo competition. We will also investigate the performance on many algorithms.\n",
    "\n",
    "The Support Vector Machine is an algorithm that divides a set of data into two classes. In 2 dimensions, we can think of this as a line separating the classes. What the SVM allows us to do is extend this concept to different dimensions (i.e. feature space) and create a 'best-fit' hyperplane to separate the classes. The transformation to a different dimensional space is called a kernel.\n",
    "\n",
    "The prediction of the class (where class is redefined from $y_i \\in \\{0, 1\\}$ to $y_i \\in \\{-1, 1\\}$) in this case will be done with a linear kernel - i.e. a linear hyperplane where the hyperplane equation is $\\left(w^\\top x_i+b\\right)$:  \n",
    "\n",
    "$$ class =  y_i\\left(w^\\top x_i +b\\right)_+ $$  \n",
    "\n",
    "where:  \n",
    "\n",
    "$$\n",
    "class  = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1, & \\quad \\text{if} \\quad \\left(w^\\top x_i+b\\right) \\geq 1 \\\\\n",
    "            -1, & \\quad \\text{if} \\quad \\left(w^\\top x_i+b\\right) \\leq -1\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "The goal in Support Vector Machines is to maximize the distance between the hyperplane and any of the examples, in otherwords, maximize the 'margin' [**1**]. The margin can be thought of in two dimensions as shown in the Figure 2:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"assets/SVM_Margin.png\" width=\"350\">\n",
    "    <figcaption> <b>Figure 2. Support Vector Machine Margin [2]</b> </figcaption>\n",
    "</figure>\n",
    "\n",
    "From trigonometry and vector algebra, we can compute the distance between the two margins by adding the perpendicular distances from the positive and negative margins to the hyperplane that solves $\\left(w^\\top x_i+b\\right)=0$. This distance, the margin, we will denote as $\\frac{2}{\\parallel w\\parallel }$. This allows us to say that in order to maximize the margin, we must minimize $\\parallel w\\parallel$. In order to maintain differentiability, we will actually minimize $\\parallel w\\parallel^2$, because the same value of $w$ satisifies both minimizations. Note that the edges of the margins are defined by what we call the 'support' vectors, hence the name for the algorithm. Points away from the edges do not contribute to the solution.\n",
    "\n",
    "This allows us to write what is called the 'hard-margin' Support Vector Machine. The objective function is:\n",
    "\n",
    "$$\\underset{w}{min}(J\\text{'}(w))=\\underset{}{\\underset{w}{min}(\\frac{\\lambda }{2})\\parallel w{\\parallel }^{2}}$$   \n",
    "such that   $$y_i\\left(w^\\top x_i +b\\right)\\ge 1$$  \n",
    "\n",
    "This is called a 'hard-margin' SVM, because the data have to be on one or the other side of our hyperplane margins. Unfortunately, the world is not usually that kind - we may be able to separate a large amount of our data with the hyperplane, but there will still be some samples that just aren't classified correctly, no matter where we place the hyperplane. For this reason, we need to allow some 'slack', or 'softness' in the margin. In order to achieve this we introduce the Hinge Loss function for the Soft Margin SVM:\n",
    "\n",
    "$${\\mathscr{L}}^{Hinge}=max\\left[0,\\left(1-{y}_{i}\\left({w}^{T}{x}_{i}+b\\right)\\right)\\right]={\\left(1 - y_i\\left(w^\\top x_i +b\\right)\\right)_+}$$\n",
    "\n",
    "This allows us to assign a loss of zero for correctly classified points far away from the boundary of the margin, but to assign larger errors to points within the margin or that are incorrectly classified [**3**]. This is best understood with Figure 3:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"assets/loss-hinge.png\" width=\"350\">\n",
    "    <figcaption> <b>Figure 3. Hinge Loss [4]</b> </figcaption>\n",
    "</figure>\n",
    "\n",
    "Finally, we combine the loss function with our objective function to get our final objective for minimzation in the soft margin SVM (and we take the average of the losses over all the examples in the dataset):\n",
    "\n",
    "$$\\underset{w,b}{min}(J(w,b))=\\underset{w, b}{\\text{min }} \\frac{\\lambda}{2}\\left\\|w\\right\\|_2^2 + \\frac{1}{n}\\sum_{i=1}^{N}{\\left(1 - y_i\\left(w^\\top x_i +b\\right)\\right)_+}$$\n",
    "\n",
    "Interestingly, we can look at the above equation in the same way as we see a cost function with regularizer, and think of $\\lambda$ as the regularization paramter here.\n",
    "\n",
    "## 2.2 Gradient Descent  \n",
    "\n",
    "In order to solve for our weights and bias ($w$,$b$) that minimize our objective function, we will use gradient descent. In order to determine the update rule, we must take the derivative of the function. This is written in two parts (the derivative of a sum is the sum of derivatives):\n",
    "\n",
    "First the regularizer, or margin part:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\omega} \\frac{\\lambda}{2}\\left\\|w\\right\\|_2^2 = \\lambda w\n",
    "$$  \n",
    "\n",
    "Then, the hyperplane location:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\omega} {\\left(1 - y_i\\left(w^\\top x_i+b\\right)\\right)_+}  = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            0 & \\quad \\text{if} \\quad y_i\\left(w^\\top x_i+b\\right) \\geq 1 \\\\\n",
    "            -y_ix_i , & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "Thus, when a point is correctly classified, it only contributes to the regularization term, it does not contribute to the gradient of the hyperplane, i.e. it does not contribute to the 'support vector'. When a point is incorrectly classified, we update the weights with terms from both the regularizer and the hyperplane gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Toy Example\n",
    "\n",
    "Our toy example is presented with our homegrown SVM algorithm in Section 4 below. It can also be found in the [Homegrown Notebook - Kombucha](./book/KombuchaModel.ipynb). For our Toy data we took a random sample of 8000 rows from the given training data set and checked that the 1/0 label distribution was similar to the original set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__References - Section 2__  \n",
    "\n",
    "[1] H. Daumé, A Course in Machine Learning. 2017.  \n",
    "[2] K. K. Mahto, “Demystifying Maths of SVM,” Medium, 18-Apr-2019. [Online]. Available: https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e. [Accessed: 11-Dec-2019].  \n",
    "[3] A. Hertzmann, D. J. Fleet, and M. Brubaker, “SupportVectorMachines from CSC411 Notes,” University of Toronto, 2015. Available: http://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/SupportVectorMachines.pdf. [Accessed: 11-Dec-2019].  \n",
    "[4] “CS 221 - Reflex-based Models Cheatsheet.” [Online]. Available: https://stanford.edu/~shervine/teaching/cs-221/cheatsheet-reflex-models. [Accessed: 11-Dec-2019].  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from the Criteo 'train' set was split before any Exploratory Data Analysis (EDA) into Training/Validation/Test files, at an 80/10/10 ratio. Thus we have 36,673,203 examples in our 'Training' set, and 4,584,676 examples in each of our Validation(Dev) and Test sets. We held out the test set to use for the final model comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:35] : Starting Spark Initialization\n",
      "[18:16:40] : Stopping Spark Initialization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Engineering.setupSpark(application = 'eda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:42] : Starting Data Import\n",
      "[18:16:42] : Stopping Data Import\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Engineering.importData(location = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:44] : Starting Data Splits\n",
      "[18:16:47] : Stopping Data Splits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Engineering.splitsData(ratios = [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Numerical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic statistics for all the numerical variables were first run and reported, as shown in Table 1. Median and skewness were added. i01-i13 are the numerical variables. ctr is the 'y' value - the click through rate, or label.\n",
    "\n",
    "The work was done in the [Numerical Scaling](./book/NumericalScaling.ipynb) workbook. \n",
    "\n",
    "<figure>\n",
    "  <img src=\"assets/NumericalStats.png\" width=\"550\">\n",
    "    <figcaption> <b>Table 1 Numerical Feature Statistics Training Set</b> </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the basic statistics were complete, the following steps were taken to analyse the structure of the numerical variables:  \n",
    "\n",
    "1) Plot histograms and scatter plots of each variable, along with a box plot and violin plot of a 10% sample of each variable.  \n",
    "2) Determine the distribution of each variable  \n",
    "3) Apply a scaling for each variable. \n",
    "\n",
    "An example of the plots is shown in Figure 4 below.\n",
    " \n",
    "Variables are scaled not to create 'normal' variables, but to bring the range of values into a region of approximately (-1,3) so that machine learning techniques could be applied without forcing any weights to be too large or too small. Depending on the algorithm, sometimes this makes a difference. Table 2 shows the variable, the distribution as observed, and the scaling applied."
   ]
  },
  {
   "attachments": {
    "num_plots.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAB0AAAAHXCAYAAAAlRkkSAAAABHNCSVQICAgIfAhkiAAAABl0RVh0\nU29mdHdhcmUAZ25vbWUtc2NyZWVuc2hvdO8Dvz4AACAASURBVHic7N17eJT1nf//1z2nnAM5ABID\nQgwHg2FBQj10V+oRGiuKgovSWuvV8kOR3da26+/brt/a7raLftvd7aKW8vtVLa3lpEVWJKAVscK2\nYGrlFNEA4ZBzMgkh50lm7u8fNwnkAEySyUwy83xcl5e5Z+77/rzn5p0MzCuf+2OYpmkKAAAAAAAA\nAAAAAMKALdQFAAAAAAAAAAAAAECgODJ+0RDqGjAM/f9T9oW6BAAAAAAAAAAAAKAHR6gLwPA0c+bM\nUJeAYeivf/0rvYN+oXfQX/QO+oveQX/RO+gvegf9Re+gv+gd9Be9g/6id9Bf9A76w3H8sfhQ14Bh\nZufOnZL4YQMAAAAAAAAAAIChhzVAAQAAAAAAAAAAAIQNAlAAAAAAAAAAAAAAYYMAFAAAAAAAAAAA\nAEDYIAAFAAAAAAAAAAAAEDYIQAEAAAAAAAAAAACEDQJQAAAAAAAAAAAAAGHD4e+OI0eOVH19vQzD\n6PL4f/3Xf+nxxx/vdwEbNmzQHXfcoeTk5H6fY6A+/PBDrVixQkePHlVKSop+8IMf6KGHHup8fs2a\nNXryySe1cuVKPfHEEyGrc7hauXKlWltbezyem5ur2bNn9/u8hw4dUkZGhmJjYwdS3oCUlJRo27Zt\nqqmpUWxsrObMmaPp06dLkk6fPq0dO3aourpaUVFRuuGGG3TjjTeGrNbhKFJ7p7CwUH/4wx9UV1en\nmJgY3XTTTQN6vZEoUnunQ0tLi1588UVNmjRJd999d4gqHZ4itXd+/OMfy+fzddn/scceU2pqaihK\nHZYitXdaWlq0detWHT16VDabTTk5Obr11ltDVutwFIm988knn+i1117rsq/P59Ps2bOVm5sbomqH\nn0jsHUk6fvy4/vCHP8jj8chms+n666/XrFmzQlbrcBSpvXPy5Ent2LFDNTU1iouL07x58zRp0qSQ\n1TochXPvSFJ+fr527NihO+64Q5/73Oc6H/fn32C4tEjtncs9h8uL1N7hc+WBi9Te4XPlrvwOQCXp\nrbfe0rx58wJawD//8z9r1qxZIQtAPR6PFixYoKefflpLly7Vvn37NHfuXOXk5Gjy5Ml67LHHdPbs\nWU2bNi0k9YWLhx56KOD/sNi5c6fS0tJC9sPG6/Vq/fr1uvnmm5WTk6OSkhL99re/VVpamuLi4vTq\nq69q3rx5mjFjhkpLS/Xyyy8rPT1d48aNC0m9w1Wk9Y7L5dLGjRv1wAMPaNKkSSopKdHLL7+stLQ0\nXXnllSGpd7iKtN65MKjavn277HZ7SGoMB5HWO0lJSfJ4PPrud7+r+Pj4kNQXLiKtd1JTU/Xmm2/K\n6XTq29/+tpqamrRp0yZlZWXpiiuuCEm9w1Wk9c4111yjp59+unPf9vZ2/fKXv1R2dnZIah3OIq13\n4uPjtWHDBi1evFgTJ05UXV2dfvGLX2jMmDFKT08PSb3DVaT1TkxMjNatW6e7775bWVlZ+vTTT7Vx\n40atWLFCiYmJIal3uArH3pGkrVu3qrW1VaNHj+7yuL//BsPlRVrvXO45+C/Seqe5uZnPlQMk0nrn\n7NmzfK7cjf2ZZ555xp8dV65cqYULFyozM7PX5/fu3atFixbpueee06pVq3TFFVd0hoZ5eXlatGiR\nnn32Wa1atUqJiYmaOXOmFi5cqD179uitt97SFVdcIYfDoVGjRunCkmbMmKHU1FTFxsbqyiuvlNPp\n1IIFC/TlL39ZBQUFFx3zQidPntTYsWOVlpamZ555Rj/+8Y/1P//zP7r33nu1c+dObd++XWvXrpVh\nGEpPT9fhw4dVVlamOXPmaMyYMVqxYoU2bNigqVOn8ps6koqKijR27Fi/99+9e7eysrKUkpLS6/PF\nxcXauHGjdu/erb179yo+Pr7zm7ewsFAbN27Unj17tHfvXkVFRWns2LHasGGDTp06pcLCQsXHx8sw\nDD333HO65ZZbOs/7i1/8QnFxcXI6nfrZz34mm82m9evXKzs7W1VVVRcd80JnzpzRT3/6U8XHx2vX\nrl364IMPdPr0aU2dOlXHjx/XsWPHdN9998kwDCUmJqqyslL19fUaO3askpKSNHPmTElSQkKCjhw5\nopEjR/bp2oWb8vJyeseP3rnyyis1depUSVJiYqIOHTqk1NTUiP4wmd65fO9MmDBBkvTZZ5+poKBA\n11xzjTwej6ZMmdKHKx1+6J3L986YMWO0Z88e3X777bLZWB2hA71z+d4ZPXq0tm7dqiVLlig6OlrR\n0dG67rrrIj5Ip3f8f8/q8N577ykmJibi/61F71y+d+Lj4/WXv/xF99xzjyQpOjpan3zyiRITE5WW\nltaXyx1W6J3L947P51NJSYnuuusuGYah1NRUHTt2THa7PWI/EJTonY7eMQxDcXFxuv7663X48GGl\npqZ29kVf3s8iCb1z+d6RdMnnIhW9c/neaW1t5XPlXtA7/vUOnyt31acZoBdTU1Oju+++Wy+88IIW\nLVqk48ePa9asWcrKytLVV1+tBx54QBs2bFBubq7ef/993X777frSl76k9evXy+l0aseOHcrMzNSR\nI0cuOkZUVJSam5vl8XhUUVGh2trai47ZPQR1uVxqaWlRbW2ttmzZotbWVo0bN07vvfeejhw5oqys\nrC77T5kyRQcPHpSkiJ4eHAxNTU363e9+p7vuukvTpk1TbW2tfvnLX2rUqFFKTk7Wpk2btHDhQk2e\nPFknTpzQ2rVrNXnyZC1atEg//OEP9ZWvfEXJycmqqqq66Bg2m01tbW3yer367ne/q+bm5ouO2f0H\nTsexLS0tevDBB9Xe3q5///d/V1FRkdxud4/f9ktJSVFlZaXi4+M736Qkqa6uTlVVVbrqqqsCewEj\nWLj2TmJiYufPJJ/PpyNHjqi+vl4TJ04M/EWMUOHaO5L1W4Lbtm3TkiVLVFBQEPiLF+HCtXeam5tl\nt9v1+9//XsXFxYqKitLs2bP5O1AAhWvvlJeXKyEhQfn5+dq/f7/sdrtmzZqlG264YVCuYyQK1965\n0NmzZ5Wfn6/ly5cH7sIhbHvnb//2bxUfH6+CggJlZWWpurpaNTU1/F05gMK1d6644gqZptnluZiY\nGLnd7sBdvAg3nHsnIyPjosGUv+9n6L9w7R1JBJ6DLFx7h8+VB1+49g6fK/fUpwD0i1/8Yo/H3nnn\nHZWXlysuLk6LFi2SJGVkZOjuu+/Whg0b9KMf/UjFxcWdtxSZM2eOnE6nioqKlJSU5PfYhmHINE0t\nWbJEhmFo27Ztlxyz+7GS9OCDD0qywtSrr75ap06dUmNjo2JiYrrsHxsbq8bGRr9rw+X99re/7fHY\nww8/rIaGBrlcrs7QOikpSVOmTNGhQ4d066236sknn1RUVJQkacKECbLZbDpz5kyPP7NL6eid6dOn\nyzAMFRYWXnLM7sdK0rXXXitJcjgcSk5OVl1dnTwej5xOZ5f9XS6XPB5Pl8fOnj2rV199VbfddttF\nf+MEFxepvXPw4EH9/ve/V3R0tO655x5uy9QPkdg7eXl5ysnJ0ahRo/yuFT1FWu84HA5Nnz5ds2bN\nUnp6uoqLi/Xqq68qISGh87cG4Z9I652WlhbV1dUpOjpaK1asUGVlpX79618rOTlZkydP9rt2RF7v\nXOiDDz7QzJkzI37mcH9FWu84HA7Nnz9fGzZs0JtvvqmWlhbNnTs3ZEvqDGeR1jvjx49XfX299u/f\nr+zsbJ04cUInT57kZ08/hGPvXIq/72e4vEjrHQROJPcOnysPTKT2Dp8rn9enADQvL6/XNUBXrVql\nsrKyLrd+aGlp0f333y9JWrt2rdatWyev1yvDMOTxeOTz+fpVcMdvXdXW1l5yzN5c+Adtt9vl9XoV\nHx+vpqamLvs1NDTwl+AA+/KXv9zr/bb37t2r+vp6/cd//EfnY+3t7Z2/qbB//34dPHiw8zc1vV5v\nj9/a9FfHfblbWlouOWZvOn7gSdYPIJ/PJ5fLpba2ti77tba2yuVydW6XlJRo48aNmjNnjq677rp+\n1R3pIrV3srOzNW3aNJWUlOi1116TaZq65ppr+lV/pIq03jly5Ihqamp077339qtWnBdpvTNy5Mgu\nfTNu3DhlZ2fryJEjBKB9FGm9Ex0dLbvdrs997nMyDENjxozRtddeq8LCQgLQPoq03rnwvAcOHNDX\nv/71ftWMyOudmpoabdq0SV/5yleUnp6us2fP6je/+Y3i4uJYQ7aPIq13YmNjtXjxYr3zzjt6++23\nlZmZqYyMjD59kAlLOPbOpfjzfgb/RFrvIHAitXf4XHngIrV3+Fz5vIDcAjc9PV2TJ0/WgQMHejyX\nl5enf/3Xf9W+fft01VVXdYaOvbHb7ZKs6bkda1CdPXu2yz4d6felxuyLrKysLk0nWQn5jBkzBnRe\n+CcxMVEpKSl6/PHHezxXWFio999/X0uXLtXIkSPl8/n0k5/8pNfzdPSLaZqdPdLa2trnMfti1KhR\n+tOf/tTlsY5b60jn7yO+YMGCiJ5mPljCtXcqKytVU1OjqVOnymazady4cZo0aZI+++yziH2jCrRw\n7Z2DBw/qzJkz+vnPf95Zi8/nU01Njb761a8OaFxYwrV3GhoaVFdX1+UWKl6vVw5HQP6aCIVv7yQn\nJ6u9vV1tbW2dHwJeWBsGLlx7p8PRo0eVkJDAnQsGQbj2TlFRkZKTk5Went45ZmZmpo4dO0YAGiDh\n2juSNHHiRC1durTzuRdffLHHEkrov+HcO5fiz/sZBiZceweDL5x7h8+VB1e49g6fK/dkC8RJbr31\nVpWWlmr79u2SpMbGRn3jG9/QRx99pNLSUqWkpGjcuHEyTVPPPvusDMNQQ0ODbDab7Ha7amtrJUlj\nx46Vw+HoXAt0z549On36dJ/H7Is5c+bI5/PpxRdflM/n065du/T222933i4Xg2vixImqr69XYWGh\nJOvWIlu2bFFpaanq6+sVFxenESNGyDRN7d69W5L1Q8QwDNlsNjU3N0uy7o1ut9s777t96tSpHuG5\nP2P2xYQJE2Sapj788EOZpqmioqLOf3i3tbVp06ZNvEkNonDtndbWVr3++us6efKkJGvR62PHjikt\nLa3vFwm9CtfeWbRokb7zne/oW9/6lr71rW/pxhtvVHZ2NuFnAIVr79TU1OjXv/5159+5SkpKdPjw\n4Yj9y/FgCNfeGTlypDIyMrRz5075fD653W4dPnyY2Z8BFK6906GkpKTHejcIjHDtnTFjxqiqqqqz\nnpaWFh0/fpwgIoDCtXdaW1v1n//5nzp16lTnPi0tLb3OCkH/DOfeuRR/3s8wMOHaOxh84do7fK48\n+MK1d/hcuaeA/Gr/iBEjtHXrVn3rW9/SE088IcMwtHDhQs2YMUOTJk3S+vXrlZmZqZSUFP3TP/2T\nlixZogcffFC7du3S4sWLdcstt+hHP/qRnnzySf3kJz/RggULNGHCBM2cOVM33XRTr9OLLzVmny6A\nw6E33nhDjz/+uL7//e/ryiuv1K9//WtlZGTI6/UqLi5OkvWD5/3339d3vvMdfeMb39CqVasCceki\nXnR0tJYsWaLt27dr27ZtMgxDWVlZGjt2rFJSUnTo0CH9/Oc/V2xsrD7/+c8rOztbr732mr72ta8p\nOztbr7zyim699VbdeOONuu2227RhwwaNHDlSY8eO7Qzd+zJmX9hsNi1evFhvvfWW3n33XSUmJmrB\nggVKSkpSQUGBzpw50+M+47Nnz+71NtLou3DtnaSkJOXm5urNN99UQ0ODnE6nsrOzNWvWrEBduogX\nrr2DwReuvZOUlKR58+Zp8+bNampqUnx8vObNm6err746UJcu4oVr70jS/fffr82bN+u5555TdHS0\n5syZo8zMzIBcN4R370jW3X4SEhIGfJ3QU7j2TlJSku68805t3LhRXq9XknTNNdfoc5/7XECuG8K3\ndyTrF+lff/11NTc3a/To0VqyZEmPtR3Rf8O5d3w+n3784x93fn3ixAnt2LFDs2bNUm5uLv8GG2Th\n2jvz5s27ZF9h4MK1dyZMmMDnyoMsXHsnNzeXz5W7Mcz+3rwYEWvnzp2aOXNmqMvAMPTXv/6V3kG/\n0DvoL3oH/UXvoL/oHfQXvYP+onfQX/QO+oveQX/RO+gvegf9EZBb4AIAAAAAAAAAAADAUEAACgAA\nAAAAAAAAACBsEIACAAAAAAAAAAAACBsEoAAAAAAAAAAAAADCBgEoAAAAAAAAAAAAgLBBAAoAAAAA\nAAAAAAAgbBimaZqhLgLDy86dO0NdAgAAAAAAAAAAANArAlAAAAAAAAAAAAAAYYNb4AIAAAAAAAAA\nAAAIGwSgAAAAAAAAAAAAAMIGASgAAAAAAAAAAACAsEEACgAAAAAAAAAAACBsEIACAAAAAAAAAAAA\nCBsEoAAAAAAAAAAAAADCBgEoAAAAAAAAAAAAgLBBAAoAAAAAAAAAAAAgbBCAAgAAAAAAAAAAAAgb\nBKAAAAAAAAAAAAAAwgYBKAAAAAAAAAAAAICwQQAKAAAAAAAAAAAAIGwQgAIAAAAAAAAAAAAIGwSg\nAAAAAAAAAAAAAMIGASgAAAAAAAAAAACAsEEACgAAAAAAAAAAACBsEIACAAAAAAAAAAAACBsEoAAA\nAAAAAAAAAADCBgEoAAAAAAAAAAAAgLBBAAoAAAAAAAAAAAAgbBCAAgAAAAAAAAAAAAgbBKAAAAAA\nAAAAAAAAwoYjmIOlpqZqwoQJwRwSAMLOyZMnVVVVFeoyhgzeWwAgMHh/6Yr3FwAIDN5fzuO9BQAC\ng/cWAP4IagA6YcIE5efnB3NIAAg7OTk5oS5hSOG9BQACg/eXrnh/AYDA4P3lPN5bACAweG8B4A9u\ngQsAAAAAAAAAAAAgbBCAAgAAAAAAAAAAAAgbBKAAAAAAAAAAAAAAwgYBKAAAAAAAAAAAAICwQQAK\nAAAAAAAAAAAAIGwQgAIAAAAAAAAAAAAIGwSgAAAAAAAAAAAAAMIGASgAAAAAAAAAAACAsEEACgAA\nAAAAAAAAACBsEIACAAAAAAAAAAAACBsEoAAAAAAAAAAAAADCBgEoAAAAgCFv+/btmjJlijIzM7Vy\n5cqL7vfhhx/Kbrfrtdde6/OxAAAAAAAgPBCAAgAAABjSvF6vli9frry8PBUUFGjdunUqKCjodb+n\nnnpKc+fO7fOxAAAAAAAgfBCAAgAAABjS9u3bp8zMTGVkZMjlcmnx4sXasmVLj/1WrVql+++/X6NH\nj+7zsQAAAAAAIHwQgAIAAAAY0kpKSjRu3LjO7fT0dJWUlPTYZ/PmzVq2bFmfj+2wZs0a5eTkKCcn\nR1VVVQF8BQAAAAAAIJgcoS4AAAAAAC7FNM0ejxmG0WX7m9/8pp599lnZ7fY+H9th6dKlWrp0qSQp\nJyenv+UCAMLAo48+qq1bt2r06NE6dOhQj+dN09Q//uM/atu2bYqNjdUrr7yi6667bnCKOVsu+dqs\nr50xkiHJ02Rt25ySM1ZqPSuZPsnmkOJSpYYKa9uwSyPHSXWnJZ/X2o5NlVpqJV+7JEOKHim1NUre\nc2M4oiTDJrU1W9t2h+SKl5rPnBvDLsVdITWUSab3gjGKrXMadilulFVTe6s1RsxIKSq+6+tqa7Hq\nME3rNbjipaZqq05HlBSbIjVWWnXZnVLcaKn7e3hD5bkxJMUk9RzDH2dOn38dCWlSt79LqKXOut6G\nIUWPsP4MLsXbJtVXWOe0O6XENKmhSvK2Wtuxo6SWM9b1tdmt12l39r3ugWprtv5MZVrXPjox+DUM\nF+2tUnOt1f/OWKufB5vPKzVWW9/7jmjr+zpcNFZZ19TmtF6XzX75YwCgHwhAAQAAAAxp6enpOn36\ndOd2cXGx0tLSuuyTn5+vxYsXS5Kqq6u1bds2ORwOv44NujVren/8XPgKAAi9Rx55RE888YQefvjh\nXp/Py8tTYWGhCgsLtXfvXj322GPau3dv4As5Wya9+0Pp4EYrLFjyunT6T9IHP7MChGvvl+74F2nN\nLVJTlXT949L0hdKmR6Qzp6Srb5VyfyZt+qpUfkBKyZS+slna+k3p6LtW+Hnnv0ipk6VX7rKCiMf3\nSgVvSH/8P1ZIlnWvdNv/ll6aawWr42+QFqyxxij9SEqdJD3wG6umg5ukpKukh9+U8p6SCrdLrgTp\n9h9I19wjxY+yXlfzGWvfPzxjha+3/4s0Jkt6Y5kVav7NQ9KNy6VND0vuY1LK1dIDv5VGX3M+BD1b\nJm37jvTpW5IzTrrl+9b1SBjj//WtLJA2PixVF0pJE6VFL0ujsiRnlPV8Y5W07SmpYLMVfN7yPWnG\nly8egHmarGvy+29IZ0ulrHukW/5Z2vgVqeqINHK8tOgV6cAmae8vpNhk6e5V0tW3SK64/vVIfzTV\nSn95Rfrjc1J7i5S9SJr7k/AK2QKl5axUsEXa8T3JUy9NmivNXyXFj778sf3lbZNK/iK99jWrj66Y\nLv39b63vreHuzClpw5elsv3WLwfc/5J05SzJ4Qp1ZQDCELfABQAAADCkzZ49W4WFhSoqKpLH49H6\n9es1f/78LvsUFRXpxIkTOnHihBYuXKgXX3xR9957r1/HAgDQ3c0336zk5OSLPr9lyxY9/PDDMgxD\nN9xwg86cOaOysrLAFuH1Sp/lSfvXWbPBYkZK7U3Szn+1gknTZ4WIBW9IdzxjHTPpdmndYitkkKTJ\n86Qtj1vhpyRN+Dtp939a4adkzUT87xXWLERXvDRmujVz8w/PSJ5Ga4zDv5cOrLdCCkk69Wfpnaet\ncFWywsOND1tjSdL4m6S9q63aTdM631vftmZ7dmiotMJLT4O1z/jrpfUPWY9L0tRcacMSK/yUrP+v\nf8gKJCUr/M1/WTqy1Tre0yDt+F9Sk9v/63vmtLTxq1b9klRbJK1fYgXJHdd//wbp8OvWdfA0Sju+\nL53t/Vb6kqxZgusfskKrjuv/2tes8FOy/lzWLZYm3WFtN9VYIW/zGf/rDoQzJ6V3n5HamqzXdmCD\n1Uc+X3DrGA4aq6X/fuLcLGtT+my79OdfSO2ewRuzyS397oHzfVR+QNr8/1j9Mpw11Uqbl1nhp2S9\nvt89IDUP89cFYMgiAAUAAAAwpDkcDj3//POaO3eurrnmGj3wwAOaNm2aVq9erdWrV/frWAAABqIv\na0z3W+sZ6djO89upU6TiD3vud/w9afS59zZH1PkQUbJmTJ7+8/ntMVldtztUHJLGzpCm3SOd2NPz\n+VN/lsZM67addX67+jMp4YpzY0yznu+u5K8XfJ1//mtHtNRab81E7BCTJNWe6Hp8bdH5290210on\n/thzjNN9mIVreq26L3S25PwYbQ3S0Xd6HnfqEmN46q1b5nYYOd66thdqqLT+nDr4vFYgGUwndvd8\n7LMdUntzcOsYDsr393zs+C6rZweLp6FrH0nW923HrbCHK6+n58+f1rNSa0No6gEQ9rgFLgAAAIAh\nLzc3V7m5uV0eW7ZsWa/7vvLKK5c9FgCAgfB3jek1a9Zozblbn1dVVfVtkKiR0lWflz5509p2H5Vu\n6OW9b/xN52dKetus26p2zBSrLrRmbhafCxyrPrW2Kw53PcfoLKnioLWG5x0/7DnGlbO6hoVXXtd1\nOznj/OzMqk+t50s/6nqOsdPPf5024/zX7S3W+pN2lxWQSFb4MyLdWle0w4h0ax/JunXvuOulU3/q\nWae/DLtVd83x84/Fjzk/hjNOmnCzFXZdKP0SY7jirf885wKds6XSqKnnZ4BK1p+P94Igy7BZQWkw\njb+h52MZt1hhNLoac23Px666qX/rzfrLFde1jyQp7TrJGOYf5dsc1vfo6X3nH3PFDe61BBDRmAEK\nAAAAAAAA9IG/a0wvXbpU+fn5ys/P16hRo/o2iN1urb859W5r3cuGCikqQfr8NyW709pn8hel6X8v\n5X3X2v70LWutwLhzYxVske550VpDU5IK35H+7ttWeChZ61rO/TdrncPmGmt90bhUa5+OIDDzdmnm\nl6WiD6ztK7Kt9SI/2Wptj0i31rU8/Ia1fexd6cYnrPBWskK125+xbrPbISFNuu0H52dCntgt3f8r\nKXrEubr/21pXNPHcNU1Mk/7+d+dflzNaun6ZlPGFc2NESV/4Xt/W/0xIkxa+bNUvWeHnA2vPj2F3\nSNc9LGWeu12t3SXN+X+lEeN6P59kBbOLXrFCTsm6ffD9v5JGnlu7MW6U9PevWn9OkhVy3fPi+dcd\nLMkZ0t892a2PFllrwKKruFFWv3eEw1d9Xvr8P3SdxRto0cnW93FHH6VcLd33/0lxKZc+bqiLS5EW\n/NJai1iyZno/8JuLr6kLAANkmL39ytogycnJUX5+/uV3BABcFD9Lu+J6AEBg8PO0q0G9HudmAvWw\ndOngjAcAITSc319OnDihL33pSzp06FCP59566y09//zz2rZtm/bu3at/+Id/0L59+3o5y3n9vhZ1\npdbMTMOwZlAZNmumpOmzth1xUttZ61aqdqcUlSS1uK1Zhg6XFDvKmp3p9Zx7PtFaz9LrsY53xUlt\nLdbtNQ1Dsrms/3tbz43hlBwJUludtY/NaZ2zqeOcLis8bKy0bh9rd1nBRmu9dQ7DbgW33UMOT6MV\nvJpeyRlr1dFca53TEWOFP43V1gxRR7QVzHYP6OrLrTENmxUmxib17dq2t0kN5edfR9woK1y9UFOt\n1NZ47nUkSlFxlz5nW7PUVG1df3uUdW2a3dY1dkRJsanW2qttzdb1j0nqOWYwtDacW9fSZ137mD5e\nu0jiabJmJZte65cGYoMQRHrbrLVAvZ5z/T/K+r4MBw2V1ve13WVdy44gvg+G83sLgOAZ5vPmAQAA\nAAAAgMB68MEHtWvXLlVXVys9PV0//OEP1dZm3bZ02bJlys3N1bZt25SZmanY2Fi9/PLLg1fMiJ4z\nS3vqFi5GdwvpRnabtdivGVeJXTdd3c7ZMZOyw+Vua+mKs/67UMc6op3bl5nR2X3/vnI4e16b7mKT\nJPUhHHTG9JwlGt/tdcSl+n++wRIVpbY2PAAAIABJREFUz61H/eWKtf4LJrtz4P09VMWPDnUFACIE\nASgAAAAAAABwgXXr1l3yecMw9MILLwSpGgAAAPQVa4ACAAAAAAAAAAAACBsEoAAAAAAAAAAAAADC\nBgEoAAAAAAAAAAAAgLBBAAoAAAAAAAAAAAAgbBCAAgAAAAAAAAAAAAgbBKAAAAAAAAAAAAAAwgYB\nKAAAAAAAAAAAAICwQQAKAAAAAAAAAAAAIGwQgAIAAAAAAAAAAAAIGwSgAAAAAAAAAAAAAMIGASgA\nAAAAAAAAAACAsOEIdQH+WLVqlY4ePdq5XVJSIkm68sorL3lcZmamVqxYMai1AQAAAAAAAAAAABg6\nhkUAevToUX186BN5Y5MlSfamOklSeevFy7c31QSlNgAAJOnZ557TmNGj9cgjj4S6FAAAAAAAAACI\naMMiAJUkb2yymqfmSpJijmyTpM7t3nTsAwDAYCsrK1Petm2KjonR4sWLFR0dHeqSAAAAAAAAACBi\nsQYoAAAD9Pbbb0uSWpqbtXv37hBXAwAAAAAAAACRjQAUAIAB8Pl8emtbnrwJY6XoBG3Lywt1SQAA\nAAAAAAAQ0QhAAQAYgIMHD6qyolye1ElqTb5af/3oI1VWVoa6LAAAAAAAAACIWASgAAAMwI4dO2TY\nnWpPukptKZkyTVPvvPNOqMsCAAAAAAAAgIhFAAoAQD81Nzfr3Z071Zo0QbI7ZUYnypcwRnnbt8s0\nzVCXBwAAAAAAAAARiQAUAIB++uCDD9Ta0qL21Emdj3lSMlV8+rSOHDkSwsoAAAAAAAAAIHIRgAIA\n0E/b8vKk6ER548d0PtaWNFGGzaEdO3aEsDIAAAAAAAAAiFwEoAAA9IPX69XBAwfkGTleMozzTzhc\n8iSm6c9794WuOAAAAAAAAACIYASgAAD0Q3V1tbxer3zRI3o854tJUmVFudra2kJQGQAAAAAAAABE\nNgJQAAD6oaysTJLkc8X3eM4XPUI+n0+lpaXBLgsAAAAAAAAAIh4BKAAA/dAZgEYl9HiuY1boqVOn\ngloTAAAAAAAAAIAAFACAfikrK5MMQ+ZFZoBK0unTp4NdFgAAAAAAAABEPAJQAAD6obS0VEZUvGTr\n5a3U4ZIRFUsACgAAAAAAAAAhQAAKAEA/lJaVqc3Zc/Znh3ZXok5yC1wAAAAAAAAACDoCUAAA+qGk\npLTX9T87eKNHsAYoAAAAAAAAAIQAASgAAH3U0tKiujO1MqMuPgPUFz1CDfX1qqurC2JlAAAAAAAA\nAAACUAAA+qi8vFySLjkD1Bc9QpJYBxQAAAAAAAAAgowAFACAPiorK5NEAAoAAAAAAAAAQxEBKAAA\nfdQRgJqXCEDNqHjJsLEOKAAEyPbt2zVlyhRlZmZq5cqVPZ7fsmWLpk+frhkzZignJ0e7d+/ufG7C\nhAnKzs7ufA4AAAAAAIQ3R6gLAABguCkrK5Nhd8h0RF98J8MmxYxgBigABIDX69Xy5cv1zjvvKD09\nXbNnz9b8+fOVlZXVuc9tt92m+fPnyzAMHThwQA888ICOHDnS+fx7772n1NTUUJQPAAAAAACCjBmg\nAAD0UVlZmTX70zAuuV+bK0EnTjIDFAAGat++fcrMzFRGRoZcLpcWL16sLVu2dNknPj5exrmfy42N\njZ1fAwAAAACAyEMACgBAHxWXlKrdFX/Z/XzRI1RWWqL29vYgVAUA4aukpETjxo3r3E5PT1dJSUmP\n/TZv3qypU6fqrrvu0ksvvdT5uGEYuvPOOzVr1iytWbPmouOsWbNGOTk5ysnJUVVVVWBfBAAAAAAA\nCBoCUAAA+sA0TZWXlcnnuvj6nx18MSPl9XpVXl4ehMoAIHyZptnjsd5meC5YsEBHjhzRG2+8oaef\nfrrz8T179uijjz5SXl6eXnjhBf3xj3/sdZylS5cqPz9f+fn5GjVqVOBeAAAAAAAACCoCUAAA+qCu\nrk4tLc3yRfkRgEaPkCSdOsVtcAFgINLT07usqVxcXKy0tLSL7n/zzTfr2LFjqq6ulqTOfUePHq0F\nCxZo3759g1swAAAAAAAIKQJQAAD6oGM2Z18C0As/tAcA9N3s2bNVWFiooqIieTwerV+/XvPnz++y\nz9GjRztnin700UfyeDxKSUlRY2Oj6uvrJVlrg7799tu69tprg/4aAAAAAABA8DhCXQAAAMNJaWmp\nJMn0IwCVI0qGK4YAFAAGyOFw6Pnnn9fcuXPl9Xr16KOPatq0aVq9erUkadmyZXr99de1du1aOZ1O\nxcTEaMOGDTIMQxUVFVqwYIEkqb29XQ899JDmzZsXypcDAAAAAAAGGQEoAAB9UFZWJknyRcX7tX+7\nK1EnuQUuAAxYbm6ucnNzuzy2bNmyzq+feuopPfXUUz2Oy8jI0P79+we9PgAAAAAAMHRwC1wAAPqg\nrKxMhitWsjv92t8bnahTJwlAAQAAAAAAACBYCEABAOiDsrIyeV1xfu9vRiWoru6MWltbB7EqAAAA\nAAAAAEAHAlAAAPqguLhEXpcf63+e4zu3Vmh5eflglQQAAAAAAAAAuAABKAAAfmpvb1dVdZXf639K\nkumy9q2oqBissgAAAAAAAAAAFyAABQDAT7W1tfJ5vZ2hpj86wlJmgAIAAAAAAABAcBCAAgDgp5qa\nGkmSzxnr9zGmM1YybASgAAAAAAAAABAkBKAAAPjJ7XZLkkxnjP8HGYYUHU8ACgAAAAAAAABBQgAK\nAICfOmaA9ikAldTujFNZWdlglAQAAAAAAAAA6IYAFAAAP/U3APW5ElRaxgxQAAAAAAAAAAgGAlAA\nAPzkdrtlOKMlm71Px5lR8ao7U6vW1tZBqgwAAAAAAAAA0IEAFAAAP7ndbpnO2D4f53PFS5IqKysD\nXRIAAAAAAAAAoBsCUAAA/OR2u9XuiO7zcWaUFYCWl3MbXAAAAAAAAAAYbASgAAD4qara3ef1P6Xz\nM0AJQAEAAAAAAABg8BGAAgDgB9M0daa2Rr5+3ALXdMVKho0AFAAAAAAAAACCgAAUAAA/NDQ0qL29\nvV8zQGXYZETHE4ACAAAAAAAAQBAQgAIA4Ae32y1JMvsxA1SS2pxxKisrC2RJAAAAAAAAAIBeEIAC\nAOCH8wFoP2aASjJd8SpjBigAAAAAAAAADDoCUAAA/FBTUyNJ/VoDVJJ8rnjV1tTI4/EEsiwAAAAA\nAAAAQDcEoAAA+KFzBqirfzNAfVHxkqTKysqA1QQAAABgcGzfvl1TpkxRZmamVq5c2eP5uro63X33\n3fqbv/kbTZs2TS+//HIIqgQAAMDFEIACAOCHmpoaGXanZHP263gzKkGSVM5tcAEAAIAhzev1avny\n5crLy1NBQYHWrVungoKCLvu88MILysrK0v79+7Vr1y59+9vf5m4vAAAAQwgBKAAAfqipqZFcMZJh\n9Ot4n8uaAUoACgAAAAxt+/btU2ZmpjIyMuRyubR48WJt2bKlyz6GYai+vl6maaqhoUHJyclyOBwh\nqhgAAADdEYACAOCH6mq32u39u/2tJJmuWMmwEYACAAAAQ1xJSYnGjRvXuZ2enq6SkpIu+zzxxBP6\n5JNPlJaWpuzsbP385z+XzcbHbAAAAEMFfzMDAMAP1e5q+Zz9D0Bl2GRExRGAAgAAAEOcaZo9HjO6\n3Qlmx44dmjFjhkpLS/Xxxx/riSee0NmzZ3sct2bNGuXk5CgnJ0dVVVWDVjMAAAC6IgAFAMAPNe4a\nmQMJQCW1OeNUVkYACgAAAAxl6enpOn36dOd2cXGx0tLSuuzz8ssv67777pNhGMrMzNTEiRN15MiR\nHudaunSp8vPzlZ+fr1GjRg167QAAALAQgAIAcBmtra1qamqU6Ywd0HnMqASVlpUFqCoAAAAAg2H2\n7NkqLCxUUVGRPB6P1q9fr/nz53fZZ/z48Xr33XclSRUVFfr000+VkZERinIBAADQC1ZnBwDgMmpq\naiRpYLfAleRzxetM2VF5PB65XK5AlAYAAAAgwBwOh55//nnNnTtXXq9Xjz76qKZNm6bVq1dLkpYt\nW6ann35ajzzyiLKzs2Wapp599lmlpqaGuHIAAAB0IAAFAOAyOgLQgd4C1xcVL9M0VVFRoXHjxgWi\nNAAAAACDIDc3V7m5uV0eW7ZsWefXaWlpevvtt4NdFgAAAPzELXABALgMt9stSTJdA78FriSVlpYO\nuCYAAAAAAAAAQO8IQAEAuIzAzQBNlCSVsQ4oAAAAAAAAAAwaAlAAAC7D7XZLhiHTET2g85jOGBk2\nBzNAAQAAAAAAAGAQEYACAHAZbrdbhitWMgb4tmkYMqMTVF5eHpjCAAAAAAAAAAA9EIACAHAZNTU1\n8g1w9meHdme8iotLAnIuAAAAAAAAAEBPBKAAAFxGdbVbXsfA1v/s4ItKUGlZmUzTDMj5AAAAAAAA\nAABdEYACAHAZ1W63fM7ABaAtzU06e/ZsQM4HAAAAAAAAAOiKABQAgEvwer06W3dGpjM2IOfzRSVI\nksrKygJyPgAAAAAAAABAVwSgAABcQl1dnXw+n8wAzQA1zwWgpaWlATkfAAAAAAAAAKArAlAAAC7B\n7XZLUgBngMZLYgYoAAAAAAAAAAwWAlAAAC6hpqZGkgK2BqjsThmuWGaAAkAfbd++XVOmTFFmZqZW\nrlzZ4/ktW7Zo+vTpmjFjhnJycrR7926/jwUAAAAAAOGFABQAgEvonAHqCswMUEnyuuIJQAGgD7xe\nr5YvX668vDwVFBRo3bp1Kigo6LLPbbfdpv379+vjjz/WSy+9pK9//et+HwsAAAAAAMILASgAAJdw\n/ha4AZoBKisALS4hAAUAf+3bt0+ZmZnKyMiQy+XS4sWLtWXLli77xMfHyzAMSVJjY2Pn1/4cCwAA\nAAAAwgsBKAAAl+B2u2U4oySbI2Dn9EUlqLqqUu3t7QE7JwCEs5KSEo0bN65zOz09XSUlJT3227x5\ns6ZOnaq77rpLL730Up+OlaQ1a9YoJydHOTk5qqqqCvCrAAAAAAAAwUIACgDAJdTU1Mh0Bu72t5IV\ngJqmqYqKioCeFwDClWmaPR7rmOF5oQULFujIkSN644039PTTT/fpWElaunSp8vPzlZ+fr1GjRg2w\nagAAAAAAECoEoAAAXEJ1dbXa7dEBPacZlSBJKisrC+h5ASBcpaen6/Tp053bxcXFSktLu+j+N998\ns44dO6bq6uo+HwsAAAAAAIY/AlAAAC6hqto9KDNAJam0lHVAAcAfs2fPVmFhoYqKiuTxeLR+/XrN\nnz+/yz5Hjx7tnO350UcfyePxKCUlxa9jAQAAAABAeAncgmYAAIQZ0zRVW1sjMyWwt0E0XbGSzc4M\nUADwk8Ph0PPPP6+5c+fK6/Xq0Ucf1bRp07R69WpJ0rJly/T6669r7dq1cjqdiomJ0YYNG2QYxkWP\nBQAAAAAA4YsAFACAi2hoaFB7W5t8AZ4BKsMmRSUwAxQA+iA3N1e5ubldHlu2bFnn10899ZSeeuop\nv48FAAAAAADha1gEoCUlJbK1NAV1zFWrVkmSVqxYEdRxAQBDR01NjSTJdMYE/NztrjiVlBCAAgAA\nAAAAAECgDYsAtLm5WYavLahjHj16NKjjAQCGHrfbLUkBXwNUstYBLS07HfDzAgAAAAAAAECks4W6\nAAAAhqqOADTgt8CVFYA2NTaqvr4+4OcGAAAAAAAAgEhGAAoAwEV0zgB1Bf4WuGZUoiSpuLg44OcG\nAAAAAAAAgEhGAAoAwEXU1NTIsDslmzPg5/bGjJQknThxIuDnBgAAAAAAAIBIRgAKAMBFuN1uyRUj\nGUbAz21GJUg2OwEoAAAAAAAAAAQYASgAABdR7Xar3RH4299KkgybzOgRBKAAAAAAAAAAEGAEoAAA\nXERVVbV8gxWASmqPHqljx4sG7fwAAAAAAAAAEIkIQAEAuIiamhqZzthBO78vZqSqqyrV1NQ0aGMA\nAAAAAAAAQKQhAAUAoBctLS1qaW6S6Rq8GaC+mJGSpFOnTg3aGAAAAAAAAAAQaQhAAQDohdvtliT5\nBnEGqDc6SZJYBxQAAAAAAAAAAogAFACAXnQEoIN5C1wzOkGy2QlAAQAAAAAAACCACEABAOhFTU2N\nJMl0Dt4tcGXYZEaPIAAFAAAAAAAAgAAiAAUAoBfBmAEqSe3RI3S8qGhQxwAAAAAAAACASEIACgBA\nL9xutzVD0xE1qOP4YpJUWVGh5ubmQR0HAAAAAAAAACIFASgAAL1wu90yomIlwxjUcXzRIyVJp06d\nGtRxAAAAAAAAACBSEIACANALt9str2MQ1/88xxdjBaCsAwoAAAAAAAAAgUEACgBAL6qqgxSARidK\nho0AFAAAAAAAAAAChAAUAIBeuN1umc7YwR/IsMmMGUkACgAAAAAAAAABQgAKAEA3bW1taqg/K9M5\n+DNAJak9eoSOFxUFZSwAAAAAAAAACHcEoAAAdFNbWytJMl1BmAEqyRc9UpUVFWppaQnKeACAEGlv\nl773PamiItSVAAAAAAAQ1ghAAQDoxu12S5J8QZoB6otJkmmaOnnyZFDGAwCESGGh9G//Jv30p1Jp\naairAQAAAAAgbBGAAgDQTXV1tSQFZw1QSd7YZEnSZ599FpTxAAAh4vFY/6+vl372M+n06dDWAwAA\nAABAmCIABQCgm/LyckmSLyo+KOOZUQkyXDE6dOhQUMYDAIRIW5v1/4ULJZtN2rQptPUAAAAAABCm\nCEABAOimvLxchsMp2aOCM6BhyBM7SvsPHAzOeACA0OgIQMeOlSZOlBobQ1sPAAAAAABhigAUAIBu\nKioqZLoSJMMI2pje+DEqLytVTU1N0MYEAARZxy1w7XbJ5Tq/DQAAAAAAAooAFACAbkpLy9TujAvq\nmN740ZKkw4cPB3VcAEAQdcwAtdslp/P8NgAAAAAACCgCUAAAuikvLw/a+p8dfHGpks3OOqAAEM66\nB6DMAAUAAAAAYFAQgAIAcIH6+no1NTUGPQCVzS5fbIoOHiQABYCwdWEA6nIxAxQAAAAAgEFCAAoA\nwAUqKiokSaYryAGopPb4Mfr0s0/V2toa9LEBAEHQ2wxQ0wxtTQAAAAAAhCECUAAALlBeXi5J8kUl\nBH1sb/xoedvbVVhYGPSxAQBB0H0GqCS1t4euHgAAAAAAwhQBKAAAF+gIQEMxA9QbP1qSWAcUAMJV\nRwDqcFgzQCXWAQUAAAAAYBAQgAIAcIHy8nIZdqdMR1TQxzadMVLMCAJQAAhXHWHnhTNAWQcUAAAA\nAICAIwAFAOAC5eXlMqPiJcMIyfhtsaO0/8BBmawJBwDhp/saoBc+BgAAAAAAAoYAFACAC5SWland\nGfzb33bwxo9W/dk6lZSUhKwGAMAg6W0NUG6BCwAAAABAwBGAAgBwgfLycvmiQhiAJoyRJO3fvz9k\nNQAABgkBKAAAAAAAQUEACgDAOfX19WpqbJTPFboA1Bc9UoqK1wcffBCyGgAAg4Rb4AIAAAAAEBQE\noAAAnFNRUSFJ1hqgoWIY8oy8Sh/m56uhoSF0dQDAELN9+3ZNmTJFmZmZWrlyZY/nX331VU2fPl3T\np0/XTTfd1GUm/YQJE5Sdna0ZM2YoJycnmGV31VsAygxQAAAAAAACjgAUAIBzysvLJSmkM0AlqS15\norzt7frTn/4U0joAYKjwer1avny58vLyVFBQoHXr1qmgoKDLPhMnTtT777+vAwcO6Omnn9bSpUu7\nPP/ee+/p448/Vn5+fjBL76ojALXZzt8ClxmgAAAAAAAEHAEoAADnDIkZoJJ8caNkRMXp/ff/GNI6\nAGCo2LdvnzIzM5WRkSGXy6XFixdry5YtXfa56aablJSUJEm64YYbVFxcHIpSL83jsWZ+Gga3wAUA\nAAAAYBARgAIAcE55ebkMu0OmIzq0hRiGWkdcpb17/6ympqbQ1gIAQ0BJSYnGjRvXuZ2enq6SkpKL\n7v+rX/1KX/ziFzu3DcPQnXfeqVmzZmnNmjUXPW7NmjXKyclRTk6OqqqqAlP8hdrazgefHTNAuQUu\nAAxJl7v1uiTt2rVLM2bM0LRp0zRnzpwgVwgAAIBLcYS6AAAAhory8nKZUQnWzJwQa0+eoLbKAu3d\nu1e33HJLqMsBgJAyTbPHY8ZFfla/9957+tWvfqXdu3d3PrZnzx6lpaWpsrJSd9xxh6ZOnaqbb765\nx7FLly7tvHXuoKwVSgAKAMNCx63X33nnHaWnp2v27NmaP3++srKyOvc5c+aMHn/8cW3fvl3jx49X\nZWVlCCsGAABAd8wABQDgnNLSMrU740JdhiTJGz9ahitWu3btCnUpABBy6enpOn36dOd2cXGx0tLS\neux34MABff3rX9eWLVuUkpLS+XjHvqNHj9aCBQu0b9++wS+6NxcGoNwCFwCGLH9uvf673/1O9913\nn8aPHy/Jeo8BAADA0EEACgDAOWXl5fKFeP3PToZNrSPG609//rNaWlpCXQ0AhNTs2bNVWFiooqIi\neTwerV+/XvPnz++yz6lTp3TffffpN7/5jSZPntz5eGNjo+rr6zu/fvvtt3XttdcGtf5OvQWgzAAF\ngCHHn1uvf/bZZ6qtrdUXvvAFzZo1S2vXrg12mQAAALgEboELAICkhoYGNTU2yJeUEOpSOrUnT5Cn\n6oj27t3LmkIAIprD4dDzzz+vuXPnyuv16tFHH9W0adO0evVqSdKyZcv0ox/9SG63W48//njnMfn5\n+aqoqNCCBQskSe3t7XrooYc0b9680LyQCwNQm01yOJgBCgBDkD+3Xm9vb9df/vIXvfvuu2pubtaN\nN96oG264ocsv4UjW+tId608PyvrSAAAA6BUBKAAAkioqKiRJ5lCZASrJm3CF5IrVm1u3EoACiHi5\nubnKzc39v+zde3icdZ3//9d9z2RyPjSTtkmankJoKaW10JQvJ4GuYKGVIuyCVVZAFmKkAqLLioAr\numXF66uiCFoDXAJa2q/yQyssLeDaxUWktUDLIUJTbEsT6Cn0kOMc798f05lmcupMMjP3ZOb5uK65\nJvfch897ZvcidV55v++o1xobGyM/P/zww3r44YcHnFdbW6utW7cmvb6Y9A1ApdDPdIACQNqJZfR6\nTU2NKioqVFhYqMLCQp177rnaunXrgAA06feXBgAAwKAYgQsAgKQ9e/ZIkoKu9AlAZZjyjJ+lzX/9\nq7Zv3253NQCA0fJ6JZfr2LbLRQcoAKShWEavX3rppfrf//1f+f1+dXd3a+PGjZo1a5ZNFQMAAKA/\nAlAAAKTIPX3SqQNUkrwTTpLhyNGaNWvsLgUAMFp0gALAmNB39PqsWbN05ZVXRkavh8evz5o1Sxdd\ndJHmzp2r008/Xddff71995gGAADAAIzABQBAUktLi4zcQlk5+XaXEs2ZK0/FDP3xj3/U9ddfr8rK\nSrsrAgCMVP8AlA5QAEhbxxu9Lkm33XabbrvttlSWBQAAgBjRAQoAgKR33n1Xvrxyu8sYlHfibAUt\n6Te/+Y3dpQAARmOwDlACUAAAAAAAEo4AFACQ9Xp7e9W6e7cChW67SxmUlVskX3mtnn7mGR05csTu\ncgAAI8UIXAAAAAAAUoIAFACQ9d577z1ZlqVgQXoGoJLkrZwjr8ejp556yu5SAAAjNdgIXAJQAAAA\nAAASjgAUAJD1WlpaJEmBNA5AgwXj5B83TU88sVp79uyxuxwAwEhwD1AAAAAAAFKCABQAkPW2bdsm\nIydPlqvQ7lKG1Tv5dPkCAf34/vvtLgUAMBJebyj0DOMeoAAAAAAAJAUBKAAg623b1iJffrlkGHaX\nMiwrt0g9VafqLy+/rJdeesnucgAA8WIELgAAAAAAKUEACgDIaj6fTzt2/D2tx9/25Zs4W1ZBue77\n0Y/V3d1tdzkAgHj0D0DpAAUAAAAAICkIQAEAWW3nzp0KBAIKjpEAVKapnilnqv3Afj366KN2VwMA\niMdgASgdoAAAAAAAJBwBKAAgq7W0tEiSAoVjJACVFCieKO/4k/TrX/9aL7/8st3lAABiNdgIXJ9P\nsiz7agIAAAAAIAMRgAIAstq2bdtkOHJk5ZbYXUpcPFNOl1Xo1op77tEHH3xgdzkAgFgMFoCGXwcA\nAAAAAAlDAAoAyGrbtm2Tv6BcMgy7S4mP6VRX7UL1ePz65jf/XR6Px+6KAADHM9gI3PDrAAAAAAAg\nYQhAAQBZKxAIaPt77ymQP3bG3/Zl5ZWoa/rH9d5723XffffJYoQiAKQ3r/dY16d0LADlPqAAAAAA\nACQUASgAIGu1trbK6/GMqft/9hcomyJP9TytX79eDz30kN3lAACGM9QIXAJQAAAAAAASyml3AQAA\n2GXbtm2SpGDB2A1AJclbfaoMX6+eeOIJFRYW6qqrrrK7JADAYLgHKAAAAAAAKUEACgDIWrt27ZIM\nQ8G8MrtLGR3DkGfqmTICXj300EMqKirSpZdeandVAIC+gsHQg3uAAgAAAACQdASgAICsFQwGJcOU\nzAyYCG8Y6p1+roygX/fdd586Ozv1uc99ToZh2F0ZAEA6FnIyAhcAAAAAgKTLgG98AQCAJMk01XPC\nQvnKa/XQQw/pvvvuk9/vt7sqAIA0eABKBygAAAAAAElBAAoAQCYxHeqtPU+eyrn6/e9/rzvvvEsd\nHR12VwUAGC4ApQMUAAAAAICEIgAFACDTGIa8k+vVO/VMbdy0Sdd+4Tpt2bLF7qoAILuFQ87w2Nu+\nPxOAAgAAAACQUASgAABkKN+EWeo6aYnauzz6yq236qGHHpLH47G7LADITsPdA5QRuAAAAAAAJJTT\n7gIAAEDyBIvGq2PWpcp9f6NWrVql5194QQ033KBPfOITMk3+DgoAUqZvABru+OQeoACQNE899dSw\n+y+//PIUVQIAAAA7EIACAJDpHDnyTD9HfvcJ2tf6V91zzz36f7/+tf75qqt09tlnK6dvNxIAIDkG\nC0AZgQsASfP0008Puc8wDAJQAACADEcACgBAlgiUVKlz1iVytr+n7bu36O6771ZpWZk+tWSJFi5c\nqBNOOEGGYdhdJgBkpsFG4DpLphy7AAAgAElEQVSd0fsAAAnzi1/8wu4SAAAAYCMCUAAAsolhyF9R\npw53rRyH2+Tf/65WPfGEVq1apbKycfo//+d0zZkzR7W1tZo+fbry8/PtrhgAMsNgAahphkJQOkAB\nIGn27t2rO+64Qx988IHWrVun5uZm/eUvf9G//Mu/2F0aAAAAkogAFACAbGSYCpRNVk/ZZBnebjmO\ntMl3uFXP//FFPffcc6FDDEPlbrcqJ1Zq4sQJqqio0Lhx41RWVhZ5lJaWqqysTPn5+XSPAsBwBgtA\npdAYXAJQAEiaa6+9Vl/4whd0zz33SJJmzJihz3zmMwSgAAAAGc60u4Cx5P7779f555+vBx98cNDt\nVatW6fzzz9eaNWskSX/84x91/vnna8OGDZKk7du3a8mSJdq+fbskafPmzfqHf/gHvfrqq5E1+h/T\n3t6um2++We3t7YNu91+j//7B9D9msDr6Ol5NsZwT7zViOT6WOoYz2vPTZY2RiLcuu95Hun5+o5Wp\n7wtjl+UqkL/iRPWesFBHPvZZdc69Qj11n1Bv1Tztscq09f12/fEvm/Wb/+8p/fznP9f3vvc9feMb\n39CXvvQlfe5zn9PixYt14YUX6rLLL9e1X/iCbrnlFt1555367ne/qx/96EdauXKlfvGLX2jVqlVa\ns2aNnnzySf3ud7/T008/rWeffVbPPfec/vu//1svvviiXn75ZW3cuFGvvfaatm7dqjfffFPNzc16\n5513Io+//e1vevvtt/Xmm29q69at2rJli15//XW9/vrr2rJli9544w29/fbb2rZtm3bu3Km2tja1\nt7erq6tLfr/f7o8bQLYKB6Dh+36GuVyMwAWAJDpw4ICuvPJKmWboKzCn0ymHw2FzVQAAAEg2OkDj\n8NRTT0mSfvOb32j58uUDth966CFJ0sqVK7Vs2TL953/+pyTpnnvu0cKFC7VixQp1dXVpxYoVevTR\nR3X33XcrGAzqW9/6lp555hlJGnDMY489pjfffFOPP/64br311gHb/dfov38w/Y8ZrI6+jlfTYPqf\nE+81Yjk+ljqGM9rz02WNkYi3LrveR7p+fqOVqe8LGcIwZOUWy59bLI2bGr3PsqSgX4avR4a/9+jD\nI8MX+rnb36t9Bz0yD+yVI7hbRsArBfxSwC8rmD7BY06OS/kF+SooKFRxcZFKS0pUcvRRXl6ucePG\nqby8XG63WxUVFSorK5PTyT+ZAIxSuMuzfwdoTg4BKAAkUWFhodrb2yPTSl555RWVlpbaXBUAAACS\njW/zYnT//fdHbfcflbJ8+fKo7RUrVkS6TPx+v1avXq2dO3dKknbu3Kmnn35anZ2dkqTOzk69+uqr\nKi0tjTrm1Vdf1fr162VZltavX69LLrkkavuEE06IWuPpp5+O2n/11VfL7XZH1dXe3h51zLx58wbU\nMX/+/Mjx27dvH7amwdbof86GDRviukb/8zdv3jzgeEnHrWM4/T+HeM9PlzVSUZdd7yNdP7/RytT3\nlY1y339FZvdHx14IeGX4vbKcLsnhGvpEScGCcnmmnJHkCpPAMCRHjixHjiyVxHeuZUlWMPIwrGDU\na0affdHbR4+RJanPiF3DOLbd92dZkXNC1whIwYCMYEAK+mQE/PIG/eoKeGV4fDK6u2R+cFBmwCvD\n3yvL5xnkbRsqLilVeXm5KtzlKi0tjQSmBQUFys/PV35+vnJzc+VyueRyuZSTkzPg4XQ6lZOTI4fD\nIafTKafTKcMwZJqmDMNQMBiUZVkKBoMKBAKRRzAYjLzWtybTNCPXCV873NmQCJZlyefzyePxqLe3\nVx6PR16vN/Lw+/3y+XyROi3LkmVZMgwj8nA4HJH3HP5swo/+n1esI5QtyxqwdvhzCn824bXDn0u6\ndXpYlqVAICCfzye/3y+/3x/5v79lWTJNM/Lo+/8/jJke44YagZuTwwhcAEiiH/7wh1q6dKnee+89\nnX322dq/f7+efPJJu8sCAABAkmVsAGr2HtH27R265ZZbRnT+9u3blZ+fH9kOd3uGvffee1Hbb7/9\ndtT2H/7wh6jtn//851HbP/zhD6O2v/Wtb6miomLAa+Ev8wKBgFasWBG1fd999w24ZvjLvUAgMGh3\n2WOPPRZ1jfA9MPqu2bcLdMWKFcPWNNga/c8ZbI3hrtH//HCHat/jw18QD1fHcPp/DsnoxEvFGiMR\nb112vY90/fxGK1PfVzYyuz+Ss2NPZDsvL0+fWvopPfPMM+rtG4wOwupujw5PYzBmQ9Mww5AMh6TQ\n7ymr3+7+27YJBkKdrd5uGb4emb5uGb5ueXw9aj/Yo7/v2yUz6JHh9wwalgJ2u/XWW3XppZfaXQYG\nwz1AAcAWp512ml588UW9++67sixLM2fOVE7//xYDAAAg4yT9HqBNTU2qr69XfX299u/fn+zlxgzL\niv6qt7OzM9L12Pe1vh2eO3fujNruf41wR0R4/wsvvDBg3T/84Q9Rx/S/F1q4GzTseDUNtkb/cwZb\nY7hrxLJm//cxWB3DGe356bLGSMRbl13vI10/v9HK1PeVaun4u+VTn/qUvvzlL2vJkiV2l4LRMB2y\nnLmycvJCz85cWc5jPweduQo6ciVnnoycXLurBQbo/285pJHhOkAZgQsASdPb26v7779f3/zmN/Wt\nb31LDz74oHp7e+0uCwAAAEmW9A7QhoYGNTQ0SJLq6+uTvVxEMK9EdbUT9eMf/3hE54+0czRWhmFE\nBZhFRUWqqKiICv+KiorU29srv98vp9Opmpoatba2RrbD4+b6XtPhcET2X3jhhQPWveCCC/Tss89G\njpGiA8qioqKo46dNmzZsTYOt0f8cp9M5YI3hrhHLmpZlRb2PweoYTv/PId7z02WNkYi3LrveR7p+\nfqOVqe8r1ez63TKcZ555RpZl6b/+67+Oe2ygwK2ekxanoKo0NugI3PB2oN8IXEvRPaLGsbG3xtHt\n8OsKHj3UCo29jYzA9YfuXxr0ywj4QiOLAz4ZAa8Mv0dmwCtHwCP5emT5B3ZimQ6HSkvLVD6+XOPK\nKlVaWqri4mIVFhYqLy9PeXl5ys/Pj4xzdblccjqdcrlckfGv4TGm4YfD4YiMspUUGX/ad+RteDRq\nLCNw+4/eHem41GAwGBl96/V6I+Nvw4/BRuCGawwGg5GRvoZhDFpf3xG44TG4fce8mqYZ+Wz6fibh\nP9wKP8LjY8OfTTAYjKwracB44L7r2DVKNvzZ9v8Mw3/YFv4MHQ7HoCNww58d96Qdo4brAO3qSn09\nAJAlrr76ahUXF+umm26SJK1evVqf//zn9Zvf/MbmygAAAJBMfHsSo8svvzxqDO4JJ5wQNQZ39uzZ\nUWNwL7jggqgxuF/84hejxuB+9atf1Q9+8IPI9re//W2Vlpbq+uuvj3rtG9/4hiTJ4XDorrvu0o03\n3hjZvvnmm6NG6X71q1/VT37yk8j+8L0y+7rmmmu0fv36yDG33367vv3tb0et2dddd901bE2DrdH/\nnDvvvHPAGsNdo//5d999t+64444Bx/d9H4PVMZz+n0O856fLGiMRb112vY90/fxGK1PfVzYKFpSr\nb397Z8CrXz/9nCxngVRcdtxzxyzLCt1L09cbGhXr75Xh65Xp75Xh98jw90p+r4yAV46gV0YwIMPy\nSwG/ZAVkBYNSMHD8dZLMMAzl5xeooLBQZeNLVVZWqZKSEo0bN07l5eUaN26c3G63Kioq5Ha7VVJS\nktD7a6Yz0zQj4aTdwoFmOOwd69Lps4UNuAcoANji3Xff1datWyPbCxcu1Mc+9jEbKwIAAEAqjP1v\nklLk5ptvjgpAH3nkEZ1//vmR7QcffDBq+6677tL//M//RLq8PvvZz+q5557Tzp07NW3aNF1yySX6\n+c9/rs7OThUVFWn+/PmSjnU/Tps2TfPnz9dFF12kp59+WhdddJHq6uqitpcuXar7778/ssYll1yi\nlpaWyH632z3gfbjd7qhrLFy4UD/4wQ8G1BFWV1c3bE2DrdH/nIULF+qxxx6L+Rr9z6+vrx/0+OPV\nMZz+n0O856fLGiMRb112vY90/fxGK1PfVzYa0/fjPB6/V46ej2T2HJTZe0SGt0sOX5cc/h5Z3p4h\nA0yn06nikhIVlxarpLhCxcVFys/PV15eXqRzzeFwRHVAhrf7vh7ueAt3wYW7AcOCwWBUt1y4S1BS\npPvQ4XBEdf2FuzRzc3OVn5+v/Px827oAAWSpcADqckW/7nIxAhcAkujUU0/VK6+8ojPOCP37fePG\njTr77LNtrgoAAADJRgAah3AX6BVXXDHo9g033KCHHnpIjY2NkqQ77rhD3/nOd3TnnXdKCoWit9xy\ni+666y5Joc7Gf/u3f4vqjux/zDXXXKOdO3dGusT6b/dfo//+wfQ/ZrA6+jpeTbGcE+81Yjk+ljqG\nM9rz02WNkYi3LrveR7p+fqOVqe8LY1jAJ0fHh3IebpOr4wOp53Bkl8uVq/ETxqty4hRVVFRo3Lhx\nGjdunMrKylRWVqbS0tLIg1ARQDKtX79et9xyiwKBgK6//nrdfvvtUftXrVql733ve5JCt1D42c9+\nFunwOd65KRHu8qQDFABSYs6cOTIMQz6fT48//rimTJkiwzC0a9cunXzyyXaXBwAAgCQzrL43kUyy\n+vp6bd68Oe7zlixZos5erzpP+7wkKf+dZyVp2Pun5b/zrOYn4B6gIz0fAJJlpP8tzVSj+Tyampr0\nxJr/p4751yS4qjHAsmR2HVDO/neUe3CHrIBfLleuTjvtVM2ZM0cnnHCCamtrNX78eEJNIEuk8++X\nQCCgGTNm6IUXXlBNTY0WLFig1atXR32B/fLLL2vWrFkaN26c1q1bp7vvvlsbN26M6dzBJPzz+NnP\npBtvlD78UPr974+9vnq1tGmTdN990tH7WwNAJrHr98uuXbuG3T916tQUVXJMOv+uBYCxhP+eAogF\nHaAAAGQTy5LjcKvyPnhdZtcB5ebm6cKLQyPR58yZI1f/0YwAkAY2bdqkuro61dbWSpKWLVumtWvX\nRoWYZ511VuTnM844Q62trTGfmxJD3QPU5aIDFACSoH/AuW/fPvX29tpUDQAAAFKNABQAgCxhdrcr\nb/df5TjygSqrqvTZhlt1wQUXqLCw0O7SAGBYbW1tmjx5cmS7pqZGGzduHPL4Rx55RBdffHHc5zY1\nNampqUmStH///kSUfsxwAajfLwWDiV0PACBJ+v3vf6+vfe1r+uCDDzRhwgTt2rVLs2bN0ttvv213\naQAAAEgiAlAAADJdMCBX22vK3fOmCouK9YUvf1mXXnqpcvp/CQ8AaWqwu3YMNZ57w4YNeuSRR/TS\nSy/FfW5DQ4Majo6hra+vH2m5gxsqAA1v+/2JXQ8AIEn65je/qVdeeUUXXHCBXn/9dW3YsEGrV6+2\nuywAAAAkGQEoAAAZzOg5rMIdL8roOqAlS5aosbFRxcXFdpcFAHGpqanR7t27I9utra2qrq4ecNwb\nb7yh66+/XuvWrZPb7Y7r3KQbrgNUYgwuACRJTk6O3G63gsGggsGgFi5cqK9//et2lwUAAIAkIwAF\nACBDOT/6uwp2vqTCgnx9/T/+Qx//+MftLgkARmTBggVqaWnRjh07NGnSJK1Zs0ZPPPFE1DHvv/++\nLr/8cv3yl7/UjBkz4jo3JXw+yTAkhyP69XAgSgAKAElRVlamzs5OnXvuubrqqqs0YcIEOZ18HQYA\nAJDp+BcfAACZxrKUs+ct5bX+VSefcoq+fffdqqiosLsqABgxp9OpBx54QIsWLVIgENB1112n2bNn\na+XKlZKkxsZGfec731F7e7tuvPHGyDmbN28e8tyU83pDYWf/8bvhDtBwhygAIKHWrl2rvLw83Xff\nfVq1apUOHz6sf//3f7e7LAAAACQZASgAAJnECir3/Y1y7fubzjvvPN1xxx3Kzc21uyoAGLXFixdr\n8eLFUa81NjZGfn744Yf18MMPx3xuyvl8A8ffSnSAAkCSFRYWRn6+5pprbKwEAAAAqUQACgBAprAs\n5e34X+W0v6crr7xSjY2NMk3T7qoAANLQASj3AAWApCguLpbRv+tekmVZMgxDR44csaEqAAAApAoB\nKAAga5mmKVnB0MMY40GhZSn3/VeU0/6errvuOl199dV2VwQA6Ot4ASgjcAEgoTo6OuwuAQAAADYa\n49/2AgAwcjU1NZJlyewd+3/97Wp7Ta59f9NnPvMZff7zn7e7HABAf3SAAgAAAACQMgSgAICsNWPG\nDEmS2d1ucyWjk7PnbeV+uFVLlixRY2PjoKO+AAA2IwAFAAAAACBlCEABAFlrypQpysnJkaPrgN2l\njJjjyIfKa92kc875uL761a8SfgJAujpeAOrxpLYeAAAAAAAyGAEoACBrOZ1OTa+tlaP7I7tLGRHD\n263CHS+qurpa3/jG7XI4HHaXBAAYitd7LOzsiw5QAAAAAAASjgAUAJDVZs6YIWfPR5Jl2V1KfIJB\nFfx9g3IU0Ir/+A8VFhbaXREAYDiMwAUAAAAAIGUIQAEAWW3GjBmy/B4Zng67S4mLq+1VmR17ddtt\n/6ra2lq7ywEAHA8BKAAAAAAAKUMACgDIaieeeKIkjakxuGZXu3L3vqUlS5bowgsvtLscAEAshgpA\nTVNyOglAAQAAAABIIAJQAEBWmz59ukzTlNl9wO5SYmMFlf/+yyopKVVjY6Pd1QAAYjVUACpJubkE\noAAAAAAAJBABKAAgq+Xm5mrK1GlydLfbXUpMcvZvk9m5X19efqOKi4vtLgcAEKvhAlCXK7QfAAAA\nAAAkBAEoACDrzZxxonJ6PpIsy+5ShmX4epTf9qrmzZvH6FsAGGuOF4DSAQoAAAAAQMIQgAIAst6M\nGTNkeXtk+HrsLmVYubv/KtPy69Zbb5VhGHaXAwCIh9cbCjoH43JJHk9q6wEAAAAAIIMRgAIAst6J\nJ54oSWl9H1Cj94hy2rfriiuu0NSpU+0uBwAQLzpAAQAAAABIGQJQAEDWq6urk2EYcnR/ZHcpQ3Lt\neUtOZ46uuOIKu0sBAIwEASgAAAAAAClDAAoAyHoFBQWqqq6W2ZWeHaCGr0e57S266KJFcrvddpcD\nABgJAlAAAAAAAFKGABQAAEknzZwpV+9Bu8sYVM7eZllWUJ/5zGfsLgUAMFIEoAAAAAAApAwBKAAA\nCt0H1OrtkPweu0uJFvApb/87+vg552jy5Ml2VwMAGCkCUAAYU9avX6+ZM2eqrq5O995775DH/fWv\nf5XD4dCTTz6ZwuoAAABwPASgAABIqqmpkSSZng6bK4mWs3+bLL9Hy5Yts7sUAMBoEIACwJgRCAS0\nfPlyrVu3Ts3NzVq9erWam5sHPe7rX/+6Fi1aZEOVAAAAGA4BKAAAkiorKyVJpqfT5kr6sCzl7Xtb\np8yZo9mzZ9tdDQBgNLzeUNA5GAJQAEgrmzZtUl1dnWpra+VyubRs2TKtXbt2wHE/+clP9I//+I+a\nMGGCDVUCAABgOASgAADoWABqeNOnA9TRuVfydOqyT3/a7lIAAKN1vA5Qv18KBFJbEwBgUG1tbVG3\nn6ipqVFbW9uAY37729+qsbEx1eUBAAAgBgSgAABIKioqUl5+QVp1gDoP7pTTmaMzzzzT7lIAAKN1\nvABUkrq7U1cPAGBIlmUNeM0wjKjtr3zlK/re974nh8Mx7LWamppUX1+v+vp67d+/P6F1AgAAYGhO\nuwsAACAdGIahyspKdX6UJgGoZcl16H2dfvoCFRQU2F0NAGA0AgHJsmILQIuLU1cXAGBQNTU12r17\nd2S7tbVV1dXVUcds3rxZy5YtkyQdOHBAzz77rJxOpz7db3pLQ0ODGhoaJEn19fVJrhwAAABhBKAA\nABw1qbpKO/f9ze4yJElm137J06nzzz/f7lIAAKPl84We6QAFgDFhwYIFamlp0Y4dOzRp0iStWbNG\nTzzxRNQxO3bsiPx87bXX6lOf+tSA8BMAAAD2IQAFAOCoyspKGZ6/hrp0+o24SrWcgzvlcDgYfwsA\nmeB4AWhubuiZABQA0oLT6dQDDzygRYsWKRAI6LrrrtPs2bO1cuVKSeK+nwAAAGMAASgAAEdVVlbK\n8nulgFdy5tpXiGXJdWiX6uvrVcwoRAAY++gABYAxZ/HixVq8eHHUa0MFn48++mgKKgIAAEA8TLsL\nAAAgXVRWVkqSTE+HrXWY3e1SbwfjbwEgU3i9oedw0NkfASgAAAAAAAlFAAoAwFHHAtBOW+twfrRT\npsOhs88+29Y6AAAJQgcoAAAAAAApRQAKAMBR4QDU8NoYgFqWcg/v0qmnnqqSkhL76gAAJE6sAWhX\nV2rqAQAAAAAgwxGAAgBwVFFRkfLyC2ztADU8R6Sewzr34x+3rQYAQILRAQoAAAAAQEoRgAIAcJRh\nGKqsrJTpte8eoI6OvZKkefPm2VYDACDBCEABAAAAAEgpAlAAAPqYVF0lh9e+EYSOzn0qLCzS5MmT\nbasBAJBgBKAAAAAAAKQUASgAAH1MnDhRprdTsixb1nd179ecOafINPkVDQAZgwAUAAAAAICU4ttV\nAAD6qKyslOX3SgFv6hf3e6TugzrllFNSvzYAIHm8R3+nhIPO/hyO0IMAFAAAAACAhCAABQCgj8rK\nSkmS6elM+dqOzn2SRAAKAJnmeB2gUigcJQAFAAAAACAhCEABAOgjEoB6O1K+tqNzr0yHQyeddFLK\n1wYAJBEBKAAAAAAAKUUACgBAH+EA1LChA9TZuU91dXXKy8tL+doAgCSKNQDt6kpNPQAAAAAAZDgC\nUAAA+iguLlZeXn7qR+AGg3J2H9Acxt8CwKDWr1+vmTNnqq6uTvfee++A/e+8847OPPNM5ebm6vvf\n/37UvmnTpmnOnDmaN2+e6uvrU1XyMXSAAgAAAACQUk67CwAAIJ0YhqHKykp1HEptAGp2t8sK+DVn\nzpyUrgsAY0EgENDy5cv1wgsvqKamRgsWLNDSpUt18sknR44pLy/X/fffr9/97neDXmPDhg2qqKhI\nVcnRCEABAAAAAEgpOkABAOinurpKTl9qA1BH5z5J0uzZs1O6LgCMBZs2bVJdXZ1qa2vlcrm0bNky\nrV27NuqYCRMmaMGCBcoZLmS0CwEoAAAAAAApRQAKAEA/lZWVMj2pvQ+bo3Ovxo+foPHjx6d0XQAY\nC9ra2jR58uTIdk1Njdra2mI+3zAMffKTn9T8+fPV1NQ05HFNTU2qr69XfX299u/fP6qao3i9oWeX\na+hjCEABAAAAAEgYRuACANBPZWWlLL9H8nskZ27yF7Qsubr3a+45ZyR/LQAYgyzLGvCaYRgxn//n\nP/9Z1dXV2rdvny688EKddNJJOvfccwcc19DQoIaGBklK7L1CY+0A7UrtH98AAAAAAJCp6AAFAKCf\niRMnSpJMb2rG4BreLlmeLp1yyikpWQ8Axpqamhrt3r07st3a2qrq6uqYzw8fO2HCBF122WXatGlT\nwmscFiNwAQAAAABIKQJQAAD6qaiokCQZ3tR8Ee3obpcknXTSSSlZDwDGmgULFqilpUU7duyQ1+vV\nmjVrtHTp0pjO7erqUkdHR+Tn559/PvV/cEIACgAAAABASjECFwCAftxutyTJ9PUokIL1zJ5DkqSp\nU6emYDUAGHucTqceeOABLVq0SIFAQNddd51mz56tlStXSpIaGxu1Z88e1dfX68iRIzJNUz/60Y/U\n3NysAwcO6LLLLpMk+f1+fe5zn9NFF12U2jfACFwAAAAAAFKKABQAgH7Ky8slSYYvNZ04Zs9BVYwf\nr4KCgpSsBwBj0eLFi7V48eKo1xobGyM/V1ZWqrW1dcB5JSUl2rp1a9LrG1asAWhPjxQMSiaDegAA\nAAAAGA3+lzUAAP24XC4VFhWlLAB1eg7rhNralKwFALBBLAFobm7oubc3+fUAAAAAAJDhCEABABiE\n2+2W4e1J/kJWUGbPYcbfAkAm83pDXZ0Ox9DHuFyhZ+4DCgAAAADAqBGAAgAwiPEVFXL4kx+AGp5O\nWUG/pk+fnvS1AAA28fmG7/6UCEABAAAAAEggAlAAAAbhdrtTEoA6eg5KEh2gAJDJCEABAAAAAEgp\nAlAAAAbhdrtlebsly0rqOmbPIUnStGnTkroOAMBGBKAAAAAAAKQUASgAAIMoLy+XggEp4E3qOmbP\nIbkrxqugoCCp6wAAbEQACgAAAABAShGAAgAwCLfbLUkyfcn9ItrpOaQTarn/JwBkNI9Hys0d/hgC\nUAAAAAAAEoYAFACAQYQDUMObxC+iraDMnsOMvwWATOf1xh6AdnUlvx4AAAAAADIcASgAAIOIBKC+\nnqStYXg6ZQX9mjp1atLWAACkAa/3WMA5FDpAAQAAAABIGAJQAAAGUV5eLim5I3DNnkOSRAcoAGQ6\nj4cAFAAAAACAFCIABQBgEAUFBcrNy0tqB6ij96Ak0QEKAJkunhG4BKAAAAAAAIwaASgAAEMoLy+X\nkeQO0HJ3hYqKipK2BgAgDTACFwAAAACAlCIABQBgCOMrKmQmsQPU2XtYtdOnJe36AIA0EcsIXKdT\nMk0CUAAAAAAAEoAAFACAIbjdbjn8SQpALUtm7yFNnz49OdcHAKSPWEbgGoZUUEAACgAAAABAAhCA\nAgAwBLfbnbQRuIanQ1bAz/0/ASAbxNIBKhGAAgAAAACQIASgAAAMwe12y/L7pIAv4dc2ew9JkqZN\nm5bwawMA0kws9wCVpMJCqasr+fUAAAAAAJDhCEABABiC2+2WpKR0gZq9HZKkyZMnJ/zaAIA0E8sI\nXEkqK5MOHkx+PQAAAAAAZDin3QXEIj8/Xx1eK6Vr1tXVpXQ9AED6KS8vlySZvh4F8koTem3T06G8\n/AKVlJQk9LoAgDQU6wjcigrpwIHk1wMAAAAAQIYbEwHopEmTtMezN6Vr3nTTTSldDwCQfiIdoN4k\ndIB6OlRdVSXDMBJ+bQBAmol1BG5FhbRjR/LrAQAAAAAgwzECFwCAIRwbgduT8Gs7fZ2qqZmU8OsC\nANJQrCNw3W6pvT359Vi57OYAACAASURBVAAAAAAAkOEIQAEAGEJxcbEcTmfi7wFqWTJ6O1RVVZXY\n6wIA0lM8I3APHpT8/uTXBAAAAABABiMABQBgCIZhaFzZOJkJDkANX4+soJ8AFACygWXFNwJXkj76\nKLk1AQAAAACQ4QhAAQAYRkVFRcJH4JqeI5JEAAoA2SAQCIWgsY7AlRiDCwAAAADAKBGAAgAwjIoK\nt5z+xAaghqdDklRdXZ3Q6wIA0pDHE3qOpwP0wIHk1QMAAAAAQBYgAAUAYBhutzsJHaAdMgxDEydO\nTOh1AQBpyOsNPROAAgAAAACQMgSgAAAMw+12y/L1SsFAwq5pejpU7q6QK5YvwwEAY1s4AGUELgAA\nAAAAKUMACgDAMMrLyyUpoV2gpqdTkyYx/hYAskI8I3DDASgdoAAAAAAAjAoBKAAAw3Af/TLa8HUn\n7JpOX6cmcf9PAMgO8YzALSgIPQhAAQAAAAAYFQJQAACGEQ5AzUQFoAG/LE+XqqqqEnM9AEB6i2cE\nrhTqAmUELgAAAAAAo0IACgDAMI51gCZmBK7p7ZAkVdMBCgDZIZ4RuJJUUUEHKAAAAAAAo0QACgDA\nMMrKymQYRsICUMMTCkDpAAWALBHPCFwp1AFKAAoAAAAAwKgQgAIAMAyHw6HiklIZ3sSMwDU9dIAC\nQFaJdwRuRQUjcAEAAAAAGCUCUAAAjqOiwi0zUSNwPR3Kzc1TWVlZQq4HAEhzjMAFAAAAACDlCEAB\nADiO8RUVMv2JGoHbqcqqKhmGkZDrAQDS3EhG4B48KPn9yasJAAAAAIAMRwAKAMBxlJeXy5GgADTH\n26maSYy/BYCsMZIRuFIoBAUAAAAAACNCAAoAwHG43W5Z3m7JskZ3IcuS4TnC/T8BIJuMZASuxBhc\nAAAAAABGgQAUAIDjKC8vD4WX/t5RXcfw98oK+FVVVZWgygAge6xfv14zZ85UXV2d7r333gH733nn\nHZ155pnKzc3V97///bjOTaqRjMCVCEABAAAAABgFAlAAAI6jvLxckmT4RjcG1/B0SBIdoAAQp0Ag\noOXLl2vdunVqbm7W6tWr1dzcHHVMeXm57r//fv3rv/5r3Ocm1UhH4La3J6ceAAAAAACyAAEoAADH\n4T7ajWP4ukd1HfNoAEoHKADEZ9OmTaqrq1Ntba1cLpeWLVumtWvXRh0zYcIELViwQDk5OXGfm1SM\nwAUAAAAAIOUIQAEAOI5jAejoOkDDAWhlZeWoawKAbNLW1qbJkydHtmtqatTW1pb0cxNipCNw6QAF\nAFsdb3z6qlWrNHfuXM2dO1dnnXWWtm7dakOVAAAAGIrT7gIAAEh34RG45ig7QA1Pp0rLxik31jGI\nAABJkmVZA14zDCPh5zY1NampqUmStH///jgqHEa8I3ALCqT8fDpAAcBG4fHpL7zwgmpqarRgwQIt\nXbpUJ598cuSY6dOn68UXX9S4ceO0bt06NTQ0aOPGjTZWDQAAgL7oAAUA4Djy8vKUl18gwzvKDlBv\nh6qr6P4EgHjV1NRo9+7dke3W1taY76ccz7kNDQ3avHmzNm/erPHjx4+u6LB4R+BKoTG4BKAAYJtY\nxqefddZZGjdunCTpjDPOUGtrqx2lAgAAYAgEoAAAxKC8vHzU9wB1+rq4/ycAjMCCBQvU0tKiHTt2\nyOv1as2aNVq6dGnSz00Ir1cyDMnhiP0ct5sRuABgo3jHpz/yyCO6+OKLU1EaAAAAYsQIXAAAYjC+\nwq3df98z8gtYQam3k/t/AsAIOJ1OPfDAA1q0aJECgYCuu+46zZ49WytXrpQkNTY2as+ePaqvr9eR\nI0dkmqZ+9KMfqbm5WSUlJYOemzIeT2j8bYwjeyXRAQoANotnfPqGDRv0yCOP6KWXXhp0f1LGqwMA\nAOC4CEABAIiB2+2WY9vOEZ9v+HokK0gACgAjtHjxYi1evDjqtcbGxsjPlZWVQ44fHOzclPF64xt/\nK4U6QHftSk49AIDjinV8+htvvKHrr79e69atk9vtHvRaDQ0NamhokCTV19cnp2AAAAAMwAhcAABi\nUF5eLsM78hG4pqdDkjRx4sRElQQAGAtGEoBWVDACFwBsFMv49Pfff1+XX365fvnLX2rGjBk2VQoA\nAICh0AEKAEAM3G63rIBPCvgkR07c5xveLkmiAxQAsk14BG48Kiqkgwclv19y8j/ZACDVYhm9/p3v\nfEft7e268cYbI+ds3rzZzrIBAADQB/9rGgCAGIRHWhm+blmO0rjPD3eAEoACQJYZSQfoxImSZUl7\n9kg1NcmpCwAwrOONXn/44Yf18MMPp7osAAAAxIgRuAAAxKC8vFySZPp6RnS+4elUSWmZcuPtAgIA\njG1eb/wdoLNmhZ6bmxNfDwAAAAAAWYAAFACAGIQDUMM3svuAmt5OVVfR/QkAWcfjib8D9JRTQs9v\nvZX4egAAAAAAyAIEoAAAxCAyAtc7sg5Qp69LVVVViSwJADAWjGQEbkWFVFkpvflmcmoCAAAAACDD\nEYACABCDkpISORwOGSMZgWtZkqeT+38CQDYayQhcKdQFSgcoAAAAAAAjQgAKAEAMDMNQ2bhxMkcw\nAtfw9UjBAAEoAGSjkYzAlUIB6NtvS8Fg4msCAAAAACDDEYACABCjioqKEXWAGp4OSSIABYBsNJIR\nuFIoAO3pkXbsSHxNAAAAAABkOAJQAABiVOF2yxGIPwA1vZ2SCEABICuNdATunDmhZ+4DCgAAAABA\n3AhAAQCIUXl5ucwRdICanlAAOnHixESXBABIdyMdgXvyyaFn7gMKAAAAAEDcnHYXAADAWOF2u2V5\ne0L3YzNj/xsiw9uhktJS5eXlJbE6AEBaGukI3KIiafr0UADa1DT4MQ0No6sNAAAAAIAMRQcoAAAx\nKi8vlyQZ/vi6QE1Pp6qqqpJREgAg3Y10BK4Uug8oI3ABAAAAAIgbASgAADFyu92SJCPOMbhOX7eq\nuP8nAGSnkY7AlUL3Ad22TfL5ElsTAAAAAAAZjgAUAIAYHQtAu2M/ybIkT4cqCUABIDuNdASuFOoA\n9fulffsSWxMAAAAAABmOABQAgBiFR+CacXSAGr4eKRggAAWAbDXaEbiS1NaWuHoAAAAAAMgCBKAA\nAMSovLxchmHI8HbFfI7h7ZQkAlAAyFajGYE7c6bkdEoffJDYmgAAAAAAyHAEoAAAxMjpdMpdMV5m\nb0fM55ie0LEEoACQhQKB0GOkAajLJZ1wgvThh4mtCwAAAACADEcACgBAHGomVcv0xhGAHu0AnThx\nYrJKAgCkK58v9DzSEbiSNGuWtHdvYuoBAAAAACBLEIACABCH6upqOX2dMR9veDpUXFKq/Pz8JFYF\nAEhLHk/oeaQdoJJ00knSvn2hTlIAAAAAABATAlAAAOJQVVUly9MtBfwxHe/oPaIpkycnuSoAQFry\nekPPow1AAwFp//7E1AQAAAAAQBYgAAUAIA5VVVWSFPMYXKfniKZOnZLMkgAA6SocgI52BK4k7dkz\n+noAAAAAAMgSBKAAAMQhHIAanhjG4Pq9srzdmkwHKABkp0SMwJ05M/RMAAoAAAAAQMwIQAEAiEOk\nA9Rz/A5Qs/ewJBGAAkC2SsQI3NJSqayMABQAAAAAgDg47S4AAICxZNy4cXK5cuUlAAUAHE+8I3Cb\nmgZ/feJEAlAAAAAAAOJABygAAHEwDEOVVZUyYgxATdNUdXV1CioDAKSdRIzAlaSqqlAAalmjrwkA\nAAAAgCxAAAoAQJxqJk2S03f8e4CavYc1sbJKOTk5KagKAJB2EjECV5IqK6WeHunIkdHXBAAAAABA\nFiAABQAgTlVVVTI9ncftxHF6jmja1CkpqgoAkHbCHaCxjsAdSmVl6JkxuAAAAAAAxIQAFACAOFVV\nVcnye2X4PUMfZAVl9B7m/p8AkM0S2QEqEYACAAAAABAjAlAAAOJUVVUlScPeB9TwdknBgKZMoQMU\nALJWogLQsjIpL0/68MPR1wQAAAAAQBYgAAUAIE7hANQcJgA1ew9LEgEoAGSzRI3ANYxQFygdoAAA\nAAAAxIQAFACAOFUeHUVoeocJQHtCASgjcAEgiyWqA1QKBaB0gAIAAAAAEBMCUAAA4lRQUKCS0tJh\nR+CavYdVUFiosrKyFFYGAEgriQxAa2qkQ4ekI0dGfy0AAAAAADKc0+4CAAAYi6qrqvXRB4eH3G96\nDmvK5MkyDCOFVQEA0kqiRuBK0vTpoecdO6SPfWz01wMAAMCY5PP51Nraqt7eXrtLSbq8vDzV1NQo\nJyfH7lIAjEEEoAAAjMCkSdV6d1ebeobYn+M5oqlT56a0JgBAmklkB+iUKZJpEoACAABkudbWVhUX\nF2vatGkZ/UfXlmWpvb1dra2tmh7+Y0AAiAMjcAEAGIGqqipZvZ2SFRy4M+CT5eni/p8AkO0SGYC6\nXNKkSdLOnaO/FgAAAMas3t5eud3ujA4/JckwDLnd7qzodAWQHASgAACMQFVVlWQFZXi7Buwze0Oj\ncQlAASDLJXIErhQag7tzpxQc5I9vAAAAkDUyPfwMy5b3CSA5CEABABiBqqoqSZLp6RywjwAUABJv\n/fr1mjlzpurq6nTvvfcO2G9Zlm6++WbV1dVp7ty5eu211yL7pk2bpjlz5mjevHmqr69PXdHhDtBE\n3bNo2jSpp0faty8x1wMAAAAAIEMRgAIAMALhANTwdAzYZ/YelmEYmjRpUqrLAoCMFAgEtHz5cq1b\nt07Nzc1avXq1mpubo45Zt26dWlpa1NLSoqamJn3pS1+K2r9hwwZt2bJFmzdvTl3hXm8o/EzUX66H\n7320Y0dirgcAAACMQFFR0bD7d+7cqVNOOSWua1577bV68sknR1MWAEQhAAUAYATGjx8v0zRl9h4Z\nsM/sOaTxEyYqN1EjDwEgy23atEl1dXWqra2Vy+XSsmXLtHbt2qhj1q5dq6uvvlqGYeiMM87QoUOH\n9OGHH9pU8VEeT+LG30pSZaWUl0cACgAAAADAcRCAAgAwAk6nUyfNmiXXkd2SZR3bEfDJdaRN9fNP\ns684AMgwbW1tUWPFa2pq1NbWFvMxhmHok5/8pObPn6+mpqYh12lqalJ9fb3q6+u1f//+0Rfu9Uou\n1+ivE2aaoTG4BKAAAABIA52dnfrEJz6h0047TXPmzIn6I0W/369rrrlGc+fO1T/90z+pu7tbkvTq\nq6/qvPPO0/z587Vo0SL7/2gRQMYiAAUAYIQWX3yxjO6DMrsORF5zHtwlK+DTokWLbKwMADKL1fcP\nTY4y+o2VHe6YP//5z3rttde0bt06Pfjgg/rTn/406DoNDQ3avHmzNm/erPHjx4++8EQHoFIoAG1t\nPXZ/UQAAAMAmeXl5+u1vf6vXXntNGzZs0Ne+9rXIv8vfffddNTQ06I033lBJSYl++tOfyufz6aab\nbtKTTz6pV199Vdddd53uvPNOm98FgExFAAoAwAidf/75ysnJUU57S+Q1V/t2Tays1Jw5c2ysDAAy\nS01NjXbv3h3Zbm1tVXV1dczHhJ8nTJigyy67TJs2bUpB1Ur8CFwpdB/QYFDq814BAAAAO1iWpTvu\nuENz587VBRdcoLa2Nu3du1eSNHnyZJ199tmSpH/+53/WSy+9pHfffVdvvfWWLrzwQs2bN08rVqxQ\na2urnW8BQAYjAAUAYISKiop07rnnKvfgDinol+HplOPIB7r4ootkmvyKBYBEWbBggVpaWrRjxw55\nvV6tWbNGS5cujTpm6dKlevzxx2VZll555RWVlpaqqqpKXV1d6ujokCR1dXXp+eef1ymnnJKawpPR\nATp9euh5167EXhcAAACI06pVq7R//369+uqr2rJliyZOnKje3l5JAye2GIYhy7I0e/ZsbdmyRVu2\nbNGbb76p559/3o7SAWQBvp0FAGAULr74Ylk+j5yHdiun/T1JYvwtACSY0+nUAw88oEWLFmnWrFm6\n8sorNXv2bK1cuVIrV66UJC1evFi1tbWqq6vTDTfcoJ/+9KeSpL179+qcc87Rxz72MZ1++ulasmSJ\nLrrootQU7vUmvgO0pEQqLqYDFAAAALY7fPiwJkyYoJycHG3YsEG7+vyR3vvvv6+//OUvkqTVq1fr\nnHPO0cyZM7V///7I6z6fT2+//bYttQPIfE67CwAAYCw79dRT5XZXaO+BFjm9HZozd66qqqrsLgsA\nMs7ixYu1ePHiqNcaGxsjPxuGoQcffHDAebW1tdq6dWvS6xuUx5P4DlDDkCZPJgAFAACA7a666ipd\ncsklqq+v17x583TSSSdF9s2aNUuPPfaYvvjFL+rEE0/Ul770JblcLj355JO6+eabdfjwYfn9fn3l\nK1/R7NmzbXwXADIVASgAAKPgcDh08cUX6Ve/+pUk6eJUdRUBANJfMkbgSqEA9L//O3nXBwAAAIbR\n2dkpSaqoqIh0c/bX3Nw86Ovz5s3Tn/70pwGvP/roowmrDwCkMRSAOro/Uv47zx79uV2SIttDHS9N\nTEVpAIAs98lPflK/+tWvlONy6bzzzrO7HABAukjGCFwpFID6/dI770hz5yb++gAAAAAAjHFjIgCt\nq6uL2m5r80uSJk0aLuCcOOA8AACSYcqUKTr77LNVVVWlwsJCu8sBAKQLjyd0v85Emzw59LxlCwEo\nAAAAAACDGBMB6E033WR3CQAADOuee+6xuwQAQLpJ1ojaCROknBzp9delq69O/PUBAAAAABjjTLsL\nAAAAAICMlKwRuKYp1dSEOkABAAAAAMAAY6IDFAAAAADGHI8nOR2gUmgM7pYtkmVJhhG9r6lp8HMa\nGpJTCwAAAAAAaYYOUAAAAABIhmSNwJVCHaCHDknvv5+c6wMAAAAAMIYRgAIAAABAMng8yRmBK4U6\nQCXG4AIAACDl1q9fr5kzZ6qurk733nuv3eUAwKAIQAEAAAAgGZLdAWqaBKAAAAAY1u9eb9PZ9/5R\n02//L5197x/1u9fbRnW9QCCg5cuXa926dWpubtbq1avV3NycoGoBIHEIQAEAAAAgGZIZgLpc0owZ\n0uuvJ+f6AAAAGPN+93qbvvHUm2o71CNLUtuhHn3jqTdHFYJu2rRJdXV1qq2tlcvl0rJly7R27drE\nFQ0ACUIACgAAAADJkMwRuJJ07rnSc89JbaP7K34AAABkpv/73Lvq8QWiXuvxBfR/n3t3xNdsa2vT\n5PDtGCTV1NSojX+PAkhDBKAAAAAAkGiWJfl8yesAlaTbb5cCAWnFimNrfv/7jMUFAACAJOmDQz1x\nvR4Ly7IGvGYYxoivBwDJQgAKAAAAAInW3R16LihI3hrTp0s33CA9/LD0979L3/2udNttoe19+5K3\nLgAAAMaE6rL8uF6PRU1NjXbv3h3Zbm1tVXV19YivBwDJQgAKAAAAAIn24Yeh58rK5K5z552S0yld\nemno509/OrT9+ONSMJjctQEAAJDWbls0U/k5jqjX8nMcum3RzBFfc8GCBWppadGOHTvk9Xq1Zs0a\nLV26dLSlAkDCEYACAAAAQKKFA9CqquSuU10t3XST9NZb0vnnS2vWSFdcIbW0SC++mNy1AQAAkNY+\nfeokfffyOZpUli9D0qSyfH338jn69KmTRnxNp9OpBx54QIsWLdKsWbN05ZVXavbs2YkrGgASxGl3\nAQAAAACQcT74IPScinFgd90lTZggXX+9lJsrnXWW9P+3d+fxUdX3/sdfsyWThSQEEkgyYV9MAiFA\ncNe2WGgfWENVasFStW6txUp/1Z8/+/u1Snv7UNtbWzfutVhbcSnULheoQFxAr73KYoBgQcAoCWYT\nQkiAhCSTzJzfH4cJ2YAgE+ac8H4+HvOYnJMzc95zZsKHk0++31NUBH//O+TlwcCBfZ9BRERERCzp\n65Mzzqrh2ZNZs2Yxa9assD6niEi4qQEqIiIiIiISbudqBChAQgLcd9+JZYcDbrwRHnoI1qyBb32r\n7zOIiPQzhYWFLFy4kEAgwO23384DDzzQ6fuGYbBw4ULWrFlDbGwszz//PFOmTOmTLDVHW/AHgjiA\nGI8ThwMaW4LgAI/LQbzHRV1TGwBOJ6QlxlBZdwwDcDocpCfFUFnXhIGBAxgyIIqDja0EDMCARK+b\n5kAAf5uBAcRGucx9NAfMfTgdJHjd1Db6AQcuJwztsA+Hw0FGUgzV9U0EDAMHDlLjozjS0kZLmzkd\ne3KcB39bkIaWAADx0W7cLgf1x1oBiHY7SYqN4lBjC4EgRHucDIyN6nYs6o75aWkN4nJCclw09cf8\n7ftIivXgwMGR5lYcwACvm5iozr/6DAYNak+zj64amltp9AdwAEmxUQQNgyNNZu54r5vYqNP/erW2\noYXWQBC308ngAdGn3f5ocytHm833NNbjIt7r5tAxP4YBXo+TBK+Hgw1+AkGDKLeT5LjTv45I8LcF\nqT/mxwDiolzEez2RjgR0/xy5nI5IRxIRkT6gBqiIiIiIiEi4VVdDVBQkJ0dm/ykpcMUV8M478OUv\nw5AhkckhImJDgUCABQsW8MYbb+Dz+Zg2bRoFBQVkZ2e3b7N27VpKSkooKSlh06ZN3HXXXWzatCns\nWfYfaeaJN0v4y5ZyolxOViy4jPW7D/DU+o9pbg1w3RQf984Yx5z/eJfqoy0svnEyw5JbuGd5MaUH\nG8lJT+DpeZPZuPcgP/6vneSkD+C5m6fx4MqdvLFrPwNjo3jwa9lkDvQy79nNuJwOfvftqZTVNvLv\nhXtoag0wOy+de64ay9wlG6g+3MKlowfxqzm5/GDZNrZ9Wk9W2gCemjeFxW99zH9tq2TEoFhevO0i\nHlmzm8Kd1STEePjp1dnERrn44Z+LcTjgtstHMmtCGvN+v5HGlgAPfHU8ORmJ3PvKdqoPN3PxqGQe\n/+ZkhiZ624/FZ4eb+eGft7Fx7yF8A2NYdsfF/Ptru1n9r8+Ii3bxf75yAXmZiVz/zAYMA75z2Qju\nvHJ0e3PQ3xbgg4rDLFxeTGV9E/nDB/LUvMmkJcWc9PjXNrTwyNrdrNhWSWyUed3EkYPjuOOFLQSC\nBt++eBgLpo89ZQNyX20j3395KzurjjBycByLb5zC+KEDTtp0O3CkmVeKyvnPtz/BHwjy6LUTifN6\n+MmKf1Hb6Gf6+FR+PjuH25YWsfuzo+T6Ell84xQyk2M/56esbxxpamXV9ip+VbibY/4AX8/L4Mez\nLmBQ/OkbwH2p4+coLdHLb26YxJRhA4nucp1MERGxP10DVEREREREJNyqq2HoUHM0ZqRcfTV4PLBy\nZeQyiIjY0ObNmxkzZgyjRo0iKiqKuXPnsrLLv6UrV67kpptuwuFwcPHFF1NfX091aPR/GL21+wB/\n2vwprQGDzIGxNLS08cja3TS0tNEWNHilqJzXdn7GL66bAMCkzCRuW1pE6cFGAHZWHeEHy7dx+dgU\nAG6+ZAS/e2cvr3+4H8OAQ41+/tcrxSTHRxPtceL1OHE6HDy4cidHj+/jb1srWbW9iuz0RADe+6SW\nX67dzSWjBgGwq/oo3395C1eOM/eRlzmQFzbsY82OaoIG1B9r5d6/bGfwgGicDgfNrUEWv/UJn9Q0\nkBLvJRA0mDI8mVuff5/qw80AbNx7iEWrdnK02RxpebS5lYdW7WDj3kMA5PoSWf5+Oau2VxMIGhxp\nauP/rdiBy+VkYEwULW1BnvnvvRSX17cfy7pjrdz8h81U1jcBULSvjv/7X//i8PHRnF0Fggartlfx\n1y0VtAUNjjS38dOVO3E4HHg9TvyBIM+9W8bGvbUnff9qG1ram58ApQcbueWPm6ltbDnpYz49dIxf\nv/4RjX5ztOyo1Hi+//IWDjaYI0DX7T7A0299zLQR5h9ZhZq6hxr9J33OSPjsSDM/WbGDI83m5+iv\nWytYtb2KQNCIWKajza0sWrWz/XNUfbiZ7zz/PvUn+QyIiIi9qQEqIiIiIiISbtXV52b621NJSDBH\nf27ZAvv2RTaLiIiNVFZWkpmZ2b7s8/morKw8423OVv0xP29/VNO+fOX4wbz7cfdm29sf1TBqcDxg\nTjl64Gjn5tqOyiM4jv9BzuiUeDZ80vk5DMNslOZmJDI6JZ7tFfV0tXFvLdlpCSeWSw+R1WH5o/0N\npB8frZmdnsDm0u45d1UfITP5xGjLzaWHyE4bQLTbSWNLG82twU7bv7f3IE3Hm4BN/gAbOjQas9MS\ne9zHlrI6Lh49qH157Y4TTemG5rb2pmLIhr21tLR2XhdyzN/Gul0Huq0vLq9nVEp8+3Lhjs9oCwS7\nbQfQGjDam58hB4620OzveZ8A75SceM9T4qMpqz1G157hpr2dj//WT+tOmiFSisoOdVv35q79NLa0\nRSCNqevnCKC5NUhtw8kb0iIiYl9qgIqIiIiIiIRbVRWkp0c6BcyYAQMGwMsvQ1vkfuEoImInhtF9\nhJqjy4j+3mwDsGTJEvLz88nPz6empqbb909lQLSLqcMHti8XldUzdXhSt+2mDh9IzVFz5KR5Lc3O\n11kcnRJHKG7V4SZyfd2fY9yQAezZf5RPDx3r1FgLyc1Iah9VCjAxI7HTcmZyzPFrhELpwQYmZCR2\ne44xqfF8dnyEJ8AEXyJ7DzbS0hZkgNeNx9X5+OVmJBLtNn91GeV2ktvhOUsPNjCxh31MzEik+NO6\n9uUrxqS0fx0X7W5/vo7be9w9/3o0xuPiwpHdp7LPSkvg00PH2pcvHzsYt6vn53A7HYxOieu0LinW\ng/cU062GRnYC1Db6yRzYfYreCRmJlNWeOP4XDB2A02LXsezpc3bRyEHERkVuqtlot5NcX+fPjcfl\nsOw1VEVE5OyoASoiIiIiIhJuVhgBChATA/PmmSNAf/nLSKcREbEFn89HeXl5+3JFRQXpXf6opTfb\nANx5550UFRVRVFRESkpKt++fisvlomBSOl88PrVscXkdqQO83HTJ8PbrR14+ZjBzpvj4/svbANhZ\neZin5k5ub4IOSYjmibmTKdpnjsZ7at3HLLxqbHvzMNrt5P6vjudYSxsHG/wcbGjB5YTvXjmqfR+X\njBrEDdN8vHN8TV0stQAAGl9JREFUNOrolHh++rUsVn9QBUDKgGienjeFVcXmCNg3dx3gzitHMTnT\nbIB5XA7unTmOw8daOdLchsMBs/PSyUlPZM9nRwHY8Ektj31jUntzbOTgOB6+LpfEWLMxlRQbxcPX\n5TJysNlMXL/7ADddOoL84w1it9PB978wmii3k7Ljzcmv5aZx2ZgTo0ETY908MTeP+Gg3AMOSY/n3\nOZMYGNtz88vtcnLjhcO4eJTZkHQ5HXz3C6Nwu+Dg8RGDX8kewlUXpJ70PRw8IJqnb5zCkITo46/D\nw39+awoD4zwnfcy4IQOYNy0TpwNa2oLsrWlg0TXZ7c3b7LQE7p05jsLjo1vTEr08OW8ygyN8bc2u\nMpJiuOsLJz5HF49K5sYLh520WXwuJMZG8fC1E9s/R7FRLh77Rh4JMSd/P0RExL4cRk9/stZH8vPz\nKSoqOle7ExHpl/RvaWc6HiIi4aF/Tzs7q+PR3Gw2Hv/t3+AnP+n+/SVLzi5cyJ139ry+p+d/9lnY\nvh2KiiA3Nzz7FxHpBTvWl7a2NsaNG8e6devIyMhg2rRp/OlPfyInJ6d9m9WrV/P000+zZs0aNm3a\nxD333MPmzZtP+byf91hUH26iLWDgcIDH5QQMWgMGQcNs/MW5nTS0BmkNBIlyO4mPcnPU30pLaxCv\nx0VSjIf6plaaWwNEu10keJ0cbQniDwTxOJ1EuRy0GUb7PtxOB06HA38gSDBo4HY5iYty0eAP4G8L\nmqNMY9zUHWujpS1AtMfFQK+LuqYA/rYAUR4XA7wujrWY27tdTmKiXLQFgjS3BnE4wOt24XQ6aPIH\nCASDRHlcDDye099m5k4Z0L2hV3O0hebWAFFuJ0kxHuqaWvG3BnA5zX24nQ4aW9owMEd8JnZpbLW0\nBag/1kpLW5AYj4vB8VE9jtzt6FCjnyZ/AJfTQXy0i6ABjf42DMNsoCWdpIEaEgga1Daaub1uFwPj\nPHhcpx4FWdvQQlNrAMMwm9QDvB4ON7XSFjRzD/C6qT/WSnNbgBiPi0Fx0ZYbAQrQ0NxKQ0sbgSDE\nRLksM9Ky4+coMebUI3KtaNeuXWRlZUU0w6233sqrr75KamoqO3bs6NN99fR67VhbROTcc0c6gIiI\niIiISL/y2WfmvRVGgIbMmweffgrz58Pbb0Ny9yn9RETE5Ha7efrpp/nKV75CIBDg1ltvJScnh2ee\neQaA733ve8yaNYs1a9YwZswYYmNj+eMf/9hnedISu0+B2lXXyWATukyDGxvd+VeAA07/lN33Edt5\nOTa6yz68XTJ0We5R59lhGXKaRlTXpujQHrY/1Wi+aLeLIQln1uxKjovqlvNMRgy6nA5SB/TmYJww\nqIfRnDFdpo5NPcPXEQnxXg/xXuuNruypud6vffAKrPs5HK6ARB9c9SDk3nBWT3nLLbdw9913c9NN\nN4UppIhI+KkBKiIiIiIiEk7V5pR0lrgGaEh8PLz4IlxzDUyfDm+8ASkpJx+NerLRpSIi54lZs2Yx\na9asTuu+973vtX/tcDhYvHjxuY4lInJmPngF/nEPtDaZy4fLzWU4qybolVdeSVlZ2dnnExHpQ7oG\nqIiIiIiISDhVmddEs9QIUICZM+Ef/4CPPoIvfAH+8AcoKTGn7BURERGR/mfdz080P0Nam8z1IiL9\nnEaAioiIiIiIhFNoBKjVGqBgNkELC+HrX4fbbjPXJSXBwoXWGrEqIiIiImfvcMWZrRcR6UfUABUR\nEREREQmn6mpwucwpZvvSyaavPZ0rr4QDB2DfPnjsMXj5Zfj1r80m6PDh4c0oIiIiIpGT6DOnve1p\nvYhIP6cpcEVERERERMKpuhqGDAGnhU+33G4YPRry8uB//2/weuE3v4FduyKdTERERETC5aoHwRPT\neZ0nxlwvItLPWfiMXERERERExIaqq+01nWxqqtkETU6GJ5+E//mfSCcSERERkXDIvQGueRISMwGH\neX/Nk+b6szBv3jwuueQS9uzZg8/n47nnngtPXhGRMNIUuCIiIiIiIuFUXQ3DhkU6xZkZOBDuv9+c\nVvfFF81m6COPQEzM6R8rIiIiItaVe8NZNzy7WrZsWVifT0SkL6gBKiIiIiIiEk5VVXDRRZFO0d3p\nrhkaEwN33w1//Ss88QSsXQt/+ANcdtm5ySciIiIiIiISJpoCV0REREREJFxaW6GmBtLSIp3k83G5\n4JvfhHXrwO+HK6+E3/4WDCPSyURERERERER6TQ1QERERERGRcNm/37y3awM0ZPp0+OADuPZa+NGP\n4LbboLk50qlEREREBDDOkz9OO19ep4j0DU2BKyIiIiIiEi7V1eZ9enpkc4TDgAHwyivws5/Bz38O\nK1fChAmQmwvx8RAdbU6b6/XCD34AzjP4+9pAAJYvN6fY/fKX4d57ISqq716LiIiISD/h9Xqpra1l\n0KBBOByOSMfpM4ZhUFtbi9frjXQUEbEpNUBFRERERETCJdQAtfsI0I7XC83IMEeB/vOfsGEDvPNO\n9+1/+lPIzzevfXrLLTB+fM/PaxiwYgXcdZc5WjYpCdavh6efhvnz4Ze/7F2+lhaorIThw81pe8PN\nMMzpjNWUFREREYvx+XxUVFRQU1MT6Sh9zuv14vP5Ih1DRGxKDVAREREREZFwqaoy7+3eAO1q/Hjz\n1twMFRXQ1GR+3dxsfj1kCGzeDL/+NTz6KFxzjdkIvewy83stLbBrF/z4x1BYaI6Q/e53IS8PduyA\nZcvMxw4eDPfdBycbzbBzJzz7LLz0EtTWmg3KjAyYMsXcV1wc3Hmnue2hQ/D++xAMwqRJ5ntysuf1\n+2H1anNE6qZNUFcHbW3g80FOjjnqddQoc5Rr6Pk7euYZ+Ogj83V6vZCQAKmpZqNXREREJIw8Hg8j\nR46MdAwREctTA1RERERERCyvsLCQhQsXEggEuP3223nggQc6fd8wDBYuXMiaNWuIjY3l+eefZ8qU\nKb16bFglJ8Pll5tNv/7I64UxY7qvDzUF9++H//gP8/aPf5jrBg0ym5GGYU6r+/jjZuMyNHIzNxfG\njYMXXoD774d334Wf/ASmTj3RsNy+3ZyG9+9/B48Hvv518/6zz2DvXvjb32DVKrNJ+dxzUFMDpaWd\nMw4dCgUFMGeOuU+vFz79FJ5/Hl580XxMWhrMng0pKeY1UEtK4I034LXXzNGqU6ZAZiZccYXZDN25\n0/zeb35jNk07crvh9783m68dbwMHhu3tEBERERERkZ6pASoiIiIiIpYWCARYsGABb7zxBj6fj2nT\nplFQUEB2dnb7NmvXrqWkpISSkhI2bdrEXXfdxaZNm3r12LC64Qbzdr7pOmXuokVmc3HvXrNJmZRk\nNkJzcszrhnbl9cIdd5jT4N5/v3m90YwMs2m5d6/ZXExIgAcfNK83Onhw532Wl8Pbb5vT4jY0mN/P\ny4ORI80makWFud3LL3d+HJiN2EmT4JvfhOzsE43ZESPM+6Ym+Ne/oKjInP53/XqzuRkMmjeArCzz\nfU9ONkeB1tWZ+zQMePVV+OMfT+xv2DBztOoVV5iN2MxM83W63WbWxkZzdGtpKWzZAlu3wscfm8tO\np/mYjg3VjAzzeqzR0eZznIxhmLfDh83jVVYGxcXm81dVmcctEDCb0RMmmO/VhAlmUzk62mw4n8l1\nXnvD7zfz7NplHuMdO8z7ffvM93vQILPhPnmy+d4kJUFsrJl9584Tt08+MY99ZqaZPz/ffP+HDjWb\n2dHRZ5Yr9N42N5vHat8+M9vWrWZTvKXFHCGckWHmyskx7y+4wGzy98WxOluBgPmzWF4Ou3ebr2f3\nbvPntLLSHJV96aXmsU5PN4/d0KHmH3Oc6fETEREREbEANUBFRERERMTSNm/ezJgxYxg1ahQAc+fO\nZeXKlZ2amCtXruSmm27C4XBw8cUXU19fT3V1NWVlZad9rPQBjwdGjzZvveVwmM2tRx81m2AffGA2\nH3NzzUbMRReZU9z+/e/dH5uZCd/+9smfe9w48/7yy81mW339iWt8Tp5sNq1OJiYGLrzQvPn95lTA\nb79tvsZJk8yRqoWFJ3/8nDlw5IjZeKqsNJtp//3f5rS/vZGZCfHxZnMtEIA9e+Ctt8wGXFdOp9ms\ncjjMBl4gcKKZZxjdt3c4zAZXcvKJxvTWrbB2rfnYrlwu83X3dN3Vnp7/VOtbW7u/hthYs/mWmWk2\nH6urYdu2zg3kjhISzO0nTDAbx/v2wcaN8LvfdX+dTmfP93DiWIWO18kMG2a+F1FR5nEoKTGvjdvS\n0n1bp9PcpmtTuuvx6On4nG6bz/McgUDndR6P+d4PGmQ270tL4c9/7v4HAmD+jGzd2n29iIiIiIiF\nndMG6L59+8jPz/9cjz148CCDBw8Oc6LwU87wsUNGUM5wskNGiHzOffv2RWzfVnQ2tSUcIv15sAsd\np97Tseo9Have682xsnJ9qaysJDMzs33Z5/OxadOm025TWVnZq8eGLFmyhCXHf/m/Z8+ePqsvVv/s\nWiafw2E25RoazBGQH3/cN9l27/78j3311U6Lvc6Xlnbm14k9dsy8T0kxb59Dr/J9nmxh0Cmb3282\nEBMTzdvpNDaa9wMH9tkUwz0eu8GDzVuEnfXPxfvvm/c9TW8dchb/Hlq5vpxrZ3vuYpl/n0/DDjnt\nkBGUM9zskNMOGSHyOVVbRKQ3zmkDtKam5nM/Nj8/n6KiojCm6RvKGT52yAjKGU52yAj2yXm+OJva\nEg76PPSOjlPv6Vj1no5V79n9WBk9jG5yhEZunWab3jw25M477+TO0LUs+5DV3w8r57NyNlC+s2Hl\nbGDtfFbOJp2d7bmLXd5rO+S0Q0ZQznCzQ047ZAT75BSR85umwBUREREREUvz+XyUl5e3L1dUVJCe\nnt6rbfx+/2kfKyIiIiIiIiL9izPSAURERERERE5l2rRplJSUUFpait/vZ/ny5RQUFHTapqCggBde\neAHDMNi4cSOJiYmkpaX16rEiIiIiIiIi0r+4Fi1atCjSIXpr6tSpkY7QK8oZPnbICMoZTnbICPbJ\nKeeGPg+9o+PUezpWvadj1Xt2PlZOp5OxY8cyf/58nnrqKebPn8/111/PM888Q1FREfn5+YwdO5YN\nGzZwzz33UFhYyLPPPkt6evpJHxtpVn8/rJzPytlA+c6GlbOBtfNZOZuEl13eazvktENGUM5ws0NO\nO2QE++QUkfOXw+jpojgiIiIiIiIiIiIiIiIiIjakKXBFREREREREREREREREpN9QA1RERERERERE\nRERERERE+g1bNEALCwsZP348Y8aM4dFHH410nHa33norqampTJgwoX3doUOHmDFjBmPHjmXGjBnU\n1dVFMCGUl5fzpS99iaysLHJycnjiiScsmbO5uZkLL7yQSZMmkZOTw0MPPWTJnACBQIDJkyfzta99\nDbBmxhEjRjBx4kTy8vLIz88HrJmzvr6eOXPmcMEFF5CVlcWGDRssl3PPnj3k5eW13xISEnj88cct\nl1POPavWJiuwS+2xCjvUFauwQ92wgt/+9rfk5OQwYcIE5s2bR3Nzs46ThVitflj5nMbK9cQu5y9W\nrjFWPmexcr3R+cn5yWq1I8TKNaQjK9eTELvUlRAr15cQK9eZjqxcc0B1R0TszfIN0EAgwIIFC1i7\ndi0ffvghy5Yt48MPP4x0LABuueUWCgsLO6179NFHueqqqygpKeGqq66K+H9M3W43jz32GLt27WLj\nxo0sXryYDz/80HI5o6OjWb9+Pdu3b6e4uJjCwkI2btxouZwATzzxBFlZWe3LVswI8NZbb1FcXExR\nURFgzZwLFy7kq1/9Krt372b79u1kZWVZLuf48eMpLi6muLiYLVu2EBsby7XXXmu5nHJuWbk2WYFd\nao9V2KWuWIEd6kakVVZW8uSTT1JUVMSOHTsIBAIsX75cx8kirFg/rHxOY+V6YpfzF6vXGKues1i5\n3uj85PxjxdoRYuUa0pGV60mIXepKiNXrS4hV60xHVq45oLojIjZnWNx7771nzJw5s3354YcfNh5+\n+OEIJuqstLTUyMnJaV8eN26cUVVVZRiGYVRVVRnjxo2LVLQeFRQUGK+//rqlczY2NhqTJ082Nm7c\naLmc5eXlxvTp041169YZV199tWEY1nzPhw8fbtTU1HRaZ7Wchw8fNkaMGGEEg8FO662Ws6PXXnvN\nuPTSSw3DsHZO6XtWr01WY4faEyl2qStWYMe6EQkVFRWGz+czamtrjdbWVuPqq682XnvtNR0ni7Bq\n/bDLOY1V64lVz1+sXmOses5ip3qj85Pzg1VrR4hdakhHVq0nIVatKyFWry8hVq0zHdmp5hiG6o6I\n2I/lR4BWVlaSmZnZvuzz+aisrIxgolPbv38/aWlpAKSlpXHgwIEIJzqhrKyMbdu2cdFFF1kyZyAQ\nIC8vj9TUVGbMmGHJnD/84Q/51a9+hdN54kfHahkBHA4HM2fOZOrUqSxZsgSwXs69e/eSkpLCd77z\nHSZPnsztt99OY2Oj5XJ2tHz5cubNmwdY73jKuWW32hRJVq89kWaXumIFdqwbkZCRkcF9993HsGHD\nSEtLIzExkZkzZ+o4WYRd6ocVPy9WrCdWP3+xeo2x6jmLneqNzk/OD3apHSFW/yxasZ6EWL2uhFi9\nvoRYtc50ZKeaA6o7ImI/lm+AGobRbZ3D4YhAEntraGjg+uuv5/HHHychISHScXrkcrkoLi6moqKC\nzZs3s2PHjkhH6uTVV18lNTWVqVOnRjrKab377rts3bqVtWvXsnjxYt55551IR+qmra2NrVu3ctdd\nd7Ft2zbi4uIsPV2G3+9n1apVfOMb34h0FLEA1abesUPtiSQ71RUrsFvdiJS6ujpWrlxJaWkpVVVV\nNDY28tJLL0U6lhyn+vH5WLWeWPn8xQ41xqrnLHapNzo/OX+odoSPVetJiJXrSogd6kuIVetMR3ap\nOaC6IyL2ZPkGqM/no7y8vH25oqKC9PT0CCY6tSFDhlBdXQ1AdXU1qampEU4Era2tXH/99XzrW9/i\nuuuuA6yZMyQpKYkvfvGLFBYWWirnu+++y6pVqxgxYgRz585l/fr1zJ8/31IZQ0I/I6mpqVx77bVs\n3rzZcjl9Ph8+n4+LLroIgDlz5rB161bL5QxZu3YtU6ZMYciQIYC1f4ak79mtNkWC3WpPJNiprliB\n3epGpLz55puMHDmSlJQUPB4P1113He+9956Ok0XYpX5Y6fNih3pixfMXO9QYq56z2KXe6Pzk/GGX\n2hFi1c+iHepJiBXrSogd6kuIVetMR3apOaC6IyL2ZPkG6LRp0ygpKaG0tBS/38/y5cspKCiIdKyT\nKigoYOnSpQAsXbqU2bNnRzSPYRjcdtttZGVl8aMf/ah9vdVy1tTUUF9fD0BTUxNvvvkmF1xwgaVy\nPvLII1RUVFBWVsby5cuZPn06L730kqUyAjQ2NnL06NH2r19//XUmTJhguZxDhw4lMzOTPXv2ALBu\n3Tqys7MtlzNk2bJl7dN8gPV+huTcslttOtfsUnsizS51xSrsVjciZdiwYWzcuJFjx45hGAbr1q0j\nKytLx8ki7FI/rPJ5sXI9sfr5i9VrjJXPWexSb3R+cv6wS+0IseJn0cr1JMTqdSXE6vUlxMp1piO7\n1BxQ3RERm4rY1UfPwOrVq42xY8cao0aNMn7xi19EOk67uXPnGkOHDjXcbreRkZFh/P73vzcOHjxo\nTJ8+3RgzZowxffp0o7a2NqIZ//nPfxqAMXHiRGPSpEnGpEmTjNWrV1su5/bt2428vDxj4sSJRk5O\njvGzn/3MMAzDcjlD3nrrrfYLvVst4yeffGLk5uYaubm5RnZ2dvvPjNVyGoZhbNu2zZg6daoxceJE\nY/bs2cahQ4csmbOxsdFITk426uvr29dZMaecW1atTVZgl9pjJVauK1Zil7oRaQ8++KAxfvx4Iycn\nx5g/f77R3Nys42QhVqsfVj6nsXI9sdP5ixVrjNXPWaxeb3R+cv6xWu0IsXIN6cjK9STETnUlxIr1\nJcTqdaYjq9ccw1DdERH7chhGDxcTEBERERERERERERERERGxIctPgSsiIiIiIiIiIiIiIiIi0ltq\ngIqIiIiIiIiIiIiIiIhIv6EGqIiIiIiIiIiIiIiIiIj0G2qAioiIiIiIiIiIiIiIiEi/oQaoiIiI\niIiIiIiIiIiIiPQbaoCKAIcOHWLGjBmMHTuWGTNmUFdXB0BtbS1f+tKXiI+P5+67745wShERsRPV\nFhER6QuqLyIi0hdUX0REpL9xLVq0aFGkQ4hE2kMPPUROTg6vvPIKVVVVvPnmm8yYMQPDMMjKyiIv\nL48DBw4wa9asSEcVERGbUG0REZG+oPoiIiJ9QfVFRET6G40AlfNKWVkZWVlZ3HHHHeTk5DBz5kya\nmppYuXIlN998MwA333wzK1asACAuLo7LL78cr9cbydgiImJhqi0iItIXVF9ERKQvqL6IiMj5Qg1Q\nOe+UlJSwYMECdu7cSVJSEn/729/Yv38/aWlpAKSlpXHgwIEIpxQRETtRbRERkb6g+iIiIn1B9UVE\nRM4HaoDKeWfkyJHk5eUBMHXqVMrKyiIbSEREbE+1RURE+oLqi4iI9AXVFxEROR+oASrnnejo6Pav\nXS4XbW1tDBkyhOrqagCqq6tJTU2NVDwREbEh1RYREekLqi8iItIXVF9EROR8oAaoCFBQUMDSpUsB\nWLp0KbNnz45wIhERsTvVFhER6QuqLyIi0hdUX0REpL9xRzqAiBU88MAD3HDDDTz33HMMGzaMv/zl\nL+3fGzFiBEeOHMHv97NixQpef/11srOzI5hWRETsQLVFRET6guqLiIj0BdUXERHpbxyGYRiRDiEi\nIiIiIiIiIiIiIiIiEg6aAldERERERERERERERERE+g01QEVERERERERERERERESk31ADVERERERE\nRERERERERET6DTVARURERERERERERERERKTfUANURERERERERERERERERPoNNUBFRERERERERERE\nREREpN9QA1RERERERERERERERERE+g01QEVERERERERERERERESk3/j/mXrXUP9AAAUAAAAASUVO\nRK5CYII=\n"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![num_plots.png](attachment:num_plots.png)\n",
    "**Figure 4 - Example of plots for Numerical Variable Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Numerical Variable | Distribution Type          | Scaling                  |\n",
    "|--------------------|----------------------------|--------------------------|\n",
    "| i01                | Exponentially Decreasing   | i01' = i01/(2*SD)        |\n",
    "| i02                | Truncated Skewed Normal    | i02' = (i02 - median)/SD |\n",
    "| i03                | Exponentially Decreasing   | i03' = i03/SD            |\n",
    "| i04                | Truncated Skewed Normal    | i04' = (i04-median)/SD   |\n",
    "| i05                | Truncated Skewed Normal    | i05' = (i05-median)/SD   |\n",
    "| i06                | Exponentially Decreasing   | i06' = i06/2*SD          |\n",
    "| i07                | Exponentially Decreasing   | i07' = i07/2*SD          |\n",
    "| i08                | Exponentially Decreasing   | i08' = i08/2*SD          |\n",
    "| i09                | Truncated Skewed Normal    | i09' = (i09-median)/SD   |\n",
    "| i10                | Sigmoid                    | i10' = i10/Max(i10)      |\n",
    "| i11                | Truncated Skewed Normal    | i11' = (i11-median)/SD   |\n",
    "| i12                | Exponentially Decreasing   | i12' = i12/2*SD          |\n",
    "| i13                | Truncated Skewed Normal    | i13' = (i13-median)/SD   |  \n",
    "**Table 2 - Scaling Applied to Numeric Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Categor Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 26 categorical features, which we labelled from c01 to c26 based on their oder in the dataset and they had all been hashed. We analyzed these features and found that they had very high cardinality. The number of unique categories for each feature is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distinct</th>\n",
       "      <th>frequent</th>\n",
       "      <th>uncommon</th>\n",
       "      <th>top five</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c01</th>\n",
       "      <td>1460</td>\n",
       "      <td>23</td>\n",
       "      <td>1437</td>\n",
       "      <td>05db9164, 68fd1e64, 5a9ed9b0, 8cf07265, be589b51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c02</th>\n",
       "      <td>581</td>\n",
       "      <td>100</td>\n",
       "      <td>481</td>\n",
       "      <td>38a947a1, 207b2d81, 38d50e09, 1cfdf714, 287130e0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c03</th>\n",
       "      <td>8381767</td>\n",
       "      <td>45</td>\n",
       "      <td>8381722</td>\n",
       "      <td>deadbeef, d032c263, 02cf9876, aa8c1539, 9143c832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c04</th>\n",
       "      <td>1883842</td>\n",
       "      <td>60</td>\n",
       "      <td>1883782</td>\n",
       "      <td>c18be181, deadbeef, 29998ed1, d16679b9, 85dd697c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c05</th>\n",
       "      <td>305</td>\n",
       "      <td>13</td>\n",
       "      <td>292</td>\n",
       "      <td>25c83c98, 4cf72387, 43b19349, 384874ce, 30903e74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c06</th>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>7e0ccccf, fbad5c96, fe6b92e5, deadbeef, 13718bbd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c07</th>\n",
       "      <td>12488</td>\n",
       "      <td>62</td>\n",
       "      <td>12426</td>\n",
       "      <td>1c86e0eb, dc7659bd, 7195046d, 5e64ce5f, 468a0854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c08</th>\n",
       "      <td>633</td>\n",
       "      <td>17</td>\n",
       "      <td>616</td>\n",
       "      <td>0b153874, 5b392875, 1f89b562, 37e4aa92, 062b5529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c09</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a73ee510, 7cc72ec2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c10</th>\n",
       "      <td>88956</td>\n",
       "      <td>42</td>\n",
       "      <td>88914</td>\n",
       "      <td>3b08e48b, efea433b, fbbf2c95, fa7d0797, 03e48276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c11</th>\n",
       "      <td>5656</td>\n",
       "      <td>93</td>\n",
       "      <td>5563</td>\n",
       "      <td>755e4a50, e51ddf94, 7f8ffe57, 4d8549da, 8b94178b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c12</th>\n",
       "      <td>6952733</td>\n",
       "      <td>52</td>\n",
       "      <td>6952681</td>\n",
       "      <td>deadbeef, dfbb09fb, 6aaba33c, 8fe001f4, d8c29807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c13</th>\n",
       "      <td>3194</td>\n",
       "      <td>105</td>\n",
       "      <td>3089</td>\n",
       "      <td>5978055e, 3516f6e6, 46f42a63, 025225f2, 1aa94af3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c14</th>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>b28479f6, 07d13a8f, 1adce6ef, 64c94865, cfef1c29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c15</th>\n",
       "      <td>14754</td>\n",
       "      <td>112</td>\n",
       "      <td>14642</td>\n",
       "      <td>2d0bb053, d345b1a0, 3628a186, 10040656, 10935a85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c16</th>\n",
       "      <td>4590520</td>\n",
       "      <td>58</td>\n",
       "      <td>4590462</td>\n",
       "      <td>deadbeef, 84898b2a, b041b04a, 36103458, c64d548f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c17</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>e5ba7672, 07c540c4, d4bb7bd8, 3486227d, 776ce399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c18</th>\n",
       "      <td>5582</td>\n",
       "      <td>126</td>\n",
       "      <td>5456</td>\n",
       "      <td>e88ffc9d, 891589e7, 2804effd, c21c3e4c, 5aed7436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c19</th>\n",
       "      <td>2169</td>\n",
       "      <td>22</td>\n",
       "      <td>2147</td>\n",
       "      <td>deadbeef, 21ddcdc9, 55dd3565, 5b885066, 9437f62f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c20</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>deadbeef, b1252a9d, 5840adea, a458ea53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c21</th>\n",
       "      <td>5894178</td>\n",
       "      <td>52</td>\n",
       "      <td>5894126</td>\n",
       "      <td>deadbeef, 0014c32a, 723b4dfd, e587c466, 5f957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c22</th>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>deadbeef, ad3062eb, c9d4222a, 78e2e389, 8ec974f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c23</th>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>32c7478e, 3a171ecb, 423fab69, bcdee96c, be7c41b4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c24</th>\n",
       "      <td>258639</td>\n",
       "      <td>60</td>\n",
       "      <td>258579</td>\n",
       "      <td>3fdb382b, b34f3128, 3b183c5c, 1793a828, deadbeef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c25</th>\n",
       "      <td>105</td>\n",
       "      <td>19</td>\n",
       "      <td>86</td>\n",
       "      <td>deadbeef, 001f3601, e8b83407, ea9a246c, cb079c2d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c26</th>\n",
       "      <td>133254</td>\n",
       "      <td>37</td>\n",
       "      <td>133217</td>\n",
       "      <td>deadbeef, 49d68486, c84c4aec, 2fede552, c27f155b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     distinct  frequent  uncommon  \\\n",
       "c01      1460        23      1437   \n",
       "c02       581       100       481   \n",
       "c03   8381767        45   8381722   \n",
       "c04   1883842        60   1883782   \n",
       "c05       305        13       292   \n",
       "c06        24         7        17   \n",
       "c07     12488        62     12426   \n",
       "c08       633        17       616   \n",
       "c09         3         2         1   \n",
       "c10     88956        42     88914   \n",
       "c11      5656        93      5563   \n",
       "c12   6952733        52   6952681   \n",
       "c13      3194       105      3089   \n",
       "c14        27        13        14   \n",
       "c15     14754       112     14642   \n",
       "c16   4590520        58   4590462   \n",
       "c17        10         9         1   \n",
       "c18      5582       126      5456   \n",
       "c19      2169        22      2147   \n",
       "c20         4         4         0   \n",
       "c21   5894178        52   5894126   \n",
       "c22        18         6        12   \n",
       "c23        15        11         4   \n",
       "c24    258639        60    258579   \n",
       "c25       105        19        86   \n",
       "c26    133254        37    133217   \n",
       "\n",
       "                                             top five  \n",
       "c01  05db9164, 68fd1e64, 5a9ed9b0, 8cf07265, be589b51  \n",
       "c02  38a947a1, 207b2d81, 38d50e09, 1cfdf714, 287130e0  \n",
       "c03  deadbeef, d032c263, 02cf9876, aa8c1539, 9143c832  \n",
       "c04  c18be181, deadbeef, 29998ed1, d16679b9, 85dd697c  \n",
       "c05  25c83c98, 4cf72387, 43b19349, 384874ce, 30903e74  \n",
       "c06  7e0ccccf, fbad5c96, fe6b92e5, deadbeef, 13718bbd  \n",
       "c07  1c86e0eb, dc7659bd, 7195046d, 5e64ce5f, 468a0854  \n",
       "c08  0b153874, 5b392875, 1f89b562, 37e4aa92, 062b5529  \n",
       "c09                                a73ee510, 7cc72ec2  \n",
       "c10  3b08e48b, efea433b, fbbf2c95, fa7d0797, 03e48276  \n",
       "c11  755e4a50, e51ddf94, 7f8ffe57, 4d8549da, 8b94178b  \n",
       "c12  deadbeef, dfbb09fb, 6aaba33c, 8fe001f4, d8c29807  \n",
       "c13  5978055e, 3516f6e6, 46f42a63, 025225f2, 1aa94af3  \n",
       "c14  b28479f6, 07d13a8f, 1adce6ef, 64c94865, cfef1c29  \n",
       "c15  2d0bb053, d345b1a0, 3628a186, 10040656, 10935a85  \n",
       "c16  deadbeef, 84898b2a, b041b04a, 36103458, c64d548f  \n",
       "c17  e5ba7672, 07c540c4, d4bb7bd8, 3486227d, 776ce399  \n",
       "c18  e88ffc9d, 891589e7, 2804effd, c21c3e4c, 5aed7436  \n",
       "c19  deadbeef, 21ddcdc9, 55dd3565, 5b885066, 9437f62f  \n",
       "c20            deadbeef, b1252a9d, 5840adea, a458ea53  \n",
       "c21  deadbeef, 0014c32a, 723b4dfd, e587c466, 5f957280  \n",
       "c22  deadbeef, ad3062eb, c9d4222a, 78e2e389, 8ec974f4  \n",
       "c23  32c7478e, 3a171ecb, 423fab69, bcdee96c, be7c41b4  \n",
       "c24  3fdb382b, b34f3128, 3b183c5c, 1793a828, deadbeef  \n",
       "c25  deadbeef, 001f3601, e8b83407, ea9a246c, cb079c2d  \n",
       "c26  deadbeef, 49d68486, c84c4aec, 2fede552, c27f155b  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_measures = load(open('data/model.pickled.normed.filled.masked-060000.cat_measures', 'rb'))\n",
    "\n",
    "df             = pd.DataFrame.from_dict({ k: [len(v)] for k,v in cat_measures['distinct'].items() }, orient = 'index', columns = ['distinct'])\n",
    "df['frequent'] = [len(v) for v in cat_measures['frequent'].values()]\n",
    "df['uncommon'] = [len(v) for v in cat_measures['uncommon'].values()]\n",
    "df['top five'] = [', '.join(v[:5]) for v in cat_measures['frequent'].values()]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the number of distict categories ranges from 3 to 5894178. Working with this number of cardinal features presents challenges so we examined ways to reduce the cardinality of these features, which included:\n",
    "1. For each feature, consider the most commonly occurring features in the training set only and encode any other rare categories as one category\n",
    "2. Use Field-aware Factorization Machines which handle high cardinality data in recommendation and click thoguh rate datases by default\n",
    "3. Hash the categorical features to a smaller feature space with possible collisions\n",
    "4. Drop columns with extreme cardinality since these may correspond to features like user IDs or street address that ordinarily may not provide much value to the dataset.\n",
    "\n",
    "In the end, we chose option 1 because this allowed us to create a dataset that could be used with multiple algorithms while allowing us to shrink the feature space. We also found that for most features. Mnay of the categories appeared in under 1% of the training set (shown below) and so encoding the rare categories into a special category would greatly reduce the potential feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue we had to deal with categorical features, just like with the numeric features was how to deal with null values. Many of the categorical variables had a high number of null values with some columsn having over 40% null values. Below is a breakdown of the percentage of null values for each column. We would need an effective way to deal with these null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 General Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EDA guided how we went about out feature engineering. The items that we needed to tackle in our feature engineering were:\n",
    "1. Scale the numeric features so that they are within the same range using an appropriate scaler based on the feature\n",
    "2. Deal with null values. For the numeric features, null values were replaced with the median while with the categorical features, null values were replaced witha special category, \"deadbeef\".\n",
    "3. Reduce the number of categories in each column by encoding rare categories as rarebeef and only keeping those that appear more than 1% of the time\n",
    "4. One-hot encode the categorical features so that they can be used to train the model\n",
    "5. Reduce the number of features in the dataset by using a feature selector based on the chi-squared test\n",
    "6. Oversmaple the data to deal with the issue of unbalanced data which would affect the Logistic Regression and tree-based models\n",
    "7. Create interaction variables between the features in the dataset\n",
    "\n",
    "The initial feature engineering is available in the [Engineering-Initial](./book/Engineering-Initial.ipynb) workbook and [engineering.py](./code/engineering.py) python module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Features\n",
    "The numeric features had different ranges and so it was imprtant to scale them especially when working with linear models like Logistic Regression. As shown in the EDA above, the numeric features had different distributions so they were scaled differently based on this. The scaling used for each numeric feature is shown in section 3.2.1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allDoStandardize(subset: str, iStep: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Apply standardizations to all numerical features to ensure numerical features have balanced weights.\n",
    "    Replace all undefined categorical values with deadbeef.\n",
    "\n",
    "    Input  : Spark Sql Dataframe of original labled data and relevant statistics from training data\n",
    "    Output : Spark Sql Dataframe of all labeled data with standardized numeric features and filled categoric features\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "      # replace all undefined numerical values with the median for that feature\n",
    "        oData = iData.fillna(num_measures['median'])\n",
    "\n",
    "      # add standardized numerical feature columns  \n",
    "        oData = oData.withColumn('n01_standard',((oData['n01']                              )/(2.0*num_measures['stddev']['n01'])))\n",
    "        oData = oData.withColumn('n02_standard',((oData['n02']-num_measures['median']['n02'])/(1.0*num_measures['stddev']['n02'])))\n",
    "        oData = oData.withColumn('n03_standard',((oData['n03']                              )/(1.0*num_measures['stddev']['n03'])))\n",
    "        oData = oData.withColumn('n04_standard',((oData['n04']-num_measures['median']['n04'])/(1.0*num_measures['stddev']['n04'])))\n",
    "        oData = oData.withColumn('n05_standard',((oData['n05']-num_measures['median']['n05'])/(1.0*num_measures['stddev']['n05'])))\n",
    "        oData = oData.withColumn('n06_standard',((oData['n06']                              )/(2.0*num_measures['stddev']['n06'])))\n",
    "        oData = oData.withColumn('n07_standard',((oData['n07']                              )/(2.0*num_measures['stddev']['n07'])))\n",
    "        oData = oData.withColumn('n08_standard',((oData['n08']                              )/(2.0*num_measures['stddev']['n08'])))\n",
    "        oData = oData.withColumn('n09_standard',((oData['n09']-num_measures['median']['n09'])/(1.0*num_measures['stddev']['n09'])))\n",
    "        oData = oData.withColumn('n10_standard',((oData['n10']                              )/(1.0*num_measures['max'   ]['n10'])))\n",
    "        oData = oData.withColumn('n11_standard',((oData['n11']-num_measures['median']['n11'])/(1.0*num_measures['stddev']['n11'])))\n",
    "        oData = oData.withColumn('n12_standard',((oData['n12']                              )/(2.0*num_measures['stddev']['n12'])))\n",
    "        oData = oData.withColumn('n13_standard',((oData['n13']-num_measures['median']['n13'])/(1.0*num_measures['stddev']['n13'])))\n",
    "\n",
    "        assembler_num = VectorAssembler(inputCols = Common.num_features, outputCol = 'num_features')\n",
    "        assembler_std = VectorAssembler(inputCols = Common.std_features, outputCol = 'std_features')\n",
    "\n",
    "        pipeline      = Pipeline(stages = [assembler_num, assembler_std])\n",
    "        model         = pipeline.fit(oData)\n",
    "\n",
    "        oData         = model.transform(oData)\n",
    "        oData         = oData.select('label', 'std_features', *Common.cat_features)\n",
    "\n",
    "      # replace all undefined categorical values with special term deadbeef\n",
    "        oData         = oData.fillna('deadbeef', Common.cat_features)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the Null Values\n",
    "The null values were dealt with differently for numeric and categorical features. For the numeric features, the null values were imputed using the median in order because most of them were highly skewed. For the cateogircal values, the null values were all encoded to a category labelled **deadbeef**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the cardinality of categorical features\n",
    "As mentioned before, the cardinaly of the categorical features was handled by looking at each feature and encoding all the rare categories to one special category called **rarebeef** [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catDoMeasurement(subset: str, iStep: str):\n",
    "\n",
    "    \"\"\"module\n",
    "    Calculate frequency stats on training data for use in standardization.\n",
    "\n",
    "    Input  : Raw training dataframe.\n",
    "    Output : Categorical training stats.\n",
    "    \"\"\"    \n",
    "    ...\n",
    "        for feature in Common.cat_features:\n",
    "            cat_measures[feature] = iData.select(feature).groupBy(feature).count() \\\n",
    "                .sort('count', ascending = False) \\\n",
    "                .withColumnRenamed(feature,  'value') \\\n",
    "                .withColumnRenamed('count', 'counts') \\\n",
    "            .toPandas()\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catMaskUncommons(subset: str, iStep: str, min: int):\n",
    "    \"\"\"\n",
    "    Mask uncommon categorical feature categories.\n",
    "    This greatly reduces the number of one-hot encoded features.\n",
    "    The features in test/valid which were not also in train will be masked as well.\n",
    "\n",
    "    Input  : Categorical features with Undefined values replaced with a special term : deadbeef. Categorical feature frequency stats.\n",
    "    Output : Categorical features with Infrequent values replaced with a special term : rarebeef.\n",
    "    \"\"\"\n",
    "    ...\n",
    "        \"\"\"\n",
    "        By using not matching list of frequent categories instead of matching a much longer list of infrequent categories we can\n",
    "        significantly improve the processing time required on the full dataset\n",
    "        \"\"\"\n",
    "\n",
    "        for feature, measure in Common.cat_measures.items():\n",
    "\n",
    "            frequent = measure[measure.counts > min].value.to_list()\n",
    "            oData    = oData.withColumn(feature, when(~oData[feature].isin(*frequent), 'rarebeef').otherwise(oData[feature]))\n",
    "            count    = oData.select(feature).groupBy(feature).count().count()\n",
    "            total   += count\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode the categorical features\n",
    "To be able to use the categorical features in our machine learning algorithms, we had to turn the categorical features into numeric features [1]. To do this, we first used Spark's StringIndexer to create a string index for each of the categories within the features and then one-hot encoded them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catDoCodeFeature(subset: str, iStep: str, min: int, fit: bool = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Index and encode all categorical features.\n",
    "    Generates indicator features for each distinct category withing each respective categorical feature.\n",
    "    Assemble the one-hot encoded indicators to a categorical feature vector.\n",
    "    Encoding estimator will be trained on the train set and any features in the test/train that are not present will be dropped.\n",
    "\n",
    "    Input  : Categorical features with Infrequent values reduced.\n",
    "    Output : Indicator features for each distinct category assembled into a SparseVector.\n",
    "    \"\"\"\n",
    "    ...\n",
    "        \"\"\"\n",
    "        Use a Spark ML pipeline to chain multiple transformation operations.\n",
    "        \"\"\"\n",
    "\n",
    "        stages   = []\n",
    "        indexes  = [f'{f}_idx' for f in Common.cat_features]\n",
    "        vectors  = [f'{f}_vec' for f in Common.cat_features]\n",
    "\n",
    "        for feature, index, vector in zip(Common.cat_features, indexes, vectors):\n",
    "            indexer  = StringIndexer(inputCol = feature, outputCol = index)\n",
    "            encoder  = OneHotEncoderEstimator(inputCols = [indexer.getOutputCol()], outputCols = [vector], dropLast = False)\n",
    "            stages  += [indexer, encoder]\n",
    "\n",
    "        assembler = VectorAssembler(inputCols = vectors, outputCol = 'cat_features')\n",
    "        stages   += [assembler]\n",
    "\n",
    "        encoding_pipe  = Pipeline(stages = stages)\n",
    "        encoding_model = encoding_pipe.fit(iData)\n",
    "\n",
    "        encoding_model.save(oPipe)\n",
    "\n",
    "        timePrint(f'Building Model : Done')\n",
    "\n",
    "    Common.encode_model = PipelineModel.load(oPipe)\n",
    "\n",
    "    if  iData != None and not exists(oFile):\n",
    "\n",
    "        oData  = Common.encode_model.transform(iData)\n",
    "        oData  = oData.select(['label', 'std_features', 'cat_features'])\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "After carrying out the other feature engineering steps, we had a number of features. To determine which ones we should keep and to reduce dimensionality, we used Pyspark's ChiSqSelector to choose the best features. This uses the Chi-Squared Test of Independence to select the best features based on the outcome variable. The variant we used chooses the top features with the highest predictive power in relation to the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catDoPickFeature(subset: str, iStep: str, top: int, fit: bool = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Use Chi-Square test to select the top N encoded categorical features.\n",
    "    Assemble picked features into a new vector.\n",
    "\n",
    "    Input  : The one-hot encoded categorical features in cat_features SparseVector.\n",
    "    Output : The top N categorical features in top_features SparseVector.\n",
    "    \"\"\"\n",
    "    ...\n",
    "        \"\"\"\n",
    "        Perform the Chi-Square test on the training dataset to select the highest N correlated features.\n",
    "        \"\"\"\n",
    "\n",
    "        timePrint(f'Building Model : {oPipe}')\n",
    "\n",
    "        selector       = ChiSqSelector(numTopFeatures = top, featuresCol = 'cat_features', outputCol = 'top_features', labelCol = 'label')\n",
    "        selector_pipe  = Pipeline(stages = [selector])\n",
    "        selector_model = selector_pipe.fit(iData)\n",
    "\n",
    "        selector_model.save(oPipe)\n",
    "\n",
    "        timePrint(f'Building Model : Done')\n",
    "\n",
    "    Common.select_model = PipelineModel.load(oPipe)\n",
    "\n",
    "    if  iData != None and top:\n",
    "\n",
    "        oData  = Common.select_model.transform(iData)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Feature Engineering Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = 60000 # minimum occurance threshold for each categorical feature category,\n",
    "            # generates 1150 one-hot encoded categorical features\n",
    "top =   987 # maximum selection threshold for ChiSquareSelector of one-hot encoded\n",
    "            # categorical features : 987 top categorical features + 13 numerical\n",
    "            # features = 1000 total features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engineering.numDoMeasurement(subset = 'train', iStep = f'', fit = True)\n",
    "\n",
    "Engineering.numDoStandardize(subset = 'train', iStep = f'')\n",
    "Engineering.numDoStandardize(subset = 'tests', iStep = f'')\n",
    "Engineering.numDoStandardize(subset = 'valid', iStep = f'')\n",
    "\n",
    "Engineering.catFillUndefined(subset = 'train', iStep = f'normed')\n",
    "Engineering.catFillUndefined(subset = 'tests', iStep = f'normed')\n",
    "Engineering.catFillUndefined(subset = 'valid', iStep = f'normed')\n",
    "\n",
    "Engineering.catFindFrequents(subset = 'train', iStep = f'normed.filled', min = min, fit = True)\n",
    "\n",
    "Engineering.catMaskUncommons(subset = 'train', iStep = f'normed.filled', min = min)\n",
    "Engineering.catMaskUncommons(subset = 'tests', iStep = f'normed.filled', min = min)\n",
    "Engineering.catMaskUncommons(subset = 'valid', iStep = f'normed.filled', min = min)\n",
    "\n",
    "Engineering.catDoCodeFeature(subset = 'train', iStep = f'normed.filled.masked-{min:06d}', fit = True)\n",
    "Engineering.catDoCodeFeature(subset = 'tests', iStep = f'normed.filled.masked-{min:06d}')\n",
    "Engineering.catDoCodeFeature(subset = 'valid', iStep = f'normed.filled.masked-{min:06d}')\n",
    "\n",
    "Engineering.catDoPickFeature(subset = 'train', iStep = f'normed.filled.masked-{min:06d}.encode', top = top, fit = True)\n",
    "Engineering.catDoPickFeature(subset = 'tests', iStep = f'normed.filled.masked-{min:06d}.encode', top = top)\n",
    "Engineering.catDoPickFeature(subset = 'valid', iStep = f'normed.filled.masked-{min:06d}.encode', top = top)\n",
    "\n",
    "Engineering.allDoPackFeature(subset = 'train', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}', fit = True)\n",
    "Engineering.allDoPackFeature(subset = 'tests', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}')\n",
    "Engineering.allDoPackFeature(subset = 'valid', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engineering.toyTakeSubSample(subset = 'train', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}.packed', len = 8000)\n",
    "Engineering.toyTakeSubSample(subset = 'tests', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}.packed', len = 1000)\n",
    "Engineering.toyTakeSubSample(subset = 'valid', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}.packed', len = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Interaction Variables\n",
    "Finally, we also explored the creation of interaction variables to add more features and to capture possible interactions between the variables. The interaction features we created were between the numeric and the categorical features.\n",
    "\n",
    "The advanced feature engineering including interactions are available in the [Engineering-Advanced](./book/Engineering-Advanced.ipynb) workbook and [interactions.scala](./code/interactions.scala) scala module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allJoinInteract(subset: String, iStep: String) {\n",
    "\n",
    "    /*\n",
    "        This had to be written in Scala as PySpark implementation of SparkML interactions transformer was not available.\n",
    "        Add interactions between the categorical indicators and numerical features.\n",
    "        Use SparkML Interactions feature transformer to calculate the product of all 13 numerical features with the arbitary number of categorical indicators.\n",
    "\n",
    "        Input  : std_features and cat_features\n",
    "        Output : cxn_features SparseVector\n",
    "    */\n",
    "\n",
    "    var iData: DataFrame = taskStarting(\"action\", \"Numerical vs Categorical Interactions\", subset, iStep)\n",
    "    var oData: DataFrame = null\n",
    "\n",
    "    if (iData != null) {\n",
    "        val interaction = new Interaction()\n",
    "            .setInputCols(Array(\"std_features\", \"cat_features\"))\n",
    "            .setOutputCol(\"cxn_features\")\n",
    "\n",
    "        oData = interaction.transform(iData)\n",
    "    }\n",
    "\n",
    "    taskStopping(\"action\", \"Numerical vs Categorical Interactions\", subset, oData)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was imbalanced and so it was necessary to balance it before using the models given that there was a 3 to 1 ratio between negative and positive classes. We used the oversampling technique to create more positive examples from the same data resulting in a close to 1 to 1 ratio in the end. This allowed us to train tree-based classifiers that would suffer from the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampleData(train):\n",
    "    '''\n",
    "    Takes in a train dataframe and oversamples the positive variables.\n",
    "    \n",
    "    Returns the oversampled datframe\n",
    "    '''\n",
    "\n",
    "    # getthe counts for each class\n",
    "    label_counts_df = train.groupby('label').count().toPandas()\n",
    "    negative_count = label_counts_df.at[0, 'count']\n",
    "    positive_count = label_counts_df.at[1, 'count']\n",
    "\n",
    "    # Calculate the number of extra data points needed and the oversample rate\n",
    "    extra = negative_count - positive_count\n",
    "    print(f'Extra data points = {extra}')\n",
    "\n",
    "\n",
    "    resample_rate = extra/positive_count\n",
    "\n",
    "    print(f'The resample rate is {resample_rate}')\n",
    "\n",
    "    # use resample rate to create more positive examples\n",
    "    extra_positives = train.filter(train['label']==1).sample(withReplacement=True, fraction=resample_rate)\n",
    "\n",
    "    # add the new examples to the dataset\n",
    "    oversampled_train = train.union(extra_positives)\n",
    "    oversampled_train = oversampled_train.sample(withReplacement = False, fraction = 1.0)\n",
    "    \n",
    "    return oversampled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__References - Section 3__\n",
    "\n",
    "[1] J. de Wit, “Deep-Learning-for-Criteo-Documentation,” GitHub, 30-Sep-2014. [Online]. Available: https://github.com/juliandewit/kaggle_criteo. [Accessed: 11-Dec-2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Homegrown Implementation from Scratch - Who needs libraries anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the homegrown SVM can be found in the [Homegrown Algorithm - Kombucha](./book/KombuchaModel.ipynb). \n",
    "\n",
    "- Challenges: \n",
    "     That Gradient Descent! Working with Hinge Loss presents a new type of challenge: the function it's not entirely convex. We were able to code the gradient using similar techniques as the ones we did for Linear Regression wit L1 regularization: We use sub-gradients, which converge into the solution. The next problem was the decision of doing Batch Gradient Descent vs. Stochastic Gradient Descent. We decided to do Batch Gradient Descent, and immediatley understood why ML on Spark uses SGD - you sacrifice number of steps for time. As we seen in the Kombucha Notebook, the amount of time of GD it's way longer on big data sets compare to SGD.\n",
    "     \n",
    "     Another challenge is the Algorithm perse. We implemented a linear SVM, but from our EDA, we know that the data is not separable in a way that a linear classifier could be used. SVM provides an answer for this problem: the introduction of the kernel trick to change the perspective plane of data [__2__]. However, Spark doesn't provide with an implementation of basic kernels like Gaussian. This it's still in development, although we believe that it will be left behind as they move towards implementing Neural Networks and Deep Learning models.\n",
    "     \n",
    "- Validation:\n",
    "    We were able to achieve a similar level of AUC using our model on the test set, for both the toy model and the entire dataset, which is a great validation of the concepts. However, our implementation it's still far from being perfect - our training times it's at least 3 times worst that the ML implementation. \n",
    "    \n",
    "- Implementation:\n",
    "    All the implementations details are in [Homegrown Algorithm - Kombucha](./book/KombuchaModel.ipynb). We start from a baseline (either the mean of the labels or decide that all labels are one of the classes), and then we update the gradient at each iteration. We were able to use the toy as a some sort of development set to Fine Tune the Regularization Parameter. We also observe that the process takes time to converge, and that the graphs are not smooth - again a byproduct of using subGradients for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Scaled Classification on Full Dataset using Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the full dataset, we applied a scalable version of our algorithm from Spark MLLib together with other algorithms. We used Logistic Regression as our baseline. and thereafter applied our main algorithm, the Linear SVM and other tree based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             Classifier \t| Parameters                        \t| Feature Count              \t| Engineering Notes                                                                                       \t| Train AUC \t| Validation AUC \t| Test AUC \t|\n",
    "|-----------------------:\t|-----------------------------------\t|----------------------------\t|---------------------------------------------------------------------------------------------------------\t|-----------\t|----------------\t|----------\t|\n",
    "|     LogisticRegression \t| <ul> <li>maxIter = 100</li> </ul> \t| 1000 = 13 + 987            \t| <ul> <li>baseline 13 num + 987 cat features</li> </ul>                                                  \t| 73.64%    \t| 73.63%         \t| 73.65%   \t|\n",
    "|              LinearSVC \t| <ul> <li>maxIter = 100</li> </ul> \t| 1000 = 13 + 987            \t| <ul> <li>baseline 13 num + 987 cat features</li> </ul>                                                  \t| 68.69%    \t| 68.64%         \t| 68.70%   \t|\n",
    "| DecisionTreeClassifier \t| <ul> <li>maxIter = 100</li> </ul> \t| 1000 = 13 + 987            \t| <ul> <li>baseline 13 num + 987 cat features</li> </ul>                                                  \t| 53.97%    \t| 53.97%         \t| 53.97%   \t|\n",
    "| RandomForestClassifier \t| <ul> <li>numTrees = 30</li> </ul> \t| 1000 = 13 + 987            \t| <ul> <li>baseline 13 num + 987 cat features</li> </ul>                                                  \t| 70.06%    \t| 70.05%         \t| 70.08% \t|\n",
    "|          GBTClassifier \t| <ul> <li>maxIter = 10</li> </ul>  \t| 1000 = 13 + 987            \t| <ul> <li>baseline 13 num + 987 cat features</li> </ul>                                                  \t| 72.26%  \t|     72.25%     \t|  72.30%        \t|\n",
    "|     LogisticRegression \t| <ul> <li>maxIter = 100</li> </ul> \t| 16,463 = 13 + 987 + 10,231 \t| <ul> <li>baseline 13 num + 987 cat features</li> <li>additiona 13 num x 987 cat interactions</li> </ul> \t| 75.04%    \t| 74.99%         \t| 75.01%   \t|\n",
    "|     LogisticRegression \t| <ul> <li>maxIter = 100</li> </ul> \t| 36,168 = 13 + 36,155       \t| <ul> <li>extended 13 num + 36155 cat features</li> </ul>                                                \t| 77.57%    \t| 77.48%         \t| 77.49%   \t|\n",
    "|     LogisticRegression \t| <ul> <li>maxIter = 100</li> </ul> \t| 56,507 = 13 + 56,494       \t| <ul> <li>extended 13 num + 56494 cat features</li> </ul>                                                \t| 77.88%    \t| 77.75%         \t| 77.75%   \t|\n",
    "|     LogisticRegression \t| <ul> <li>maxIter = 100</li> </ul> \t| 159,275 = 13 + 159,262     \t| <ul> <li>extended 13 num + 159262 cat features</li> </ul>                                               \t| 78.41%    \t| 78.09%         \t| 78.08%   \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, none of the algorithms was able to beat the baseline set by the Logistic Regression classifier.With the closest algorithm being the Gradient Boosting Tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion about L1/L2 (elasticnet) and lambda (regParam) for Logistic Regression here. - **LAURA** reword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|         Classifier \t| Parameters                                                                                                            \t| Feature Count   \t| Modeling Notes                              \t| Train AUC \t| Validation AUC \t| Test AUC \t|\n",
    "|-------------------:\t|-----------------------------------------------------------------------------------------------------------------------\t|-----------------\t|---------------------------------------------\t|-----------\t|----------------\t|----------\t|\n",
    "| LogisticRegression \t| <ul> <li>maxIter = 100</li> <li>family =  binomial </li> <li>elasticNetParam  = 0.0</li> <li>regParam = 0.0</li></ul> \t| 1000 = 13 + 987 \t| No Regularization                           \t| 73.64%    \t| 73.63%         \t| 73.65%   \t|\n",
    "| LogisticRegression \t| <ul> <li>maxIter = 100</li> <li>family =  binomial </li> <li>elasticNetParam = 0.0</li> <li>regParam = 0.1</li></ul>  \t| 1000 = 13 + 987 \t| Ridge : L2 Regularization with Lambda = 0.1 \t| 72.63%    \t| 72.63%         \t| 72.64%   \t|\n",
    "| LogisticRegression \t| <ul> <li>maxIter = 100</li> <li>family =  binomial </li> <li>elasticNetParam  = 1.0</li> <li>regParam = 0.1</li></ul> \t| 1000 = 13 + 987 \t| Lasso : L1 Regularization with Lambda = 0.5 \t| 50.00%    \t| 50.00%         \t|  50.00%  \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"assets/OverUnderFitting.png\" width=\"550\">\n",
    "    <figcaption> <b>Over and Under Fitting Tradeoff (1)</b> </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are seeing so far, I think, is that we have not yet entered 'overfitting' territory. I.e. we are still very close between validation and training sets, which says that we are not overfitting. Because we are not overfitting, the lambda will not help us.\n",
    "\n",
    "The alpha probably does no good because we've already feature-engineered the heck out of the features. Increasing alpha (increasing L1) means we are taking more features out, which decreases our results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__References - Section 4__\n",
    "\n",
    "(1) https://www.jeremyjordan.me/evaluating-a-machine-learning-model/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LAURA** to start input here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. scalability / time complexity / I/O vs Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. One Hot Encoding / vector embeddings / feature selection  \n",
    "\n",
    "Section 3.3 Discussed the one-hot encoding and feature selection. Although we discussed this briefly in the course, this project allowed us to delve into the practicalities of feature engineering in detail. If every unique category in the categorical features were one-hot encoded, there would be approximately 33 million features, and an obvious issue with dimensionality, not to mention that these are extremely sparse and managing that many features is un-realistic. In order to address the issue, we looked at many different solutions, always aiming to reduce the number of categorical features but retain as much of the inherent information in the data as possible. Binning and using a Chi-Square selector turned out to be good ways to reduce the categorical features in a way that preserved information. We also looked at interactions between the numeric and categorical features as a way to 'add back' information that may have been lost during the categorical feature reduction phase. Finally, we saved the features as sparse vectors and used sparse processing functions - this helped us to train the models in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Broadcasting / Caching / Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Batch vs. Stochastic Gradient Descent\n",
    "\n",
    "This assignment finally uncovered the truth that we started to discuss during our class: The differences on scalability between Batch Gradient Descent and Stochastic Gradient Descent. As discussed on our Homegrown Model (Kambucha), we decided to implement a normal batch Gradient Descent, where we transverse the entire dataset once to find the total gradient, and then we update the weights in one go. The pros of doing this approach, is that convergence needs **less** steps than Stochastic Gradient Descent, but each iteration it's slower. In fact, the time complexity of GD it's related to the tolerance of the algorithm $O(nClog(1/\\epsilon)$, where C it's a constant time [__1__]. So, even though it's easier to implement, it's overall less efficient when we scale up the data.\n",
    "\n",
    "On the other hand, almost all the MLLib packages of Spark use Stochastic Gradient Descent as they go to algorithm for good reasons: the model it's almost linear with respect of the tolerance with a time complexity of $O(C/\\epsilon)$. The effort placed into coding a SGD implementation it's well worth the time gained in training for many of the more complicated models out there. So, even though we sacrifice steps, we gain efficiency in time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__References - Section 5__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]: A. Zisserman, \"Lecture 2: The SVM classifier\". Oxford. Retrieved from http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Bias-Variance Tradeoff\n",
    "\n",
    "We explored this when we were running our models, first when creating the baseline with Logisitic Regression. Here we tried to examine the effect of reguliraztion, both L1 and L2 and found that there were no improvements with regularization. Also since the area under the curve (AUC) did not vary much from train to test, we were able to conclude that we were not underfitting. Given that we received a reasonable AUC, which was above 50, we were also comfortable ruling out bias.\n",
    "\n",
    "Further, when working with decsion trees, we run into issues where the model was unable to fit the data well. Again, since our train and development AUC scores were similar, we were able to rule out variance. However the scores on train development and test sets were all low and close to 50 which is not very different from random guessing. To mitigate this, we used Random Forests and Gradient Boosted Trees which use an ensemble of trees and hence avoid overfitting trees while using many tress and combinign their predictions also avoids bias underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
