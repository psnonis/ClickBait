{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Click-through Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team 14   \n",
    "Brian Musisi, Pri Nonis, Vinicio Del Sola, Laura Chutny   \n",
    "Fall 2019, Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advertisers on websites make money when people click on an ad, visit the advertiser's site and then purchase something. This means that understanding the rate (or probability) at which people click on an ad is important - higher 'click-through' rates have the potential for more revenue. This study will not address the next step, which is how an advertiser converts a person who has 'clicked-through' to their site into a paying customer. Instead, our question is how to predict the click through rate for a given (unseen) ad based on the training data supplied to the model. In other words, for a given ad, what is the probability that a person will click on the ad? Ads cost money, so advertisers need to know which ads will generate more clicks and thus which ads are more valuable to the advertiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem - a 'positive' result (1) if the ad is clicked on and a 'negative' result (0) if the ad is not clicked on. There is a very large imbalance between classes - far more impressions (views of the ad) with no click (0) than impressions which result in a click (1). In this instance we need to decide between false positives (type 1 error - where we predict a click that did not actually happen) and false negatives (type 2 errors - where we do not predict a click when there actually was one). Because advertisers pay more for ads that are clicked, we want to be conservative in our predictions, and avoid false positives.\n",
    "\n",
    "Note that Click Through Rate is defined as the number of ads that are clicked on as a fraction of the total impressions of that ad that are viewed. In this case, each example in the dataset is an impression of the ad.\n",
    "\n",
    "In order to be effective, this type of predictor should achieve an AUC, or 'Area Under The ROC (Receiver Operating Curve)' of approximately 0.75-0.8. The ROC plots the true-positive rate against the false-positive rate. As the ROC becomes more concave, the achievable true positive rate for a given false positive rate increases, as does the area under the curve. Another way to say this, as summarized on Wikipedia (__3__), is that the AUC is \"equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one\".  The 2nd place winner of the Criteo Kaggle competition had an AUC of 0.8097 (__1__), also another attempt at prediction on the Criteo dataset reported an AUC of 0.79 (__2__). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__References - Section 1__  \n",
    "(1) M. Jahrer, “Can anyone tell me the AUC of benchmark model ?,” kaggle.com, 2014. [Online]. Available: https://www.kaggle.com/c/criteo-display-ad-challenge/discussion/9821.   \n",
    "\n",
    "(2) wormhole developers, “Binary Classification on the Criteo CTR Dataset,” wormhole.readthedocs.io, 2015. [Online]. Available: https://wormhole.readthedocs.io/en/latest/tutorial/criteo_kaggle.html.   \n",
    "\n",
    "(3) “Receiver Operating Characteristic,” wikipedia.org, Nov-2019. [Online]. Available: https://en.wikipedia.org/wiki/Receiver_operating_characteristic.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many possible choices of algorithm for binary classification problems. Logistic Regression, Decision Tree / Decision Tree Forest, Support Vector Machine (SVM), and Factorization Machine have each proven to be worthy choices. In this project, for our toy model and main algorithm, we decided to focus on the SVM, as this algorithm had not been focused on in the original Kaggle Criteo competition. We will also investigate the performance on many algorithms.\n",
    "\n",
    "The Support Vector Machine is an algorithm that divides a set of data into two classes. In 2 dimensions, we can think of this as a line separating the classes. What the SVM allows us to do is extend this concept to different dimensions (i.e. feature space) and create a 'best-fit' hyperplane to separate the classes. The transformation to a different dimensional space is called a kernel.\n",
    "\n",
    "The prediction of the class (where class is redefined from $y_i \\in \\{0, 1\\}$ to $y_i \\in \\{-1, 1\\}$) in this case will be done with a linear kernel - i.e. a linear hyperplane where the hyperplane equation is $\\left(w^\\top x_i+b\\right)$:  \n",
    "\n",
    "$$ class =  y_i\\left(w^\\top x_i +b\\right)_+ $$  \n",
    "\n",
    "where:  \n",
    "\n",
    "$$\n",
    "class  = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1, & \\quad \\text{if} \\quad \\left(w^\\top x_i+b\\right) \\geq 1 \\\\\n",
    "            -1, & \\quad \\text{if} \\quad \\left(w^\\top x_i+b\\right) \\leq -1\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "The goal in Support Vector Machines is to maximize the distance between the hyperplane and any of the examples, in otherwords, maximize the 'margin' (**1**). The margin can be thought of in two dimensions as shown in the following diagram:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"SVM_Margin.png\" width=\"350\">\n",
    "    <figcaption> <b>Support Vector Machine Margin (2)</b> </figcaption>\n",
    "</figure>\n",
    "\n",
    "From trigonometry and vector algebra, we can compute the distance between the two margins by adding the perpendicular distances from the positive and negative margins to the hyperplane that solves $\\left(w^\\top x_i+b\\right)=0$. This distance, the margin, we will denote as $\\frac{2}{\\parallel w\\parallel }$. This allows us to say that in order to maximize the margin, we must minimize $\\parallel w\\parallel$. In order to maintain differentiability, we will actually minimize $\\parallel w\\parallel^2$, because the same value of $w$ satisifies both minimizations.\n",
    "\n",
    "This allows us to write what is called the 'hard-margin' Support Vector Machine. The objective function is:\n",
    "\n",
    "$\\underset{w}{min}(J\\text{'}(w))=\\underset{}{\\underset{w}{min}(\\frac{\\lambda }{2})\\parallel w{\\parallel }^{2}}$  \n",
    "such that $y_i\\left(w^\\top x_i +b\\right)\\ge 1$\n",
    "\n",
    "This is called a 'hard-margin' SVM, because the data have to be on one or the other side of our hyperplane. Unfortunately, the world is not usually that kind - we may be able to separate a large amount of our data with the hyperplane, but there will still be some samples that just aren't classified correctly, no matter where we place the hyperplane. For this reason, we need to allow some 'slack', or 'softness' in the margin. In order to achieve this we introduce the Hinge Loss function:\n",
    "\n",
    "\n",
    "\n",
    "The loss function here is Hinge Loss: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Loss function:\n",
    "- Log Loss (Cross Entropy)\n",
    "- Exponential Loss\n",
    "- Hinge Loss (as a proxy for 0/1 loss) - can't be used with general gradient descent as it is not differentiable for all x, but can be used with subgradients which are locally differentiable\n",
    "\n",
    "3. Hyperparameter tuning\n",
    "\n",
    "4. Evaluation Metric\n",
    "- Accuracy is not a good metric - we could have excellent accuracy by correctly predicting 100% of the test examples as 0, while the true number might be 96% - so we would have a great accuracy of 96, but in actual fact, we would have missed out predicting the actual positive values (1).\n",
    "- With a goal of limiting the false positives, precision (TP/(TP + FP)) will work well. We would trade this off with minimizing the number of false negatives (sensitivity (recall): TP/(TP+FN)). \n",
    "- Or we could combine them to optimize the most precision with the best sensitivity by using the F-score: 2\\*((P\\*S)/(P+S))\n",
    "- An average precision - the area under the precision recall curve (AUC) helps give better inference than just a single F score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Toy example with hand calculation/ simple code\n",
    "\n",
    "**VINICIO and/or LAURA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__References - Section 2__  \n",
    "\n",
    "[1]H. Daumé, A Course in Machine Learning. 2017.\n",
    "[2]K. K. Mahto, “Demystifying Maths of SVM,” Medium, 18-Apr-2019. [Online]. Available: https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e. [Accessed: 11-Dec-2019].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LAURA** Will clean up this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:35] : Starting Spark Initialization\n",
      "[18:16:40] : Stopping Spark Initialization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Engineering.setupSpark(application = 'eda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:42] : Starting Data Import\n",
      "[18:16:42] : Stopping Data Import\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Engineering.importData(location = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:44] : Starting Data Splits\n",
      "[18:16:47] : Stopping Data Splits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Engineering.splitsData(ratios = [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was split before any EDA into Training/Validation/Test files, at an 80/10/10 ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss Schema - numerical variables (and normalization) & Distribution of values - mean, median, skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Numerical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic statistics for all the numerical variables was first run and reported, as shown in the attached table. Median and skewness were added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n01</th>\n",
       "      <td>20033724.0</td>\n",
       "      <td>3.502948</td>\n",
       "      <td>9.459785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5775.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n02</th>\n",
       "      <td>36669584.0</td>\n",
       "      <td>105.835815</td>\n",
       "      <td>389.627045</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>36664.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n03</th>\n",
       "      <td>28799232.0</td>\n",
       "      <td>26.907507</td>\n",
       "      <td>397.482605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65535.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n04</th>\n",
       "      <td>28721092.0</td>\n",
       "      <td>7.322990</td>\n",
       "      <td>8.787283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n05</th>\n",
       "      <td>35723496.0</td>\n",
       "      <td>18537.207031</td>\n",
       "      <td>69412.343750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23159456.0</td>\n",
       "      <td>2842.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n06</th>\n",
       "      <td>28468040.0</td>\n",
       "      <td>116.084457</td>\n",
       "      <td>382.811981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>430898.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n07</th>\n",
       "      <td>35083980.0</td>\n",
       "      <td>16.340485</td>\n",
       "      <td>66.314934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56311.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n08</th>\n",
       "      <td>36651352.0</td>\n",
       "      <td>12.516854</td>\n",
       "      <td>16.639774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6047.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n09</th>\n",
       "      <td>35083980.0</td>\n",
       "      <td>106.111786</td>\n",
       "      <td>220.357422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29019.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10</th>\n",
       "      <td>20033724.0</td>\n",
       "      <td>0.617539</td>\n",
       "      <td>0.684164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n11</th>\n",
       "      <td>35083980.0</td>\n",
       "      <td>2.733224</td>\n",
       "      <td>5.199368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n12</th>\n",
       "      <td>8612511.0</td>\n",
       "      <td>0.991020</td>\n",
       "      <td>5.633997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4008.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n13</th>\n",
       "      <td>28721092.0</td>\n",
       "      <td>8.217725</td>\n",
       "      <td>16.211697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7393.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count          mean        stddev  min         max  median\n",
       "n01  20033724.0      3.502948      9.459785  0.0      5775.0     1.0\n",
       "n02  36669584.0    105.835815    389.627045 -3.0     36664.0     3.0\n",
       "n03  28799232.0     26.907507    397.482605  0.0     65535.0     7.0\n",
       "n04  28721092.0      7.322990      8.787283  0.0       933.0     4.0\n",
       "n05  35723496.0  18537.207031  69412.343750  0.0  23159456.0  2842.0\n",
       "n06  28468040.0    116.084457    382.811981  0.0    430898.0    34.0\n",
       "n07  35083980.0     16.340485     66.314934  0.0     56311.0     4.0\n",
       "n08  36651352.0     12.516854     16.639774  0.0      6047.0     8.0\n",
       "n09  35083980.0    106.111786    220.357422  0.0     29019.0    40.0\n",
       "n10  20033724.0      0.617539      0.684164  0.0        10.0     1.0\n",
       "n11  35083980.0      2.733224      5.199368  0.0       231.0     1.0\n",
       "n12   8612511.0      0.991020      5.633997  0.0      4008.0     0.0\n",
       "n13  28721092.0      8.217725     16.211697  0.0      7393.0     4.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_measures = load(open('data/model.pickled.normed.num_measures', 'rb'))\n",
    "num_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the basic statistics up - the following steps to analysing the numerical variables were taken:  \n",
    "1) Plot histograms and scatter plots of each variable, along with a box plot and violin plot of a 10% sample of each variable.  \n",
    "2) Determine the distribution of each variable  \n",
    "3) Apply a standardization for each variable.  \n",
    " \n",
    "Variables are standardized not to create 'normal' variables, but to bring the values into a region of approximately (-1,3) so that machine learning techniques could be applied. The following table shows the variable, the distribution as observed, and the standardization applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Numerical Variable | Distribution Type          | Standardization          |\n",
    "|--------------------|----------------------------|--------------------------|\n",
    "| i01                | Exponentially Decreasing   | i01' = i01/(2*SD)        |\n",
    "| i02                | Truncated Skewed Normal    | i02' = (i02 - median)/SD |\n",
    "| i03                | Exponentially Decreasing   | i03' = i03/SD            |\n",
    "| i04                | Truncated Skewed Normal    | i04' = (i04-median)/SD   |\n",
    "| i05                | Truncated Skewed Normal    | i05' = (i05-median)/SD   |\n",
    "| i06                | Exponentially Decreasing   | i06' = i06/2*SD          |\n",
    "| i07                | Exponentially Decreasing   | i07' = i07/2*SD          |\n",
    "| i08                | Exponentially Decreasing   | i08' = i08/2*SD          |\n",
    "| i09                | Truncated Skewed Normal    | i09' = (i09-median)/SD   |\n",
    "| i10                | Sigmoid                    | i10' = i10/Max(i10)      |\n",
    "| i11                | Truncated Skewed Normal    | i11' = (i11-median)/SD   |\n",
    "| i12                | Exponentially Decreasing   | i12' = i12/2*SD          |\n",
    "| i13                | Truncated Skewed Normal    | i13' = (i13-median)/SD   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f3c903778949e38521625308717553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs  = []\n",
    "\n",
    "def featureAnalysisNumerical(df, feature):\n",
    "\n",
    "    output = widgets.Output()\n",
    "    data   = df[~np.isnan(df[feature])]\n",
    "    y      = data['ctr']\n",
    "    x      = data[feature]\n",
    "    \n",
    "    xmax   = max(x)\n",
    "    \n",
    "    with output:\n",
    "        fig = plt.figure(figsize = (28, 7))\n",
    "\n",
    "        ax1 = fig.add_subplot(1, 4, 1)\n",
    "        ax1 = sns.boxplot(x)\n",
    "      # ax1.set(xlim=(-1,40))\n",
    "        \n",
    "        ax2 = fig.add_subplot(1, 4, 2)\n",
    "        ax2 = sns.violinplot(x)\n",
    "      # ax2.set(xlim=(-1,40))\n",
    "\n",
    "        ax3 = fig.add_subplot(1, 4, 3)\n",
    "        ax3 = sns.distplot(x, hist = True, color = 'red')\n",
    "      # ax3.set(xlim=(-1,40))\n",
    "\n",
    "        ax4 = fig.add_subplot(1, 4, 4)\n",
    "        ax4 = sns.scatterplot(x, y, data = df, hue = 'ctr')\n",
    "\n",
    "        plt.show(fig)\n",
    "        \n",
    "    return [output]\n",
    "\n",
    "pf  = workingSet['df_toy'].sample(fraction = 0.001, seed = 2019).cache().toPandas()\n",
    "tab = widgets.Tab()\n",
    "for n, feature in enumerate(workingSet['num_columns']):\n",
    "    outputs += featureAnalysisNumerical(pf, feature)\n",
    "    tab.set_title(n, feature)\n",
    "\n",
    "tab.children = outputs\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Character Features\n",
    "\n",
    "**PRI / BRIAN** Please add some commentary and simplified code snippets here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss Schema - Character variables and (indexing? or whatever else we do with them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 26 categorical features, which we labelled from c01 to c26 based on their oder in the dataset and they had all been hashed. We analyzed these features and found that they had very high cardinality. The number of unique categories for each feature is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distinct</th>\n",
       "      <th>frequent</th>\n",
       "      <th>uncommon</th>\n",
       "      <th>top five</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c01</th>\n",
       "      <td>1460</td>\n",
       "      <td>23</td>\n",
       "      <td>1437</td>\n",
       "      <td>05db9164, 68fd1e64, 5a9ed9b0, 8cf07265, be589b51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c02</th>\n",
       "      <td>581</td>\n",
       "      <td>100</td>\n",
       "      <td>481</td>\n",
       "      <td>38a947a1, 207b2d81, 38d50e09, 1cfdf714, 287130e0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c03</th>\n",
       "      <td>8381767</td>\n",
       "      <td>45</td>\n",
       "      <td>8381722</td>\n",
       "      <td>deadbeef, d032c263, 02cf9876, aa8c1539, 9143c832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c04</th>\n",
       "      <td>1883842</td>\n",
       "      <td>60</td>\n",
       "      <td>1883782</td>\n",
       "      <td>c18be181, deadbeef, 29998ed1, d16679b9, 85dd697c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c05</th>\n",
       "      <td>305</td>\n",
       "      <td>13</td>\n",
       "      <td>292</td>\n",
       "      <td>25c83c98, 4cf72387, 43b19349, 384874ce, 30903e74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c06</th>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>7e0ccccf, fbad5c96, fe6b92e5, deadbeef, 13718bbd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c07</th>\n",
       "      <td>12488</td>\n",
       "      <td>62</td>\n",
       "      <td>12426</td>\n",
       "      <td>1c86e0eb, dc7659bd, 7195046d, 5e64ce5f, 468a0854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c08</th>\n",
       "      <td>633</td>\n",
       "      <td>17</td>\n",
       "      <td>616</td>\n",
       "      <td>0b153874, 5b392875, 1f89b562, 37e4aa92, 062b5529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c09</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a73ee510, 7cc72ec2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c10</th>\n",
       "      <td>88956</td>\n",
       "      <td>42</td>\n",
       "      <td>88914</td>\n",
       "      <td>3b08e48b, efea433b, fbbf2c95, fa7d0797, 03e48276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c11</th>\n",
       "      <td>5656</td>\n",
       "      <td>93</td>\n",
       "      <td>5563</td>\n",
       "      <td>755e4a50, e51ddf94, 7f8ffe57, 4d8549da, 8b94178b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c12</th>\n",
       "      <td>6952733</td>\n",
       "      <td>52</td>\n",
       "      <td>6952681</td>\n",
       "      <td>deadbeef, dfbb09fb, 6aaba33c, 8fe001f4, d8c29807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c13</th>\n",
       "      <td>3194</td>\n",
       "      <td>105</td>\n",
       "      <td>3089</td>\n",
       "      <td>5978055e, 3516f6e6, 46f42a63, 025225f2, 1aa94af3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c14</th>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>b28479f6, 07d13a8f, 1adce6ef, 64c94865, cfef1c29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c15</th>\n",
       "      <td>14754</td>\n",
       "      <td>112</td>\n",
       "      <td>14642</td>\n",
       "      <td>2d0bb053, d345b1a0, 3628a186, 10040656, 10935a85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c16</th>\n",
       "      <td>4590520</td>\n",
       "      <td>58</td>\n",
       "      <td>4590462</td>\n",
       "      <td>deadbeef, 84898b2a, b041b04a, 36103458, c64d548f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c17</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>e5ba7672, 07c540c4, d4bb7bd8, 3486227d, 776ce399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c18</th>\n",
       "      <td>5582</td>\n",
       "      <td>126</td>\n",
       "      <td>5456</td>\n",
       "      <td>e88ffc9d, 891589e7, 2804effd, c21c3e4c, 5aed7436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c19</th>\n",
       "      <td>2169</td>\n",
       "      <td>22</td>\n",
       "      <td>2147</td>\n",
       "      <td>deadbeef, 21ddcdc9, 55dd3565, 5b885066, 9437f62f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c20</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>deadbeef, b1252a9d, 5840adea, a458ea53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c21</th>\n",
       "      <td>5894178</td>\n",
       "      <td>52</td>\n",
       "      <td>5894126</td>\n",
       "      <td>deadbeef, 0014c32a, 723b4dfd, e587c466, 5f957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c22</th>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>deadbeef, ad3062eb, c9d4222a, 78e2e389, 8ec974f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c23</th>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>32c7478e, 3a171ecb, 423fab69, bcdee96c, be7c41b4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c24</th>\n",
       "      <td>258639</td>\n",
       "      <td>60</td>\n",
       "      <td>258579</td>\n",
       "      <td>3fdb382b, b34f3128, 3b183c5c, 1793a828, deadbeef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c25</th>\n",
       "      <td>105</td>\n",
       "      <td>19</td>\n",
       "      <td>86</td>\n",
       "      <td>deadbeef, 001f3601, e8b83407, ea9a246c, cb079c2d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c26</th>\n",
       "      <td>133254</td>\n",
       "      <td>37</td>\n",
       "      <td>133217</td>\n",
       "      <td>deadbeef, 49d68486, c84c4aec, 2fede552, c27f155b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     distinct  frequent  uncommon  \\\n",
       "c01      1460        23      1437   \n",
       "c02       581       100       481   \n",
       "c03   8381767        45   8381722   \n",
       "c04   1883842        60   1883782   \n",
       "c05       305        13       292   \n",
       "c06        24         7        17   \n",
       "c07     12488        62     12426   \n",
       "c08       633        17       616   \n",
       "c09         3         2         1   \n",
       "c10     88956        42     88914   \n",
       "c11      5656        93      5563   \n",
       "c12   6952733        52   6952681   \n",
       "c13      3194       105      3089   \n",
       "c14        27        13        14   \n",
       "c15     14754       112     14642   \n",
       "c16   4590520        58   4590462   \n",
       "c17        10         9         1   \n",
       "c18      5582       126      5456   \n",
       "c19      2169        22      2147   \n",
       "c20         4         4         0   \n",
       "c21   5894178        52   5894126   \n",
       "c22        18         6        12   \n",
       "c23        15        11         4   \n",
       "c24    258639        60    258579   \n",
       "c25       105        19        86   \n",
       "c26    133254        37    133217   \n",
       "\n",
       "                                             top five  \n",
       "c01  05db9164, 68fd1e64, 5a9ed9b0, 8cf07265, be589b51  \n",
       "c02  38a947a1, 207b2d81, 38d50e09, 1cfdf714, 287130e0  \n",
       "c03  deadbeef, d032c263, 02cf9876, aa8c1539, 9143c832  \n",
       "c04  c18be181, deadbeef, 29998ed1, d16679b9, 85dd697c  \n",
       "c05  25c83c98, 4cf72387, 43b19349, 384874ce, 30903e74  \n",
       "c06  7e0ccccf, fbad5c96, fe6b92e5, deadbeef, 13718bbd  \n",
       "c07  1c86e0eb, dc7659bd, 7195046d, 5e64ce5f, 468a0854  \n",
       "c08  0b153874, 5b392875, 1f89b562, 37e4aa92, 062b5529  \n",
       "c09                                a73ee510, 7cc72ec2  \n",
       "c10  3b08e48b, efea433b, fbbf2c95, fa7d0797, 03e48276  \n",
       "c11  755e4a50, e51ddf94, 7f8ffe57, 4d8549da, 8b94178b  \n",
       "c12  deadbeef, dfbb09fb, 6aaba33c, 8fe001f4, d8c29807  \n",
       "c13  5978055e, 3516f6e6, 46f42a63, 025225f2, 1aa94af3  \n",
       "c14  b28479f6, 07d13a8f, 1adce6ef, 64c94865, cfef1c29  \n",
       "c15  2d0bb053, d345b1a0, 3628a186, 10040656, 10935a85  \n",
       "c16  deadbeef, 84898b2a, b041b04a, 36103458, c64d548f  \n",
       "c17  e5ba7672, 07c540c4, d4bb7bd8, 3486227d, 776ce399  \n",
       "c18  e88ffc9d, 891589e7, 2804effd, c21c3e4c, 5aed7436  \n",
       "c19  deadbeef, 21ddcdc9, 55dd3565, 5b885066, 9437f62f  \n",
       "c20            deadbeef, b1252a9d, 5840adea, a458ea53  \n",
       "c21  deadbeef, 0014c32a, 723b4dfd, e587c466, 5f957280  \n",
       "c22  deadbeef, ad3062eb, c9d4222a, 78e2e389, 8ec974f4  \n",
       "c23  32c7478e, 3a171ecb, 423fab69, bcdee96c, be7c41b4  \n",
       "c24  3fdb382b, b34f3128, 3b183c5c, 1793a828, deadbeef  \n",
       "c25  deadbeef, 001f3601, e8b83407, ea9a246c, cb079c2d  \n",
       "c26  deadbeef, 49d68486, c84c4aec, 2fede552, c27f155b  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_measures = load(open('data/model.pickled.normed.filled.masked-060000.cat_measures', 'rb'))\n",
    "\n",
    "df             = pd.DataFrame.from_dict({ k: [len(v)] for k,v in cat_measures['distinct'].items() }, orient = 'index', columns = ['distinct'])\n",
    "df['frequent'] = [len(v) for v in cat_measures['frequent'].values()]\n",
    "df['uncommon'] = [len(v) for v in cat_measures['uncommon'].values()]\n",
    "df['top five'] = [', '.join(v[:5]) for v in cat_measures['frequent'].values()]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the number of distict categories ranges from 3 to 5894178. Working with this number of cardinal features presents challenges so we examined ways to reduce the cardinality of these features, which included:\n",
    "1. For each feature, consider the most commonly occurring features in the training set only and encode any other rare categories as one category\n",
    "2. Use Field-aware Factorization Machines which handle high cardinality data in recommendation and click thoguh rate datases by default\n",
    "3. Hash the categorical features to a smaller feature space with possible collisions\n",
    "4. Drop columns with extreme cardinality since these may correspond to features like user IDs or street address that ordinarily may not provide much value to the dataset.\n",
    "\n",
    "In the end, we chose option 1 because this allowed us to create a dataset that could be used with multiple algorithms while allowing us to shrink the feature space. We also found that for most features. Mnay of the categories appeared in under 1% of the training set (shown below) and so encoding the rare categories into a special category would greatly reduce the potential feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue we had to deal with categorical features, just like with the numeric features was how to deal with null values. Many of the categorical variables had a high number of null values with some columsn having over 40% null values. Below is a breakdown of the percentage of null values for each column. We would need an effective way to deal with these null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 General Feature Engineering\n",
    "\n",
    "**PRI / BRIAN** Please add some commentary and simplified code snippets here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EDA guided how we went about out feature engineering. The items that we needed to tackle in our feature engineering were:\n",
    "1. Scale the numeric features so that they are within the same range using an appropriate scaler based on the feature\n",
    "2. Deal with null values. For the numeric features, null values were replaced with the median while with the categorical features, null values were replaced witha special category, \"deadbeef\".\n",
    "3. Reduce the number of categories in each column by encoding rare categories as rarebeef and only keeping those that appear more than 1% of the time\n",
    "4. One-hot encode the categorical features so that they can be used to train the model\n",
    "5. Reduce the number of features in the dataset by using a feature selector based on the chi-squared test\n",
    "6. Create interaction variables between the features in the dataset\n",
    "\n",
    "\n",
    "#### Numeric Features\n",
    "The numeric features had different ranges and so it was imprtant to scale them especially when working with linear models like Logistic Regression. As shown in the EDA above, the numeric features had different distributions so they were scaled differently based on this. The considerations made when choosing a scaler were:\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\n",
    "#### Dealing with the Null Values\n",
    "The null values were dealt with differently for numeric and categorical features. For the numeric features, the null values were imputed using the ___ . For the cateogircal values, the null values were all encoded to a category labelled **deadbeef**\n",
    "\n",
    "#### Reducing the cardinality of categorical features\n",
    "As mentioned before, the cardinaly of the categorical features was handled by looking at each feature and encoding all the rare categories to one special category called **rarebeef**.\n",
    "\n",
    "#### One-hot encode the categorical features\n",
    "To be able to use the categorical features in our machine learning algorithms, we had to turn the categorical features into numeric features. To do this, we first used Spark's StrinIndexer to create a string index for each of the categories within the features and then one-hot encoded them\n",
    "\n",
    "#### Feature Selection\n",
    "After carrying out the other feature engineering steps, we had a number of features. To determine which ones we should keep and to reduce dimensionality, we used Pyspark's ChiSqSelector to choose the best features. This uses the Chi-Squared Test of Independence to select the best features based on the outcome variable. The variant we used chooses the top features with the highest predictive power in relation to the outcome variable.\n",
    "\n",
    "#### Create Interaction Variables\n",
    "Finally, we also explored the creation of interaction variables to add more features and to capture possible interactions between the variables. The interaction features we created were between the numeric and the categorical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of NaNs and what our approach is\n",
    "- Feature Engineering - how we increased/reduced features and implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = 60000 # minimum occurance threshold for each categorical feature category, generates 1150 one-hot encoded categorical features\n",
    "top =   987 # maximum selection threshold for ChiSquareSelector of one-hot encoded categorical features : 987 top categorical features + 13 numerical features = 1000 total features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engineering.numDoMeasurement(subset = 'train', iStep = f'', fit = True)\n",
    "\n",
    "Engineering.numDoStandardize(subset = 'train', iStep = f'')\n",
    "Engineering.numDoStandardize(subset = 'tests', iStep = f'')\n",
    "Engineering.numDoStandardize(subset = 'valid', iStep = f'')\n",
    "\n",
    "Engineering.catFillUndefined(subset = 'train', iStep = f'normed')\n",
    "Engineering.catFillUndefined(subset = 'tests', iStep = f'normed')\n",
    "Engineering.catFillUndefined(subset = 'valid', iStep = f'normed')\n",
    "\n",
    "Engineering.catFindFrequents(subset = 'train', iStep = f'normed.filled', min = min, fit = True)\n",
    "\n",
    "Engineering.catMaskUncommons(subset = 'train', iStep = f'normed.filled', min = min)\n",
    "Engineering.catMaskUncommons(subset = 'tests', iStep = f'normed.filled', min = min)\n",
    "Engineering.catMaskUncommons(subset = 'valid', iStep = f'normed.filled', min = min)\n",
    "\n",
    "Engineering.catDoCodeFeature(subset = 'train', iStep = f'normed.filled.masked-{min:06d}', fit = True)\n",
    "Engineering.catDoCodeFeature(subset = 'tests', iStep = f'normed.filled.masked-{min:06d}')\n",
    "Engineering.catDoCodeFeature(subset = 'valid', iStep = f'normed.filled.masked-{min:06d}')\n",
    "\n",
    "Engineering.catDoPickFeature(subset = 'train', iStep = f'normed.filled.masked-{min:06d}.encode', top = top, fit = True)\n",
    "Engineering.catDoPickFeature(subset = 'tests', iStep = f'normed.filled.masked-{min:06d}.encode', top = top)\n",
    "Engineering.catDoPickFeature(subset = 'valid', iStep = f'normed.filled.masked-{min:06d}.encode', top = top)\n",
    "\n",
    "Engineering.allDoPackFeature(subset = 'train', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}', fit = True)\n",
    "Engineering.allDoPackFeature(subset = 'tests', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}')\n",
    "Engineering.allDoPackFeature(subset = 'valid', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engineering.toyTakeSubSample(subset = 'train', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}.packed', len = 8000)\n",
    "Engineering.toyTakeSubSample(subset = 'tests', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}.packed', len = 1000)\n",
    "Engineering.toyTakeSubSample(subset = 'valid', iStep = f'normed.filled.masked-{min:06d}.encode.picked-{top:06d}.packed', len = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__References - Section 3__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Parallel implementation of main algorithm using MLLib\n",
    "- challenges\n",
    "- validation\n",
    "- __VINICIO__ add commentary on 'main' algorithm (SVM?) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Scaled Classification on Full Dataset using Spark ML\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**BRIAN** Add brief commentar to go with table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the full dataset, we applied a scalable version of our algorithm from Spark MLLib together with other algorithms. We used Logistic Regression as our baseline. and thereafter applied our main algorithm, the Linear SVM and other tree based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             Classifier | Parameters                                                                           | Feature Count                | Engineering Notes                                                                                           | Train AUC | Validation AUC | Test AUC |\n",
    "|-----------------------:|--------------------------------------------------------------------------------------|------------------------------|-------------------------------------------------------------------------------------------------------------|-----------|----------------|----------|\n",
    "|     LogisticRegression | <ul> <li>maxIter = 100</li> <li>family = binomial</li> <li>regParam = 0.0</li> </ul> | 1000 = 13 + 787              | <ul> <li>baseline 13 num + 987 cat features</li> </ul>                                                      | 73.64%    | 73.63%         | 73.65%   |\n",
    "|              LinearSVC |                                                                                      | 1000 = 13 + 787              | <ul> <li>baseline 13 num + 987 cat features</li> </ul>                                                      |           |                |          |\n",
    "| DecisionTreeClassifier |                                                                                      | 1000 = 13 + 787              | <ul> <li>baseline 13 num + 987 cat features</li> </ul>                                                      |           |                |          |\n",
    "| RandomForestClassifier |                                                                                      | 1000 = 13 + 787              | <ul> <li>baseline 13 num + 987 cat features</li> </ul>                                                      |           |                |          |\n",
    "|          GBTClassifier |                                                                                      | 1000 = 13 + 787              | <ul> <li>baseline 13 num + 987 cat features</li> </ul>                                                      |           |                |          |\n",
    "|     LogisticRegression |                                                                                      | 1031 = 13 + 787 + 10231      | <ul> <li>baseline 13 num + 987 cat features</li> <li>additiona 13 num x 987 cat interactions</li> </ul>     |           |                |          |\n",
    "|     LogisticRegression |                                                                                      | 56571 = 13 + 56558           | <ul> <li>extended 13 num + 56558 cat features</li> </ul>                                                    | 77.88%    | 77.75%         | 77.75%   |\n",
    "|     LogisticRegression |                                                                                      | 791503 = 13 + 56535 + 734955 | <ul> <li>extended 13 num + 56535 cat features</li> <li>additiona 13 num x 56535 cat interactions</li> </ul> |           |                |          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, none of the algorithms was able to beat the baseline set by the Logistic Regression classifier.With the closest algorithm being the Linear SVM. This is in line with what we would expect in terms of similarity between the Linear SVm and the Logistic Regression because the SVM with a linear kernel behaves similar to Logistic Regression with the main differentiator being the loss; logistic loss for the Logistic Regression and Hinge Loss for SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion about L1/L2 (elasticnet) and lambda (regParam) for Logistic Regression here. - **LAURA** reword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"OverUnderFitting.png\" width=\"550\">\n",
    "    <figcaption> <b>Over and Under Fitting Tradeoff (1)</b> </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are seeing so far, I think, is that we have not yet entered 'overfitting' territory. I.e. we are still very close between validation and training sets, which says that we are not overfitting. Because we are not overfitting, the lambda will not help us.\n",
    "\n",
    "The alpha probably does no good because we've already feature-engineered the heck out of the features. Increasing alpha (increasing L1) means we are taking more features out, which decreases our results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__References - Section 4__\n",
    "\n",
    "(1) https://www.jeremyjordan.me/evaluating-a-machine-learning-model/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LAURA** to start input here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__References - Section 5__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
